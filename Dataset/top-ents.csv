,ent_id,ent_type,max_z-score,first_year,z-score_dict,ent_cluster
0,1,Method,43.3138,2019,"{'2019': 14.3712, '2020': 34.6266, '2021': 43.3138, '2022': 41.6307}","['BERT', 'BERT', 'BERT model', 'BERT-base', 'BERT models', 'BERT base', 'BERT BASE', 'BERT-large', 'BERT-based models', 'BERT-Base', 'BERT-Large', 'BERT-base model', 'BERT-based', 'BERT-based model', 'Bert', 'BERT LARGE', 'BERT )', 'BERT base model', 'VL-BERT', 'bert', 'BERT architecture', 'BERT classifier', 'BERT large', 'BERT Base', 'BERT BASE model', 'BERTbase', 'TOD-BERT', 'BERT-like models', 'BERT embedding', 'TR-BERT', 'BERT-QA', 'BERT-large model', 'PoWER-BERT', 'BERT-flow', 'BERT Large', 'BERT-FT', 'BERT-based classifier', 'F BERT', 'ToD-BERT', 'MPC-BERT', 'BERT-based methods', 'BERT-wwm', 'BERT-PKD', 'Table-BERT', 'BERT-SPC', 'BERT L', 'bert2BERT', 'BERTlarge', 'BERT LARGE model', 'BERT-BASE', 'Topic-BERT', 'BERT-PT', 'BERT/RoBERTa', 'bert-base-cased', 'BERT-Base model', 'IS-BERT', 'VD-BERT', 'BERT-style models', 'BERT training', 'BERT-GPT', 'BERT-S', 'BERT-score', 'BERT-LARGE', 'bert-base', 'Bidirectional Encoder Representations from Transformers ( BERT )', 'KAM-BERT', 'BERTBase', 'BERT-like', 'BERT4GCN', 'BERT-RUBER', 'RACL-BERT', 'BERT+', 'BERT-base models', 'BERT S', 'BERT-iBLEU', 'BERTLarge', 'BERT-small', 'BERT-ft', 'BERT +', 'K-BERT', 'BERT- MCRF', 'BERT-PAIR', 'BERT-Large model', 'BERT-of-Theseus', 'multi-BERT', 'GAML-BERT', 'BERT-joint', 'BERT 2', 'BERT 6', 'SA-BERT', 'BERT-CLS', 'BERT large model', 'Bert-base', 'BERT-B', 'BERT-A', 'ULR-BERT', 'BERT-Q-a', 'BERT-Score', 'BERT QA', 'BERT-large-cased', 'BERT-base-cased', 'C-BERT', 'TOD-BERT-jnt', 'BERT-Tagger', 'SimCSE-BERT base', 'BERTbase model', 'DR-BERT', 'Legal-BERT', 'IB-BERT', 'BERT 4', 'BERT based models', 'BERT ( base )', 'bert-large-cased', 'BERT-base-cased model', 'bert-base-cased model', 'BERT teacher', 'UHD-BERT', 'BERT 5', 'BERT 3', 'BERT 12', 'BERT2BERT', 'MWP-BERT', 'BERT tokenizer', 'BERT+AM', 'BERT Model', 'BERT Large model', 'BERT-base architecture', 'BERT based model', 'BERT ( Bidirectional Encoder Representations from Transformers )', 'BERT-based approaches', 'BERT network', 'BERT-like model', 'BERT-space', 'Multi-BERT', '+BERT', 'BERT-fuse', 'BERT-HAE', 'BERT-ADA', 'BERT-DAAT', 'BERT-based architecture', 'BERT-based classifiers', 'BERT s', 'BERT-Multi', 'non-BERT models', 'BERT-KM', 'BERT B', 'BERT family', 'BERT-R', 'BERT QA model', 'Span-BERT', 'BERT-HA', 'BERT Base model', 'BERT classifiers', 'cross-BERT methods', 'BERT-fuse GED', 'Q-BERT', 'T-BERT', 'BERT-12-768', 'BERT-based QA model', 'XY-BERT', 'Bert-onehot', 'SPAN-BERT-SEQ', 'bert-large', 'BERT transformer', 'BERT-our', 'BERT-based approach', 'BERT-Q-t', 'ToD-BERT-ST', 'HIER-BERT', 'Joint-BERT', 'RSN-BERT', 'BERT-tagger', 'Sent-BERT', 'BERT [ CLS ]', 'Proto-BERT', 'BERT base architecture', 'BERT-classifier', 'BERT architectures', 'BERT-to-BERT', 'bert-large-cased model', 'BERT SP', 'BERT-YN', 'BERT tiny', 'BERT SV', 'Aff-BERT', 'FLANG-BERT', 'BERT-D', 'BERT-HA+STM', 'MD-Bert', 'EMAR+BERT', 'BERT-only', 'BERT classification', 'BERT- `', 'BERT+NROP', 'BERT C', 'BERT MORP HO', 'BERT-12', 'lite bert', 'BERT-style model', 'BERT-style', 'Bert-tiny', 'BERT-tiny', 'BERT-Q-q', 'scratch-BERT', 'BERT-MC', 'MC-BERT', 'BERT-S & T', 'BERT-encoded', 'BERT tagger', 'BERT IT', 'NSC-BERT', 'BERT-RCM', 'EN-BERT', 'BERT paper', 'BERT Score', 'BERT-ST', 'U2P-BERT', 'BERT-CCTK', 'BERT fooled', 'BERT CLS', 'TAS-BERT-ACOS', 'BERT-CFT', 'Bert model', 'Bert-large', 'BERT Models', 'BERT-Small', 'BERT base )', 'BERT-large models', 'BERT-based method', 'BERT-based classification model', 'vanilla BERT', 'L-BERT', 'DR-BERT model', 'BERT 1', 'IS-BERT base', 'BERT-b', 'FT-BERT', 'BERT-GRI3', 'BC-BERT MED', 'AC-MIMLLN-BERT', 'Spectral-BERT', 'MoCo-BERT', 'BERT-HIGRU', 'BERT-Tiny', 'BERT-345M', 'TFR-BERT', 'BERT ( Intra )', 'BERT B -single', 'BERT f', 'BERT-IT', 'BERT-FP', 'CaseLaw-BERT', 'BERT-GED ( JOINT )', 'TR-BERT 12', 'TR-BERT 6', 'BERT R', 'BERT-GRC', 'BERT-MCRF', 'BERT-Naive', 'BERT base -flow', 'IRNet ( BERT )', 'BERT-fuse mask', 'BERT-single', 'BERT-Sent', 'BERT+HAE', 'RUN-BERT', 'BERT-PROB', 'Bert-Base', 'BERT BASE architecture', 'BERT small', 'BERT Classifier', 'BERT *', 'base BERT', 'BERT approach', 'BERT-based architectures', 'BERT-IR', 'BERT-IR base', 'BERT FT C', 'BERT-ResNet', 'BERT c', 'LCF-BERT', 'IMN-BERT', 'Dual BERT', 'BERT-DeepConv', 'BERT-PHAE', 'BERT-RI3', 'CapsNet-BERT', 'GTS-BERT', 'PD-BERT', 'Arabic-BERT', 'BERT-ext', 'BERT ( a )', 'a-BERT', 'f-BERT', 'BERT-uc', 'R-BERT', 'SEQ2SEQ BERT', 'GRTE BERT', 'BERT 9', 'BERT oids', 'BERT-based APIs', 'BERT-GED ( SRC )', 'TwHIN-BERT * models', 'BERT-medium', 'BERT2SEQ', 'SPAN-BERT', 'AEN-BERT', 'BERT-M', 'BERT-only model', 'BERT-mix', 'BERTBASE', 'Bert-base model', 'base BERT model', 'BERT (', 'BERT-', 'BERT framework', 'BERT-cased', 'BERT-base/large', 'BERT-Base models', 'BERT BASE models', 'BERT-based systems', 'POWER-BERT', 'Slavic-BERT', 'BERT-VMASK', 'BERT-DExp', 'BERT LARGE +SSPT', 'BERT-AT', 'LIMIT-BERT', 'BERT code', 'BERT NOPT', 'BERT + know', 'BERT ( Q-q )', 'VGCN-BERT', 'BERT+TRF', 'LGAM-BERT', 'BERT-Direct', 'BERT-SRC', 'Google BERT', 'BERT B -mix', 'BERT F', 'BERT ( f )', 'NAS-BERT', 'BERT-GED ( HYP )', 'BERT+HLG', 'BERT score', 'BERT-masked', 'TwHIN-BERT', 'CT-BERT base', 'BERT-Base BSO', 'RUBER-BERT', 'BERT-pair', 'BERT-DM', 'PROTO-BERT', 'Aloe * +BERT', 'BERT-NS', 'BERTLARGE', 'BERTLarge model', 'BERTBase model', 'bert model', 'cased BERT model', 'BERT Base architecture', 'BERT ]', 'BERT-base )', 'cased BERT base model', 'BERTbase-cased', 'BERT base cased model', 'Bert-To-Bert', 'BERT-large )', 'BERT-based system', 'BERT IR', 'TPLinker BERT', 'BERT-like architectures', 'cross-BERT method', 'C2P-BERT', 'DAPT-BERT', 'BERT-CKY', 'BERT-S+R', 'BERT-base teacher', 'BERT 4 -PKD', 'BERT_QA_Arg', 'BERT IT : CLUST', 'BERT space', 'ADV BERT', 'non-BERT', 'stance-BERT', 'Flat-BERT', 'BERT-SEQ', 'BERT RoBERTa', 'BERT ( B )', 'BERT-LLTM', 'BERT-ID', 'BERT SV model', 'BERT Concat', 'BERT-derived', 'bert-large-wwm', 'BERT T', 'BERT-LSTMd', 'SMD + BERT', 'BERT-DEFAULT', 'RB-WAVG-BERT', 'bert-test', 'BERT Enc', 'BERT-M3T', 'D-BERT', 'BERT model 6', 'BERT 6 [ Large ]', 'GAIN-BERT base', 'Three-T BERT', 'BERT PIPE *', 'BERT-CHAIN', 'IS-BERT-task', 'WCN-BERT STC', 'BERT+RLHR', 'BERT ARES', 'BERT-24', 'BERT 24', 'SimCSE-BERT', 'BERT + L num', 'BERT+BLEU4', 'Blind-BERT', 'student BERT model', 'Student-BERT', 'BERT-WSD', 'Bangla-BERT', 'FT-BERT-KM', 'BERT Medium', 'BERT_QA', 'BERT-QA model', 'BERT 7', 'BERT-LWAN', 'BERT-SPC ∗', 'BERT-mini', 'BERT reranker', 'BERT zh', 'BERT-KCL', 'BERT BASE_CLS', 'BERT-Pair', 'BERT ∗', 'BERT-P', 'BERT · SC', 'BERT2DNN', 'BERTlarge model', 'BERT-models', 'Bert base', 'BERT-BASE model', 'BERT Transformer model', 'Bert base )', 'BERT-Large models', 'BERT module', 'BERT method', 'BERT classification model', 'BERT base models', 'BERT transformers', 'BERT ( large )', 'BERT-base classifier', 'BERT approaches', 'BERT systems', 'BERT networks', 'BERT system', 'BERT Indian', 'BERT-GR3', 'BC-BERT', 'BERT-Cross', 'BERT 6L/512H', 'PubMed-BERT', 'BERT BASE teacher', 'UIE-BERT', 'BERT-C', 'BERT-c', 'SMART BERT', 'BERT-Vanilla', 'VL-BERT model', 'BERT+L1', 'BERT L model', 'BERT model 3', 'SL-BERT', 'BERT+COS', 'lite BERT', 'BERT-1.2B', 'BERT ”', 'BERT+FUR', 'CA-SEP-BERT', 'BERT/RoBERTa models', 'BERT B -UDA', 'BERT ( FT )', 'KAM-BERT-Base', 'KAM-BERT-Large', 'BERT team', 'BERT+Flat', 'BERT-fix', 'BERT-fuse ( GED )', 'BERT fast', 'BERT slow', 'BERT+self-training', 'CoMN-BERT', 'ToD-BERT-jnt', 'BERT- β', 'BERT 6 -PKD', 'BERT-TC', 'BERT FT ST', 'BERT-both', 'BERT-HA+Rule', 'Bagging-BERT ( 2 )', 'LGAM-BERT models', 'BERT-DTC', 'BERT-MCL', 'TEMP-BERT', 'siamese bert-networks', 'Siamese-BERT', 'BERT ddrel', 'BERT model 2', 'BERT-SOCIAL IQA', 'BERT-T', 'BERT encoding', 'BERT-K', 'BERT-f', 'neg-bert-large', 'Bert-Tagger', 'BERT CONS', 'BERT-Linear', 'BERT_all', 'BERT-all', 'BERT-GEN', 'BERT-Gen', 'BERT 6 [ Base ]', 'BERT BASE 1M', 'CES-BERT', 'FT-BERT-ABS', 'BERT-based API', 'BERT pre', 'DAT BERT', 'BERT-GB-T1', 'BERT-NoDA', 'BERT PIPE', 'BERT l2r', 'BERT sm', 'BERT-TabGen', 'bert-as-service', 'BERT-SparseLT', 'TR-BERT L', 'BERT-DPT', 'DPT BERT', 'SCI-BERT', 'BERT SPARTUN-S', 'BERT SPARTUN', 'DiffCSE-BERT base', 'BERT-QA lo', 'shared-BERT', 'Time-BERT', 'BERT large 335M', 'CasRel BERT', 'Toxic-bert', 'BERT-Abs', 'ES-BERT', 'BERT-ES', 'BERT Plus', 'BERT-F1', 'BERT-Medium', 'RUBER u -BERT', 'CLRCMD -BERT base', 'BERT-based reranker', 'Ret-BERT', 'ZH-BERT', 'HI-BERT', 'Bert-cls', 'BERT cls', 'PL-BERT', 'BERT-prob', 'BERT P', 'Bert s p', 'BERT base ( 110M )', 'BERT BERT-NS', 'BERTbase models', 'BERT – Base', 'BERT based', 'BERT -based', 'Bert-based models', 'BERT-Based Model', 'Bert-based model', 'BERT-Classification', 'BERT Transformer', 'BERT based methods', 'BERT ( -base )', 'BERT ( Base )', 'BERT LARGE models', 'Bert-large models', 'BERT-Large-Cased', 'Bert-to-Bert', 'BERT-classifier model', 'BERT-small models', 'BERT-small model', 'BERT-S *', 'BERT methods', 'bert-base-cased )', 'BERT-based SOTA', 'BERT BERT', 'Bidirectional Encoder Representation from Transformers ( BERT )', 'BERT-sep', 'BERT * IR', 'BERT ( IR )', 'SOP-BERT', 'cross-BERT model', 'BERT-based PN', 'BERT-LogLP', 'BERT+LAMB', 'BERT teacher model', 'BERT-large teacher', 'IB-BERT teacher', 'BERT-base 109M', 'BERT & C', 'BERT-L', 'BERT-DOC-TOK', 'momentum BERT', 'BERT-multi', 'Non-BERT based models', 'BERT + char', 'GAML-BERT method', 'BERT MM', 'BERT BASE 12', 'BERT-LITE', 'BERT B -DANN', 'BERT style models', 'BERT model 1', 'BERT-based text classifier', 'RACL-BERT -', 'BERT+DAM', 'bert+', 'BERT FE ST+C', 'BERT b', 'BERT+WW', 'BERT tagging model', 'BERT-Trans', 'BERT-base ( Bb )', 'MoCo-BERT base', 'BERT-word', 'BERT joint', 'BERT-Joint', 'RUBER r -BERT', 'Tiny-BERT', 'BERT-mp', 'BERT ( Q )', 'BERT+MWA', 'BERT FT ST+C', 'BERT \\M', 'BERT sv', 'BERT-derived models', 'BERT+TBR', 'BERT+TB', 'BERT-lex', 'BERT-Sim', 'BERT-SIM', 'BERT PTM', 'BERT-Clause', 'BERT-Fuse', 'BERT-fused', 'ArcCSE-BERT base', 'ArcCSE-BERT', 'BERT-init', 'w/o BERT models', 'U-BERT', 'IB-BERT LARGE', 'UDA ( BERT )', 'BERT QAMR', 'BERT-2-128', 'BERT+ARE', 'BERT ( a ) models', 'PCL-BERT base', 'BERT token', 'BERT-based encoding', 'Bio-BERT', 'BERT-F', 'BERT GAP', 'BERT-Entity', 'BERT-entity', 'sec-bert-shape', 'DILR-BERT', 'BERT-Emo+NEP', '\ue005 BERT BASE', 'Bert embedding', 'BERT conv', 'BERT+AddTr', 'BERT + GENIA', 'BERT-PFT ( IR )', 'BERT-PFT', 'BERT-FP-NF', 'BERT-6L', 'BERT-INT', 'BERT 768', 'BERT p-w', 'BERT ReImp', 'ERICA BERT', 'BERT 8', 'BERT-Q-a model', 'BERT model 4', 'BERT-LC', 'SimCSE cls -BERT base', 'BERT-NTM', 'BERT-SparseLT model', 'BERT-SparseLT models', 'BERT-En', 'BERT-EN', 'BERT–1hop train', 'BERT+BCNN', 'BERT SPLIT', 'BERT-Flow', 'BERT-flow method', 'BERT-ED', 'DRLST-BERT', 'BERT4GCN model', 'CASREL BERT', 'BERT es', 'F1 BERT', 'BERT/GPT', 'TwHIN-BERT *', 'BERT-MCRF-training', 'BERT2SEQ model', 'BERT-LargeM', 'BERT 10', 'BERT-HA+Gold', 'BERT-for-QA model', 'QA-BERT', 'BERT-MP-512', 'BERT/SciBERT', 'BERT f-w', 'BERT-CT', 'CT-BERT', 'BERT-Summ', 'BERT-m', 'BERT mini', 'BERT-open', 'Bert-onehot model', 'BERT+Plane', 'BERT+GDIN', 'Hi-BERT', 'BERT-label', 'BERT-Sup', 'BERT- [ CLS ]', 'Bert-PAIR', 'Glyce-BERT', 'R u -BERT', 'Bertbase', 'BERTBASE model', 'Bert-Large', 'BERTbase/large', 'bert models', 'BERT -base', 'Bert_base', 'bert-base model', 'BERT transformer model', 'Bert-based', 'BERT-BASE architecture', 'BERT-based ) models', 'BERT-based Model', 'BERT based classifier', 'BERT/', 'BERT -', 'BERT - -', 'bert #', 'BERT—', 'BERT strategy', 'BERT BASE )', 'cased BERT', 'BERT ( Bidirectional Encoder Representations from Transformers', 'Bert architecture', 'BERT BASE network', 'BERT-LARGE-CASED', 'BERT-Cased', 'BERT cased', 'BERT Method', 'Encoder Representations from Transformers ( BERT )', 'BERT-Base/Large', 'BERT Base models', 'BERT-s', 'BERT-Our', 'Bert large )', 'BERT ( Large )', 'BERT LARGE architecture', 'BERT-large-cased model', 'BERT-base ( BERT', 'BERT ( BERT )', 'BERT-based network', 'BERT-based classification approach', 'BERT ( base-cased )', 'BERT-BERT', 'BERT based system', 'BERT- { base , large }', 'BERT cased model', 'Bert networks', 'Sep-BERT', 'cased BERT-base', 'BERT-based algorithms', 'BERT -based measure', 'BERT-large-cased models', 'BERT-to-BERT model', 'from Transformers ( BERT )', 'Base Model ( BERT ) Base Model', 'BERT-based metrics', 'BERT model architecture', 'BERT-based modules', 'BERT classification framework', 'BERT-cased models', 'BERT-enhanced', 'BERT-enhanced models', 'TPLinker ( BERT )', 'BERT-like architecture', 'BERT-Serini', 'BERT-AFS', 'BERT-AFS-base', 'BERT-AFS-large', 'BERT-based pointer network', 'Pointer-based BERT base', 'Pointer-based BERT large', 'BERT+Flair', 'BERT FulPri', 'BERT-Base teacher', 'BERT/XLNet', 'BERT XLNET', 'BERT XLNet', 'MULT-BERT', 'Covid-BERT', 'Centroid BERT', 'Sense-BERT', 'BERT emb', 'IB-BERT LARGE teacher', 'BERT | C', 'BERT Vanilla', 'vanilla-BERT', 'vanilla BERT model', 'Vl-bert', 'BERT COCO', 'BERT + CTED', 'BERT-class models', 'BERT + [ CLS ] + CTED', 'BERT-l', 'BERT l', 'BERT BERT l', 'BERT-DOC-TOK-SEG', 'Momentum BERT', 'de-BERT', 'GCDT + BERT', 'BERT BASE +SSPT', 'BERT-large 3', 'BERTbase model 3', 'BERT-MULTI', 'BERT-MULTI model', 'non-BERT methods', 'non-BERT-based models', 'BERT-based DR', 'BERT+HiAGM', 'HATN-BERT', 'DSSM ( BERT', 'BERT-SL', 'BERT-CX', 'BERT BASE 12 model', 'BERT-based ASC models', 'SCD-BERT base', 'BERT-base+SMS', 'NCBI_BERT', 'BERT large -LITE', 'BERT-based planner', 'BERT-Flat', 'Legal BERT', 'LEGAL-BERT', 'BERT BASE model 1', 'BERT F1 # Q', 'BERT Surgeon', 'LT-BERT', 'BERT-l-u-4', 'BERT/Roberta', 'Bert/RoBERTa', 'BERT/RoBERTa model', 'BERT B model', 'BERT serving', 'BERT “ base ” model', 'GTT ( BERT )', 'BERT-FT model', 'BERT FT', 'BERT-FT.', 'BERT-TAPT', 'SDGCN-BERT', 'English BERT model', 'English-BERT', 'bert2BERT method', 'BERT+CLIP', '–KAM-BERT', 'dual BERT model', 'supervised BERT model', 'supervised-BERT', 'BERT+FLAT', 'BERT 11', 'BERT-adapted models', 'BERT-adapted Models', 'Adapt-BERT', 'BERT ( adapt )', 'BERT Enc-Dec', 'Joint-BERT model', 'BERT-family models', 'BERT + M + P', 'BERT L task', 'BERT - MP', 'BERT-MP', 'sec-bert', 'BERT-Large 345M', 'TFM : BERT', 'BERT-TFM', 'BERT-Emo', 'ens+rs BERT', 'BERT-tickets', 'VGCN-BERT model', 'BERT-Concat', 'BERT–concat model', 'BERT-Concat system', 'head BERT', 'Bert-Dropout', 'BERT-β', 'BERT+TRF *', 'BERT Lex', 'BERT im', 'Tiny-BERT 6', 'BERT + OC', 'bert +', 'BERT-Large +', 'BERT + gold', 'Ours+BERT', 'S + BERT', 'BERT-wwm model', 'BERT SAFER', 'BERT-large ( Bl )', 'BERT ( Inter )', 'BERT-fused model', 'BERT-Tree', 'BERT Learning', 'BERT-4-512', 'BERT base +KM', 'BERT+INC', 'BERT u', 'BERT-ConvE', 'BERT mdoel', 'Siamese-BERT architecture', 'BERT/BERT QAMR', 'BERT-based toxicity classifier', 'BERT base model 2', 'BERT-base 2', 'BERT t', 'BERT-TS', 'RGAT-BERT', 'DGEDT-BERT', 'BERT-based token', 'BERT token classifier', 'BERT + Social', 'BERT k', 'BERT-DK', 'CLS + BERT', 'BERT ( text view )', 'BERT-B ft', 'BERT-L ft', 'RSN-BERT )', 'BERT-FT SWAG', 'SPN BERT', 'BERT-based tagger', 'BERT+FT', 'BERT-QE', 'JET-BERT', 'BERT training method', 'BERT training data', 'Bert-noDP', 'BERT Embedding', 'Bert Embedding', 'BERT embedding model', 'BERT-based SUMBT model', 'CPFT-BERT', 'BERT+F OP', 'BERT+F RV', 'D BERT', 'BERT-NROP', 'BERTlarge 6', 'V+L BERT model', 'BERT w/', 'BERT w.', 'BERT ( s w )', 'w/ BERT', 'BERT ( z )', 'GAIN-BERT', 'BERT+unit', 'BERT WSCR', 'Pre-BERT', 'Pre-BERT Base', 'BERT ONE', 'BERT-Full', 'BERT full', 'BERT Cased Tokenizer', 'BERT ( STL )', 'BERT-STL system', 'Seq2Seq-BERT', 'SIRE-BERT', 'BERT ( Q-a )', 'BERT-RCM model', 'BERT-base-cased 4', 'BERT SMALL 4', 'BERT-base 4', 'TAS-BERT', 'LC-BERT', 'BERT-clip', 'BERT-base 9', 'BERT-base + topic model', 'HR-BERT', 'Sci-BERT', 'BERT-Sci', 'BERT-24 models', 'Bert Score', 'BERT scores', 'BERT large -flow', 'BERT-based ED models', 'BERT 6 -FT', 'BERT 24 BERT 6', 'EmbSum-BERT', 'BERT + Adv', 'MMT ( BERT-Base )', 'BERT-based settings', 'MWP-BERT solver', 'SQSP-BERT', 'SQSP-Bert', 'BERT 12 ( Teacher )', 'BERT 12 teacher', 'BERT masking', 'Ger-BERT', 'BERT student', 'BERT-base student', 'BERT-based WSD model', 'Bangla BERT', 'BERT i2b2', 'BERT-Based CasRel', 'CASREL BERT models', 'TSLM BERT', 'BERT+SG', 'Toxic-bert classifier', 'NLU BERT-based model', 'BERT-medium model', 'LSR-BERT base', 'BERT-SeqXdoc', 'CLRCMD-BERT base', 'SO BERT', 'BERT input', 'BERT2Tag', 'BERT-base QA model', 'BERT-template', 'Pixel-BERT', 'BERT base ‡', 'BERT sent model', 'Bert2Rnd', 'BERT-GQE', 'BERT-KPE', 'BERT-Of-Cloze', 'BERT-based post training', 'BERT + Rand', 'BERT−base', 'BERT Reranker', 'BERT ( SPARTQA-AUTO )', 'BERT + GSAMN', 'Ruber-bert', 'bert-base-zh', 'BERT-base-zh', 'BERT O', 'BERT-FT-M-1', 'BERT-SA', 'BERT-FT-M-5', 'Hi-BERT model', 'BERT BASE -CR', 'BERT-only models', 'BERT ( GPT-2', 'BERT Base FLOPs', 'BERT-Pair system', 'BERT-pair model', 'BERT-pair models', 'TFMR +BERT', 'Proto-ADV ( BERT )', 'BERT2STATIC', 'BERT-FP-LBL model', 'BERT-FP-LBL', 'BERT p', 'BERT Adam optimizer', 'Proto ( BERT )', 'BERT-base 110M', 'BERT QG', 'BERT-esque model', 'BERT-8-512', 'BERT ( · )', 'BertLarge', 'bERT', 'BERT ) model', ""BERT '' model"", 'BertBase model', 'BERT-model', 'BERT_large', 'BERT-LARGE MODEL', 'BERT-LARGE model', 'Bert-Large model', 'bert-large model', 'Bertbase models', 'BERTlarge models', 'BERT ) models', 'BERT_base', 'BERT- { base', 'BERT- base', 'BERT BASE -based models', 'BERT-base based models', 'Bert-Base model', 'BERT Base ) model', 'BERT ( base ) model', 'BERT Transformer Model', 'Bidirectional Encoder Representations ( BERT ) model', 'Cased BERT model', 'BERT-Base architecture', 'base # BERT model', 'bert-based models', 'BERT-based Models', 'BERT-SMALL', 'BERT SMALL', 'BERT-based Classifier', 'bidirectional Transformer BERT', 'Bert classifier', 'BERTLarge (', 'BERT :', 'bert )', 'BERT ( )', 'bidirectional encoder representations from Transformers ( BERT )', 'BASE BERT', 'Base BERT', 'BERT-Transformer', 'Bert-based methods', 'BERT base ( )', 'BERT ( BASE )', 'BERT base (', 'BERT-Base )', 'BERT-base (', 'BERT-base ( )', 'BERT-large Models', 'BERT base network', 'BERT based approaches', 'BERT LARGE_CASED', 'Bert-large-cased', 'BERT LARGE-cased', 'BERT-Large-cased', 'cased BERT-base model', 'BERTBase-Cased', 'BERTlarge-cased', 'Bert-cased', 'BERT Module', 'bert Method', 'BERT corpus', 'bert Corpus', 'BERT-Base Cased model', 'BERT-Base-cased model', 'BERT-Base-Cased model', 'BERT Base Cased model', 'BERT-Base cased model', 'BERT-base cased model', 'BERT Base & Large', 'BERT base/large', 'BERT ( base ) models', 'BERT-BASE models', 'Bert-base models', 'bert-base models', 'BERT Classifiers', 'bidirectional Transformer ( BERT )', 'BERT our', 'BERT ( LARGE )', 'BERT Large )', 'BERT-Large )', 'BERT-base ( BERT )', 'BERT BASE ( BERT )', 'Bidirectional Encoder Representations from Transformers ( BERT ) model', 'BERT ( Bidirectional Encoder Representation from Transformers ) model', 'BERT large architecture', 'BERT Dev', 'BERT-large—in', 'BERT-Large-Cased model', 'BERT large-cased model', 'BERT large cased model', 'BERT based method', 'Ours ( BERT )', 'BERT BASE classifier', 'BERT base classifier', 'BERT layers', 'BERT Layers', 'BERT model )', 'BERT-Base BERT', 'BERT-based classifier models', 'BERT based classifier model', 'BERT-based Classifier Model', 'BERT classifier model', 'Bert classifier model', 'BERT-Base-Cased', 'Bert-base-cased', 'BERT-Base_Cased', 'BERT-Base Cased', 'BERT base-cased', 'BERT S model', 'BERT ( dev )', 'BERT-Large classifier', 'BERT-Large classifiers', 'Bidirectional Encoder Representations ( BERT )', 'BERT SMALL model', 'B ERT', 'LARGE BERT', 'BERT ( Bidirectional Encoder Representation from Transformer )', 'BERT ( S )', 'BERT-network', 'Bert-based module', 'BERT Transformer layers', '-BERT', 'BERT Data set', 'BERT ( large , cased )', 'BERT-large ( cased )', 'bidirectional encoder representation for Transformers ( BERT )', 'BERT-base architectures', 'BERT-base ( cased )', 'BERT cased large model', 'BERT-based classification models', 'bert-base-cased Method', 'BERT-based (', 'BERT-based )', 'BERT based )', 'bert-based-cased model', 'BERT ( BERT BASE )', 'bertbase-cased model', 'BERTbase-cased model', ""BERTBASE-CASED '' model"", 'BERT-cased model', 'BERT- [ base/large ] models', 'BERT transformer network', 'BERT base model ( BERT base', 'Model - BERT-Large ( SOTA )', 'BERT-cased large', 'BERT-cased-large', 'BERT ( BERT-large )', 'BERT analysis', 'cased BERT Base', 'cased BERT BASE', 'BERT algorithm', 'BERT algorithms', 'Bidirectional Encoder Representations from Transformers', 'BERT-based framework', 'BERT-base BERT-large', 'BERT ( Bidirectional Encoder Representations from Transformers ) model', 'BERT-of', 'BERT-based classification', 'BERT-to-BERT models', 'bert_metric', 'B ERT model', 'BERT ( Ours )', 'bidirectional encoder representation from transformers', 'Transformer encoder ( Bert )', 'BERT Setting', 'base BERT cased model', 'BERT Transformer models', 'Representation from Transformers ( BERT )', 'BERT Dataset', 'small BERT model', 'Bidirectional Encoder Representations from Transformers ( BERT', 'BERT-Gold', 'Bert ( large cased model )', 'BERT System', 'BERT-based techniques', 'BERT-based SOTA model', 'BERT ( -Base/Large ) model', 'BERT classification models', 'BERT-base-case', 'BERT-Base ( ours )', 'BERT-bases', 'Bert layer', 'BERT base base models', 'BERT Measure', 'Bidirectional Encoder Representation Transformer', 'BERT-based model approaches', 'BERT data', 'BERT bert-base-cased', 'BERT ( cased )', 'BERT-based SOTA architectures', 'BERT-base model Model', 'BERT Base Methods', 'BERT modules', 'BERT-cased base', 'BERT-enhanced methods', 'BERT -based IR', 'BERT-IR base models', 'BERT-large+ITPT', 'BERT+FL', 'BERT tailors', 'BERT-TableGen', 'BERT-TabGen 1', 'BERT-Rerank', 'BERT + Transformer + mixed training', 'BERT ADReSS', 'TPlinker BERT', 'TPLinker BERT model', 'BERT like', 'BERT-like approach', 'BERT-like methods', 'PFN BERT', 'AR-BERT T-IDENT', 'ZH-BERT T-IDENT', 'AR-BERT', 'PoWER-BERT model', 'BERT-powered', 'BERT-SOP', 'BERT BPEs', 'BERT-Cross BERT', 'BERT-cross', 'BERT-based cross', 'BERT-Base DOT', 'BERT-Dot', 'BERT-based Pointer Network', 'Pointer-based BERT', 'BERT-Transformer-Pointer', 'BERT+Flair ]', 'BERT work', 'BERT-based work', 'DGEDT +BERT', 'BERT BASE + MDA', 'BERT ResNet', 'BERT-ResNet model', 'BERT+LAMB model', 'BERT + Base + Social', 'Tod-BERT', 'ToD-BERT models', 'TOD-BERT model', 'BERT-Large AF', 'RB-BERT', 'BERT +toks', 'bert pubmed', 'adv-bert 1x', 'BERT BASE teacher models', 'teacher model ( BERT )', 'BERTlarge teacher', 'BERT teachers', 'BERT LARGE teacher', 'BERT-Large teacher', 'BERT BASE teacher model', 'BERT teacher models', 'teacher BERT model', 'BERT-SeqWD', 'BERT-base 5', 'BERT LARGE 5', 'BERT-large 5 models', 'BERT classifier 5', 'L-BERT row', 'BERT-base-xxl-it', 'BERT-retain', 'BERT [ SLOR ]', 'BERT/emb', 'BERT LARGE ( 340M )', 'BERT-large 340M', 'BERT-LARGE 340M', 'BERT BASE 109M', 'BERT BASE ( 109M )', 'BERT @ C', 'BERT | C.', 'BERT ( C )', 'BERT c )', 'BERT-c.', 'BERT ( c )', 'BERT C )', 'c-BERT', 'BERT ( C-BASED )', 'C-BERT )', 'BERT-c models', 'BERT BASE model ( SMART BERT )', 'BERT_QA_Arg model', 'vanilla BERT architecture', 'Vanilla BERT', 'Vanilla-BERT', 'BERT-base Vanilla', 'VL-BERTlarge model', 'BERT-based VL models', 'BERT+L1 models', 'BERT-of-Theseus †', 'BERT-based CoQA model', 'BERT-class', 'BERT-class model', 'BERT class models', 'BERT-based textual models', 'BERT ( L )', 'BERT Large ( BERT L )', 'BERT-DOC-TOK-SEG model', 'Student - BERT 6', 'BERT 6 students', 'BERT 6 [ Large ] student', 'BERT ( De', 'BERT-de', 'BERTbase-de', 'BERT de', 'BERT-large ( De', 'BERT-base ( De', 'GCDT + BERT LARGE', 'space - - BERT', 'BERT Space', 'BERT-space—our', 'BERT network using siamese', 'BERT-based SpanRel models', 'ADV BERT method', 'BERT ( Adv )', 'Bert-of-Theseus', 'BERT-of-Theseus model', 'BERT MedM', 'BERT-Large itself', 'BERT_3', 'BERT ( 3 models )', 'BERT-base model 3', 'BERT BASE model 3', 'BERT Base 3', 'BERT BASE 3', 'BERT-base 3', 'BERT-Base 3 model', 'BERT-base 3 model', 'BERT multi ( cased )', 'BERT ( Multi-BERT ) models', 'Multi-BERT models', 'LCF-BERT model', 'non-BERT-based systems', 'non-BERT-based', 'non-BERT system', 'Non-BERT Models', 'Non-BERT classifier', 'Non-BERT approaches', 'Non-BERT-based models', 'BERT + char models', 'GAML-BERT )', 'GAML-BERT ( ours )', 'GAML-BERT framework', 'GAML-BERT model', 'Beam search-BERT BASE', 'BERT AG', 'BERT/AG', 'BERT-based Swift model', 'BERT-large ( offline ) model', 'HATE-BERT', 'In-House-BERT', 'BERT DSSM', 'BERT skyline', 'skyline BERT model', 'RYANSQL + BERT †', 'IMN-BERT model', 'BERT-MM', 'Bert ( no-target )', 'CNet BERT model', 'BERT model 12', 'BERT-large 12', 'BERT BERT BASE 12 model', 'Stance-BERT', 'BERT-ASC', 'BERT-ASC *', 'BERT-based ASC', 'BERT joint I', 'BERT-large+SMS', 'BERT base -LITE', 'GLM ( BERT-base )', 'GLM ( BERT-large )', 'BERT-based Planner', 'BERT-Granu', 'RikiNet-BERT large', 'BERT-Style Models', 'BERT-style methods', 'BERT style model', 'BERT-style approaches', 'BERT-style bidirectional Transformer encoder', 'BERT-style architectures', 'BERT · SC+', 'LIMIT-BERT training approaches', 'BERT-SEQ )', 'BERT_Seq', 'LEGAL-BERT-SMALL', 'BERT ( base ) models 1', 'BERT_1', 'BERT * 1', 'cased BERT-base 1 model', 'BERT-Base 1 model', 'BERT BASE 1 model', 'BERT-base model 1', 'BERT-base 1', 'BERT BASE 1', 'BERT 1 model', 'BERTLARGE 1 model', 'bert-base-cased 1', 'cased BERT-base 1', 'text BERT model', 'BERT based text classifier', 'BERT-based text classification approach', 'BERT no-ft', 'BERT from scratch ( SCR )', 'BERT training corpus 4', 'BERT SN', 'BERT BASE Reverse', 'BERT BASE -Reverse', 'SSAN-BERT base', 'BERT SP method', 'Eider-BERT base', 'BERT-Fuse †', 'Yahoo BERT', 'BERT-GED', 'SAIS O -BERT base', 'Q-BERT W8A8 QAT ψ', 'BERT-small ” model', 'BERT-based ”', 'BERT-to-BERT ” model', 'bert-base-cased ”', 'BERT-based IE system', 'BERT+DAM model', 'IS-BERT model', 'BERT BASE —is', 'bert-score 11', 'BERT-based text toxicity classifier', 'BERT-based topic models', 'BERT topic', 'BERTlarge topic', 'BERT-base topic', 'BERT-large topic', 'Topic-BERT framework', 'Topic-BERT approach', 'BERT-based CA', 'EMT BERT SKPEMT', 'BERT/ RoBERTa', 'BERT-base ( RoBERTa-base )', 'BERT/RoBERTa-based models', 'BERT/RoBERTa-base', 'RoBERTa/BERT-base model', 'Roberta/BERT', 'BERT/RoBERTa-Large', 'BERT B ( Base )', 'BERT ( b )', 'B.BERT-Base', 'BERT B -s', 'BERT Base ( BERT B )', 'BERT Tagging model', 'BERT tagging models', 'BERT Tagging', 'tions from Transformers ( BERT )', 'BERT “ base ”', 'BERT_Seq ( chunk )', 'BERT-ft model', 'BERT ft', 'BERT-ft )', 'FT/BERT', 'BERT-based encoder ( BERT-FT )', 'BERT SPARTQA-A', 'BERT-Trans model', 'BERT-Trans ( Ours )', 'BERT-based Trans models', 'BERT-based Trans model', 'BERT NOPT models', 'BERT supported', 'BERT -based SRol models', 'BERT English', 'BERT2BERT-based models', 'BERT2BERT model', 'BERT2BERT approach', 'BERT+CLIP model', 'BERT-based local classifier', 'local BERT model', 'BERT-local', 'BERT-YN model', 'KAM-BERTLarge', 'KAM-BERT model', 'KAM-BERT architecture', 'KAM-BERT framework', 'KAM-BERT-Base ( Ours )', 'BERT-based symptom classifiers', 'BERT ( M simple', '+BERT base RNGTr', 'BERT-LLTM model', '+XLNet large +BERT', 'BERT 16', 'BERTLARGE16', 'BERT+All', 'BERT+News', 'Dual BERT model', 'dual-BERT architecture', 'BERT-large GAIL', 'BERT – 512', 'BERT-base 512', 'BERT-large 512', 'GRN + BERT', 'ternary BERT model', 'BERT+WR', 'HIER-BERT )', 'BERT word )', 'BERT-based word', 'BERT-based supervised classifier', 'Supervised-BERT', 'Supervised BERT', 'supervised BERT classifier', 'BERT model 11', 'BERT-11', 'BERT-based SED model', 'Custom ( BERT )', 'BERT-Joint )', 'BERT-based DocIR', 'BERT time of day model', 'CTRL_VL-BERT', 'BERT family models', 'fixed BERT model', 'BERT-Fixed', 'BERT fixed', 'XL-BERT )', 'XL-BERT', 'BERT-flow ( target )', 'BERT-FT+', 'BERT-TAPT-FT+', 'BERT simple', 'RyanSQL + BERT', 'BERT-tiny network', 'BERT ( NS-Solver + BERT', 'BERT Tuning methods', 'BERT-Tuned', 'BERT-Tune', 'BERT tuned models', 'BERT-fused 8', 'MP-BERT', 'BERT-Q-q model', 'BERT-GRI20', 'BERT-345M models', 'BERT345M model', 'BERT FT ST+C ) model', 'BERT FT ST+C )', 'BERT ( BERT FT ST+C', 'BERT ID', 'BERT-ID )', 'BERT ID *', 'BERT + clustering', 'BERT + DBSCAN', 'BERT NBCL', 'BERT UACL', 'BERT SSCL', 'ens+rs BERT )', 'BERT_JSNLI', 'BERT + P', 'BERT + P BERT', 'BERT + PS', 'BERT tickets', 'BERT SV models', 'BERT-Chunking-KPE', 'F BERT Empathy', 'MWP-BERT+A & D', 'MWP-BERT+A', 'BERT–concat', 'BERT-concat', 'BERT Concat model', 'BERT-Concat model', 'Concat BERT', 'BERT heads', 'MWP-BERT+D', 'BERT 6 -PKD-Skip', 'Dropout BERT models', 'bert-dropout', 'BERT solver', 'BERT-based solvers', 'BERT-HA ( iter 0 )', 'BERT PKD', 'PKD-BERT', 'BERT-PKD model', 'BERTβ', 'BERT- β algorithm', 'scratch-BERT model', 'BERT-Abs-PG ”', 'BERT lex', 'BERT-Base Cased Lex', 'base BERT im', '+BERT-base', '+BERT BASE', 'BERT LARGE+', 'BERT LARGE+ model', 'BERT-base+Ours', '+BERT +BERT +BERT', '( +BERT', 'Table-BERT algorithm', 'Table-BERT model', 'Table-BERT Table-BERT', 'BERT + STRM', 'BERT-WWM', 'Bert-wwm', 'BERT BERT-wwm', 'BERT-large-wwm', 'WWM-BERT-Large', 'WWM-BERT', 'BERT+Aug', 'BERT FT ST )', 'BERT DR - 8 - DR', 'BERT-9L', 'SG-OPT-BERT base', 'SimCSE-BERT base ♢', 'TinyBERT ( T-BERT )', 'BERT-sim', 'TASE BIO +SSE ( BERT LARGE )', 'German BERT 4', 'BERT Dec.', 'BERT Dec. model', 'BERTlarge ( Bl )', 'BERTlarge ( -BL )', 'bert-large-cased ( BL )', 'BERT + lem', 'BERT-Un', 'BERT ( or RoBERTa ) model', 'BERT + CDA O', 'BERT-fuse )', 'BERT Fuse', 'BERT-fuse approach', 'topic-informed BERT-based architecture', 'BERT -based model ( tBERT )', 'ArcCSE-BERT large', 'BERT ( SUM-QE )', 'Res-BERT+BL model', 'Single-task ( BERT-FT-S-1 )', 'BERT-Base-Cased English model 1', 'BERT-based Trees', 'BERT-Tree )', 'BERT-PT SE', 'ViL-BERT', 'BERT learns', 'BERT learning', 'EASE-BERT', 'EASE-BERT base', 'BERT ( 4/512 )', 'BERT4-512', 'BERT-init model', 'w/o BERT', 'BERT-term', 'BERT HATN-BERT BERT-AT', 'BERT-SRC model', 'BERT-U', 'u -BERT', 'BERT ( U-BERT )', 'BERT BASE IB-BERT', 'BERT + SAPBERT', 'BERT+D )', 'BERT+D', 'BERT-Telugu ( BERT-Te )', 'BERT channel', 'BERT-based intent classifier model', 'BERT intent classifier', 'BERT-B * MPPI', 'BERT-B MPPI', 'BERT Ori .', 'BERT Ori', 'BERT B -single ” method', 'BERTbase-arabic', 'Arabic BERT models', 'Arabic-BERT model', 'BERT ROUGE', 'BERT reward', 'Siamese BERT networks', 'siamese BERT networks', 'siamese BERT', 'Siamese BERT', 'Siamese-BERT model', 'Siamese BERT model', 'siamese BERT model', 'Siamese BERT network', 'siamese BERT models', 'BERT-based siamese-network models', 'BERT-Siamese architecture', 'MPU BERT', 'BERT-wwm-ext teacher model', 'BERT no init ”', 'BERT no init', 'BERT se', 'Bert-SumExt', 'BERT toxicity classifier', 'BERT2', 'BERT- ` 2', 'BERT-Base model 2', 'BERTbase model 2', 'BERT Base 2', 'BERT LARGE 2 model', 'Cased BERT-base 2', 'BERT LARGE 2', 'BERTbase 2 model', 'BERT-large model 2', 'BERT 13', 'BERT BERT-ext', 'Toxic-bert 2', 'Student - BERT 4', 'BERT-DDP', 'BERT TS', 'BERT-S & T )', 'BERT-S & T models', 'BERT ( T )', 'BERT ( A )', 'BERT ( a ) -based', 'PCL-BERT', 'BERT pooler', 'BERT ( RuBERT )', 'SPC-BERT', 'BERT-SPC model', 'BERT-L-1shot', 'BERT-L-3shot', 'BERT Tokens', 'Table-BERT w/ verb', 'BERT-PROB axis', 'BERT-encoding', 'BERT base encoding', 'K-bert', 'K-BERT model', 'Encoder Representation from Transformers ( K-BERT )', 'BERT Large GSAMN', 'BERT-PROB axes', 'PoWER-BERT searchs', 'BERT ( f-BERT )', 'CGSN ( BERT )', 'BERT ( I gnd', 'FLANG-BERT ( ours )', 'FLANG-BERT model', 'AUG-BERT', 'BERT Aug.', 'BERT Aug', 'BERT-wmm', 'BERT-wmm model', 'BERT LARGE search agent', 'BERT BASE search agent', 'BERT-based API service', 'BERT-based API services', 'ULR-BERT BASE', 'ULR-BERT LARGE', 'Bert-Entity', 'BERT-entity model', 'BERT Tagger', 'bert-tagger', 'BERT based tagger', 'BERT-Tagger model', 'BERT taggers', 'BERT-tagger method', 'TR-BERT No', 'BERT-DILR models', 'default BERT model', 'BERT/TinyBERT', 'BERT-CRFbased', 'BERT2Rank * *', 'BERT2Chunk * *', 'BERT mini4/small4', 'BERT con', 'BERT-based SMART-KPE', 'BERT2Joint', 'Span Span BERT BERT ENT', 'BERT-linear classification models', 'BERT QE model', 'BERT-QE model', 'JET-BERT ]', 'BERT+EE', 'BERT-large trained', 'BERT Train', 'BERT trains', 'BERT-Large training', 'Bert-noDP method', 'BERT Target', 'BERT ( Training-Obj )', 'bert-embedding package', 'bert-embedding/', 'BERT-embedding-layer', 'BERT embedding ( BERT )', 'BERT Embedding-based Method', 'BERT embedding-based methods', 'embedding-based BERT models', 'BERT-based embedding', 'BERT-based SUMBT models', 'BERT it', 'BERT B mul +', 'light BERT models', 'BERT-BASE-KAGNET', 'BERT-LARGE-KAGNET', 'BERT ALL', 'BERT ALL model', 'BERT-based scorer', 'schuBERT BERT', 'BERT-LLTM ( JOINT ) model', 'BERT 99M 88M', 'BERT+ Sample', 'BERT ( D BERT )', 'Bert 6', 'BERTBase model 6', 'BERT models 6', 'Bert-Large 6', 'BERT ( BERT 6 )', 'BERT-base model 6', 'BERT classifier 6', 'BERT 6 models', 'BERT 6 model', 'BERT 6 [ Large ] model', 'BERT 6 [ Base ] model', 'bert-base 6', 'BERT base 6', 'NSC-BERT model', 'BERT H L', 'DefSent-BERT-base', 'DefSent-BERT-large', 'Mini BERT GL', 'BERT-Disc', 'BERT-Disc -', 'BERT-base-uc', 'DCRAN-BERT base', 'DCRAN-BERT large', 'BERT base + SAGE', 'BERT w', 'w/ BERT BASE', 'w/ BERT LARGE', 'GIN-BERT', 'Non-BERT ACSA models', 'BERT-FPNF', 'BERT-based reranking algorithm', 'Bert+LUA', 'Bert+TLUA', 'Bert+FRA', 'BERT-Cosine', 'Bert-xml', 'BERT/RoBERTa-large as', 'bert i', 'GAIN-BERT large', 'BERT ( ‘ teacher ’ ) model', 'BERT-base WSCR', 'FT-BERT-EXT', 'pre-BERT models', 'pre-BERT', 'BERT-based ones', 'BERT-base one', 'BERT TS ( FIGER )', 'DocRED BERT', 'BERT ( Token-CLS )', 'BERT CLS-token', 'trail BERT', 'BERT-R model', 'fifin-bert', 'BERT w/ P', 'BERT-tokenizer', 'Bert tokenizer', 'Bert-Tokenizer', 'BERT Tokenizer', 'BERT-base-cased tokenizer', 'BERT cased tokenizer', 'BERT-base tokenizer', 'BERT hyp', 'bert-shape', 'BERT-STL', 'STL-BERT', 'SEQ2SEQ BERT model', 'BERT-based seq2seq model', 'BERT oid UUAS', 'BERT-driven system', 'BERT-driven', 'BERT BASE 8 model', 'BERT-base model 8', 'BERT base model 8', 'bert-base-cased 8', 'BERT 8-8-8', 'BERT ( # 8 )', 'BERT-base 8', 'BERTbase-cased 8 model', 'streaming BERT model', 'Streaming BERT Model', 'WCN-BERT STC model', 'BERT ( SPARTQAAUTO )', 'GRTE BERT model', 'BERT , 4', 'BERT-4', 'BERTbase model 4', 'BERT BASE model 4', 'BERT-base model 4', 'BERT-Base architecture 4', 'BERT ( BERT 4 )', 'bert-large-cased 4 model', 'BERTbase-cased 4', 'BERT 4 model', 'BERT ( CC )', 'BERT CC )', 'BERT CC', 'space UUE - BERT', 'LC-BERT *', 'BERT content model', 'BERT-based BIO tagging model', 'BERT-GRC model', 'bert-as-a-service', 'BERT-jnt', 'BERT size', 'BERT BASE size', 'BERT base size', 'BERT-size model', 'BERT-sized models', 'Model Ours ( BERT ) CIDEr-D', 'BERT/CLIP', 'CLIP BERT', 'CLIP-BERT', 'BERT-oriented analysis method', 'BERT base model 9', 'BERT ( base ) 9', 'Coref-BERT', 'BERT-toBERT', 'BERT-GPT-TAPT', 'En-BERT', 'BERT en', 'BERT-En )', 'BERT ( en )', 'BERT en )', 'BERT model ( En-BERT )', 'BERT ( En-BERT )', 'EN BERT-Large', 'EN BERTBase architecture', 'BERT DPT', 'Bert-Mixup', 'F BERT score', 'SA-EXAL 4 BERT', 'Sci-BERT models', 'C2F-GPT-BERT', 'GCDT w/ BERT', 'SimCSE-BERT_ base', 'Bert score', 'BERT SCORE', 'BERT-scores', 'bertbase series models', 'BERT flow', 'BERT-flow models', 'BERT-flow *', 'scratch-BERT STL', 'BERT base VUA', 'BERT-based guesser', 'scatch-BERT', 'BERT-Emo + NEP', 'BERT-related methods', 'BERT-related model', 'DiffCSE-BERT', 'BERT IRNet', 'IRNet IRNet ( BERT )', 'BERT-Blind', 'Static BERT', 'BERT ( static )', 'RYANSQL V2 + BERT', 'BERT TacoLM', 'shared BERT model', 'shared Bert model', 'BERT+LSR', 'BERT Results', 'BERT-base+Self', 'Bert s p , s u model', 'BERT-FirstP', 'Time-BERT-large', 'BERT/P2V', 'BERT-PT models', 'BERT4GCN framework', 'BERT+P )', 'AMNM bert-bert ( g dot )', 'Teacher - BERT 12', 'BERT-Mask', 'BERT ( Mask )', 'BERT masking strategy', 'BERT refined-reward', 'Ger-BERT )', 'Ger-BERT model', 'CLUST BERT BERT', 'BERT Embedding Space', 'BERT WSD systems', 'BERT-based WSD approach', 'BERT WSD models', 'BERT p + Vis sec-clu', 'BERT 20NG 20', 'BERT-DAAT model', 'BERT-fuse Mask', 'Toxic-bert model', 'Bidirectional Encoder Representations from Transformers ’ ( BERT )', 'BERT ’ s classifier', 'Bert-base ’ model', 'BERT ’ s model', 'BERT ’', 'Bert- ’', 'BERT fooled training set', 'BERT based NLU model', 'BERT NLU model', 'BERT fooled dev set', 'BERT F 1', 'F 1 BERT', 'F1 BERT F1', 'BERT-base ) F1', 'BERT-NLISTSb-base model', 'Sen-Bert', 'BERT-GPT GPT', 'GPT/BERT', 'GPT/BERT-LARGE', 'TwHIN-BERT models', 'TwHIN-BERT model', 'BERT medium', 'BERT-MEDIUM', 'Self-Training BERT )', 'LSR-BERT', 'RUBER u BERT', 'WCN-BERT + STC', 'BERT-single Models', 'BERT-single models', 'BERT-single layer', 'BERT-single model', 'BERT SO', 'BERT2Tag * *', 'BERT model 10', 'BERT-based 10', 'BERT ( dynamic )', 'dynamic BERT model', 'BERT-SPC 3', 'BERT QA framework', 'BERT-base QA models', 'BERT-large QA model', 'BERT LARGE QA model', 'BERT-QA )', 'BERT-QA *', 'BERT-QA system', 'BERT QA system', 'BERT Large based QA model', 'BERT based QA systems', 'BERT QA systems', 'BERT-QA methods', 'BERT-based QA models', 'base BERT QA model', 'BERT-for-QA', 'BERT/SciBERT models', 'BERT This method', 'SEmb-BERT D', 'SEmb-BERT B', 'SEmb-BERT L', 'BERT rerankers', 'BERT Rerankers', 'BERT-based rerankers', 'BERT ( LSB )', 'Bert-base/RoBERETa-base', 'span-bert', 'span-BERT', 'Span-BERT-base', 'BERT noft )', 'SimCSE-BERT_ large 3', 'BERT-base SpanBERT-large Dataset method', 'BERT-based deep models', 'Deep Bidirectional Transformers ( BERT )', 'Deep-BERT', 'Pixel-bert', 'BERT g', 'GSS ( BERT )', 'BERT count', 'VIRT-BERT-Tiny', 'VIRT-BERT-Mini', 'BERT last', 'BERT-based clustering', 'Sent-BERT model', 'BERT sent', 'BERT/RoBERTabased architectures', 'Tran-BERT-MS', 'BERT+HAE model', 'BERT-DOCTOK-SEG', 'BERT-base 7', 'BERT-base 7 model', 'BERT-based single-task models', 'BERT-BASE-LWAN', 'BERT-MS', 'BERT-MS )', 'BERT ( m )', 'BERT-m model', 'BERTBase M', 'BERT M', 'BERT-MS model', 'Bert-Based Cloze Model', 'BERT-Joint * 1', 'K-means BERT', 'BERT-Mini', 'BERT MINI model', 'Mini BERT', 'BERT-TBR', 'BERT curve', 'BERTlarge reranker', 'BERT-Reranker', 'BERT-base reranker', 'BERT-large reranker', 'XY-BERT architecture', 'XY-BERT )', 'BERT- X - Y', 'BERT SPARTQA-AUTO', 'BERT-GB-T1 hy', 'BERT-Base BSO model', 'bert-as-service 7', 'BERT-based RUBER', 'BERT ZH', 'ZH-BERT )', 'BERT ( o-BERT )', 'o-BERT', 'BERT ( BERT O )', 'SA-BERT model', 'BERT-based linker', 'BERT + k -means', 'BERT + k-means', 'BERT CRs', 'BERT-CR *', 'BERT-CR', 'dense BERT model', 'dense BERT BASE model', 'BERT-only Model', 'BERT only model', 'BERT-ONLY', 'BERT-only classifier', 'BERT SUP', 'Bert sup', 'DISCERN ( BERT )', 'RUN-BERT RUN-BERT', 'BERT-based static WEs', 'bert-as-service 10', 'BERT/GPT-2', 'BERT GPT-2', 'GPT-2/BERT', 'BERT-Cls', 'BERT-cls', 'BERT [ CLS', 'BERT CLS-IN', 'BERT base - [ CLS ]', 'BERT base [ CLS ]', 'Bert-Pair', 'BERT-Pair model', 'BERT-PAIR model', 'BERT-Tiny 1', 'eMLM BERT model', 'BERT maps', 'BERT-mix model', 'BERT ( Mix )', 'sparse BERT models', 'BERT∗', 'BERT ∗ model', '∗ BERT models', 'BERT ∗ )', 'BERT ∗ models', 'ET BERT models', 'BERT-based summary', 'PL-BERT-BASE', 'PL-BERT-LARGE', 'BERT-FP-LBL )', 'BERT BERT-FP-LBL', 'BERT+KB', 'pars- BERT', 'BERT LARGE + STraTA', 'BERT BASE + STraTA', 'X-BERT', 'BERT - X', 'BERT X', 'BERT ( x )', 'BERT ( P )', 'P BERT', 'Bert Adam', 'Proto-BERT )', 'BERT entropy', 'BERT-base ( 110M )', 'BERT 110M', 'BERT ( 110M )', 'BERT-QG', 'BERT-QG models', 'BERT @ N', ""BERT_N '' models"", 'n BERT models', 'PCL-BERT † base', 'PCL-BERT †', 'BERT based filtering method', 'BERT-based filtering module']"
1,7,Method,34.6696,2018,"{'2018': 3.0913, '2019': 13.3371, '2020': 26.3668, '2021': 33.0771, '2022': 34.6696}","['Transformer', 'transformer', 'Transformers', 'transformers', 'Transformer model', 'transformer models', 'transformer model', 'Transformer models', 'Transformer-based models', 'transformer-based models', 'Transformer architecture', 'TRANSFORMER', 'transformer-based model', 'Transformer-based model', 'Transformer-based', 'transformer architecture', 'transformer-based', 'Transformer architectures', 'Transformer network', 'transformer architectures', 'Transformer-base', 'Transformer-Big', 'transformer network', 'G-Transformer', 'Transformer-big', 'Transformer-Base', 'TRANSFORMER-BASE', 'Transformer base', 'Transformer ( base )', 'transformer-based architectures', 'Transformer-based architecture', 'transformer networks', 'Transformer networks', 'Transformer-based architectures', 'TRANSFORMER-BIG', 'Hi-Transformer', 'transformer-based architecture', 'Transformer Base', 'Transformer base model', 'Transformer ( big )', 'TRANSFORMER model', 'Transformer Big', 'Transformer Model', 'base Transformer', 'Transformer big', 'm-Transformer', 'TRM', 'Transformer Big model', 'transformer based models', 'transformer-base', 'Transformer framework', 'TRS', 'Transformer ( Base )', 'Transformer-based methods', 'Transformer ( H )', 'Transformer based models', 'base transformer', 'transformer-big', 'transformer-based approaches', 'Transformer Base model', 'transformer-based methods', 'Transformer system', 'base Transformer model', 'TRANSFORMER-BIG model', 'TRMs', 'Transformer big model', 'transformer based model', 'Transformer-small', 'Trm', 'Transformer Models', 'Base Transformer', 'Transformer BIG', 'Transformer Big models', 'Transformer systems', 'Transformer-base models', 'Transformer-XH', 'GTransformer', 'Transformer based model', 'Transformer-BIG', 'Transformer-base model', 'transformer-based networks', 'transformer-based classifiers', 'Transformer-in', 'structured transformers', 'Transformer-Big model', 'Transformer base architecture', 'transformer base', 'Transformer Network', 'Transformer-based approaches', 'big Transformer model', 'Transformer-based method', 'Transformer ( Big )', 'transformer-based approach', 'transformers package', 'Transformer method', 'Transformer 3', 'Transformer-base architecture', 'Transformer )', 'Transformer-Base model', 'TRANSFORMER-BASE model', 'transformer-based method', 'transformer-big architecture', 'mTransformer', 'transformer-big model', 'Transformer-big model', 'Transformer based architectures', 'Transformer based architecture', 'transformer based', 'Transformer based', 'Transformer-Base architecture', 'Transformer Networks', 'Big Transformer', 'big-Transformer', 'Transformer module', 'transformer base model', 'Transformer base models', 'base transformer model', 'transformer framework', 'Transformer-based approach', 'Transformer-Small', 'Transformer-large', 'Transformers package', 'Transformers model', 'Set Transformer', 'M2-Transformer', 'Transformer ( big ) model', 'transformer-models', 'transformer based architectures', 'transformer-base architecture', 'transformer-based system', 'transformer )', 'transformer-', 'transformer based approaches', 'big Transformer', 'Transformer ( BASE )', 'Transformers )', 'Transformer BASE model', 'Transformer Base models', 'Transformer Big setting', 'Transformer-Base/Big', 'large Transformer models', 'transformer approach', 'transformer approaches', 'transformer modules', 'transformer-based framework', 'Transformer-based framework', 'S-Transformer', 'Transformer Big architecture', 'Transformer-Big )', 'Transformer Transformer', 'Transformer-based systems', 'Transformers models', 'transformer-based network', 'base transformers', 'transformer mechanism', 'transformer-based classifier', 'transformer-based structure', 'PG-Transformer', 'TransformerXH', 'S2S Transformer', 'transformer big model', 'trs', 'trm', 'Transformer-based Models', 'TRANSFORMER-based models', 'TRANSFORMER models', 'transformer based architecture', 'base-Transformer', 'Transformer-BASE', 'TRANSFORMER-base', 'Transformer-based system', 'transformer big', 'transformer structures', 'BIG Transformer', 'big transformer', 'Transformer ( big ) models', 'Transformer-BIG models', 'Transformer-big models', 'Transformer big models', 'transformer big models', 'transformer-base model', 'transformer system', 'transformer-base models', 'Transformer-Base models', 'transformer big setting', 'transformer base/big', 'Transformer layers', 'Transformer & Transformer', 'Transformer-Transformer', 'transformer-based frameworks', 'Transformer-based network', 'large Transformer model', 'transformer-small setting', 'transformer method', 'base Transformer architecture', 'Transformers-based models', 'transformers-based models', 'Transformer small model', 'big Transformers', 'Transformer frameworks', 'transformer-based classification models', 'big TRANSFORMER models', 'Transformer-s', 'Transformer network architecture', 'Transformer layer', 'Transformer cell', 'SOTA Transformer', 'Transformer-big 3', 'Transformers 3', 'Transformer-3', 'VTransformer', 'Transformer-Base ∗', 'TRANSFORMER †', 'Transformer †', 'Tranformer', 'Transformer-1', 'Transformer 1', 'Q-Transformer', 'MF Transformer', 'transformers 7', 'Transformers ”', 'Transformer ” model', 'transformer base v1', 'M 2 Transformer', 'Ours+Trm', 'Transformer0', 'HiTransformer', 'X-Transformer', 'X-transformer', 'HMF Transformer', 'Transformer Big Model', 'Transformer BIG model', 'Transformer Architecture', 'TRansformer', 'Transformer-Based Models', 'Transformer ) models', 'TRANSFORMERs', 'transformer- based architectures', 'Transformer- based architecture', 'transformer based methods', 'Transformer Based', 'transformer–based', 'Transformer-Based', 'Transformer Base architecture', 'TRANSFORMER-BASE architecture', 'base TRANSFORMER', 'TRANSFORMER-Base', 'Transformer BASE', 'Transformer_base', 'Transformer_Base', 'TRANSFORMER base', 'Transformer based system', 'TRANSFORMER based system', 'TRANSFORMER-Big', 'TRANSFORMER big', 'Transformer- big', 'Transformer Transformer models', 'Transformer- *', 'Transformer.', 'Transformer-', 'TRANSFORMER *', 'Transformer *', 'Transformer (', 'Transformers Transformer', 'TRANSFORMER-BIG models', 'Transformer-Big models', 'transformer-big models', 'transformer module', 'Transformers *', 'transformers/', 'Transformer ( base ) model', 'Transformer-BASE models', 'Transformer Base Setting', 'Transformer-base setting', 'Transformer Base setting', 'Transformer-Base setting', 'transformer-base setting', 'transformer base setting', 'big transformer model', 'Big Transformer model', 'TRANSFORMER-Base/Big', 'large transformer models', 'Transformer Layers', 'transformer layers', 'Transformer cells', 'transformer cells', 'base TRANSFORMER model', 'Transformer approach', 'Transformer modules', 'TRANSFORMER framework', 'Transformer-layer (', 'S-transformer', 'Transformer-big architecture', 'Transformer ( big ) architecture', 'Transformer big architecture', 'Transformer-Big architecture', 'Transformer-Big (', 'Transformer-based model architectures', 'Transformer small', 'Transformer- small', 'transformer small', 'transformer-small', 'Transformer_small', 'Transformer ( base model )', 'large Transformers', 'Transformer-base ( base )', 'Transformer-big ( big )', 'TRANSFORMER TRANSFORMER', 'Transformers architecture', 'Transformer-based ) systems', 'transformer based systems', 'transformer large', 'transformer-large', 'transformer classifier', 'Transformer-base methods', 'transformers models', 'transformers Models', 'transformers architectures', 'Transformer structure', 'Transformer Base base', 'large transformer model', 'transformers System', 'large transformer', 'large Transformer', 'Transformer mechanism', 'set transformers', 'Transformers Model', 'transformers model', 'transformers Model', 'Transformer-based classifiers', 'transformer based classifiers', 'Transformer-based classifier', 'based transformer', 'Base Transformer architecture', 'Transformers packages', 'transformers packages', 'Transformer Small model', 'transformer architecture models', 'Models - Transformer', 'Transformer network model', 'Transformer base system', 'Transformer principles', 'transformer model ( base )', 'transformer base set', 'transformers-based framework', 'Layer Transformers', 'Structured transformers', 'structured transformer', 'Transformers module', 'base Transformer models', 'transformers classifier', 'transformer based classification models', 'transformer-based classification approach', 'SotA Transformer-based models', 'Transformer-large model', 'Transformers base', 'Transformers big', 'transformer technique', 'Transformers-based', 'Transformer-base architectures', 'transformer model=', 'Big Transformer architecture', 'Transformer ( Base ) ( Big )', 'Transformer ( TRS )', 'TRM ( Transformer )', 'Transformer ( large )', 'Transformer-Big systems', 'Transformers ( Trm )', 'Transformer model architecture', 'transformers-metrics', 'Transformer based )', 'transformer-based model architecture', 'transformer-layer', 'Transformer_big set', 'transformer cell', 'Transformers-based architectures', 'transformer-based technique', 'transformer-based techniques', 'structured transformer-based architecture', 'transformer-based classification model', 'transformer classifier model', 'transformer classification model', 'transformers-based approaches', 'Transformer-based structures', 'Transformer-based architecture models', 'The base model ( Transformer Base )', 'Large Transformer based models', '( transformer )', 'Transformer layer networks', 'Transformer Transformer Transformer Transformer', 'transformers 3', 'Transformer model 3', 'Transformer-1 ( 6- 6 )', 'Transformer-1 ( 6-6 )', 'Tansformer', 'VTransformer model', 'Transformer ” 1', 'M2M Transformer', 'Transformers 17', 'Transformers 5', 'Tranformer base model', 'Tranformer-based architecture', 'Tranformer Models', 'Tranformer models', 'Tranformer model', 'Transformers 1', 'transformers 1', 'transformer 1', 'Transformer-Big 1', 'Transformer-big 1', 'transformer model 1', 'Transformer H', 'H-TRM', 'Transformer ”', '“ Transformer ”', 'S2S Transformer-based models', 'S2S Transformer model', 'S2S Transformer Model', 'S2S Transformer models', 'S2S Transformers models', 'XR-Transformer', 'M2- Transformer', 'M 2 -transformer', 'Transformer-big +', 'Transformer big model 10', 'Transformer-big-8', 'Transformer ( · )', 'Hi-Transformer models', 'Hi-Transformers', 'Hi-Transformer approach', 'Transformer ( base ) ’ model', 'Transformer ( G )', 'transformer ( ψ )', 'Transformer-based MS model', 'transformer-based MS models', 'Transformer M', 'XTransformer', '\ued48 Transformer', 'Transformer 5K', 'X-Transformer model', 'X-Transformers']"
2,6,Method,28.8231,2014,"{'2014': -0.0454, '2015': 3.329, '2016': 15.1072, '2017': 19.7983, '2018': 28.0425, '2019': 28.8231, '2020': 21.1541, '2021': 16.1007, '2022': 9.5794}","['LSTM', 'LSTMs', 'LSTM model', 'LSTM models', 'LS', 'ON-LSTM', 'LSTM-based', 'S-LSTM', 'lstm', 'LSTM-based model', 'LSTM-based models', 'SC-LSTM', 'TD-LSTM', 'C-LSTM', 'PA-LSTM', 'BC-LSTM', 'ABS-LSTM', 'EF-LSTM', 'LSTM-ER', 'bc-LSTM', 'SA-LSTM', 'ODE-LSTM', 'AT-LSTM', 'LSTM+', 'LSTM-1', 'MODE-LSTM', 'MV-LSTM', 'T-LSTM', 'LSTM )', 'QA-LSTM', 'H-LSTM', 'WC-LSTM', '-LSTM', 'LSTM networks', 'LSTM classifier', 'O-LSTM', 'LC-LSTMs', 'LST M', 'LSTM-based architecture', 'C-LSTMs', 'LSTM-z', 'AF-LSTM', 'TC-LSTMs', 'LSTM +', 'LSTM-LSTM', 'LSTM-s', 'A-LSTM', 'DM-LSTM', 'CT-LSTM', 'Lstm', 'LSTMs )', 'LSTM architectures', 'LSTM-based approaches', 'LSTM-2', 'LSTM1', '=LSTM', 'LSTM network', 'LSTM system', 'LSTM-d', 'LSTM ( W )', 'sc-LSTM', 'TC-LSTM', 'LSTM *', 's-LSTM', 'LSTM approach', 'DB-LSTM', 'LF-LSTM', 'LSTM 1', 'S-LSTMs', 'ON-LSTM model', 'SC-LSTM model', 'LS Score', 'LSTM-4', 'LSTM 3', 'LSTM T & N', 'lstms', 'LS models', 'LSTM-based architectures', 'LSTM-based classifier', 'LSTM-S', 'SDN TA-LSTM', 'LSTM-based approach', 'LSTM 2', 'sc-lstm', 'LSTM+SA', 'CEFR-LS dataset', 'SECT-LSTM', 'HM-LSTM', 'IO-LSTM', 'RAS-LSTM', 'LSTM-3', 'LSTM-', 'LSTM cells', 'LSTM-based methods', 'S-LSTM model', 'LSTM cell', 'LS system', 'LSTM-based method', 'LS & A', 'LSTM2', 'CEFR-LS', 'LSTM-sCRF', '+LSTM', 'LN-LSTM', 'LSTM ( W+L )', 'LSTM T & N model', 'LSTM-Loc', 'LSTM-64', 'ls', 'LSTMS', 'LSTM Models', 'short-term memory ( LSTM )', 'LSTM-layers', 'LSTM ( FTB )', 'ODE-LSTMs', 'M-AT-LSTM', 'On-LSTM', 'D-LSTM', 'LSTM+ method', 'W-LSTM', 'LSTM+LA', 'LSTM FCE', 'B i LSTM', 'n -LS', 'lstm +', 'SEDT-LSTM', 'LSTM method', 'LSTM architecture', 'LSTM-base', 'short-term memory network ( LSTM )', 'ON-LSTM ( W )', 'HD-LSTM', 'MFCC-LSTM', 'h-LSTM', 'DCU-LSTM', 'SB-LSTM', 'SA-LSTM-P', 'CAT-LSTM', 'LSTM-1 model', 'i-LSTM', 'LSTM Model', 'LSTM-networks', 'LSTM-Classifier', 'LSTM-based systems', 'LSTM-based network', 'LS dataset', 'LSTM systems', 'LSTM-layer', 'LSTM+HA', 'ID-LSTM', 'LSTM-A', 'LSTM ( A )', 'LS-960', 'B15-LSTM', 'LSTM-CLM', 'OBJ-LSTM', 'QA-LSTM-', 'ON-LSTMs', 'ON-LSTM )', 'c-LSTM', 'C-LSTM model', 'LSTM+AT', 'C3D-LSTM', 'LSTM+ methods', 'STL-LSTM', 'STL-LSTM system', 'LSTM-w+c', 'LSTM-flat', 'LSTM-XF1', 'TS + LS', 'TC LSTM', 'LSTM+UPA', 'HS-LSTM', 'n -LSTM', 'LSTM-NCM', 'LSTM ( 1B )', 'SDN LSTM', 'LSTM+W2V', 'LSTM+WS', 'lstm −', 'WC − LSTM', 'LSTM-I', 'LSTMs model', 'LS model', 'LSTM-based Models', 'LS )', 'LSTM -', 'LSTM (', 'LSTM ]', 'LSTM ) models', 'LSTM based methods', 'LSTM ( LSTM )', 'LSTM base', 'LSTM-based system', 'short-term memory networks ( LSTM )', 'F-Lstm', 'On-lstm', 'LSTM ( ON-LSTM )', 'LSTM-FA', 'HP-LSTM', 'LSTM+T', 'LSTM-d models', 'LSTM+S', 'LSTM-w', 'SCN-LSTM', 'SC-LSTM systems', 'LSTM-128', 'LS ( CLS )', 'LSTM+SA model', 'LSTM ( STL )', 'LSTM ’ s', 'DMM LS', 'Dec-LSTMs', 'LSTM ( T )', 'LSTM+WA', 'U-LSTM', 'LSTM-50d', 'LSTM-500d', 'LSTM-only', 'LSTM-Used', 'LSTM ( no )', 'FD-LSTMs', 'LSTM 4', 'LSTM-4 models', 'SA-ON-LSTM', 'EP-ON-LSTM', 'PA-LSTM model', 'NFGEC ( LSTM )', 'LSTM ( GeoD )', 'LSTM-bin', 'Dis-LSTM', 'B i LSTM model', 'LSTM 0', 'nested LSTMs', 'r-LSTM', 'r-LSTMs', 'LSTM r', 'TD-LSTM-A', 'SC-LSTM-I', 'HT-LSTM', 'SC-LSTM-P', 'LSTMs +', 'ON-LSTM ( G )', 'ON-LSTM ( B )', 'LSTM/NiN', 'LSTM nets', 'lstm−f', 'DE-LSTM', 'LSTM ( i-LSTM )', 'LSTM-based module', 'LSTM-model', 'lstm Model', 'LSTM based', 'LSTMs-based', 'LSTM-Based', 'lstm-based', 'LSTM based models', 'LSTM—', 'LSTM/', 'LSTMs—', 'LSTM Cells', 'LSTMs—the', '- - LSTM', 'LSTM Networks', 'LSTM Cell', 'LSTM-Cell', 'LSTM-cell', 'LSTM-network', 'short-term memory ( LSTM ) networks', 'LSTM Methods', 'LSTM based architectures', 'LSTM ( lstm )', 'LSTM ( LS )', 'LSTM- and', 'LSTM network model', 'LS measures', 'L STM model', 'LSTM-based model )', 'LSTM-s.', 'term memory networks ( LSTMs )', 'Networks ( LSTMs )', 'LSTM-large', 'Structured LSTM', 'Short-term Memory networks ( LSTMs )', 'Short-Term Memory networks ( LSTM )', 'Short-term Memory Networks ( LSTMs )', 'LSTM-based networks', 'layered-LSTMs', 'LSTM-network architectures', 'Short-Term Memory Network ( LSTM', 'Short-Term Memory ( LSTM ) models', 'models—LSTMs', 'LSTM classification models', 'LS algorithm', 'LSTM approaches', 'LSTM-based classification model', 'LSTM ( Base )', 'LST M model', 'S-LSTM *', 'LSTMs LSTM cells', 'Term Memory ( LSTM )', 'LSTM-based frameworks', 'LST M S', 'Large Data LSTMs', 'DN-LSTM', 'RN-LSTM', 'LSTM—as', 'B-LSTMs', 'ODE-LSTM )', 'LSTM+max', 'B15-LSTM model', 'QA-LSTM ( D )', 'QA-LSTM ( B )', 'FS-LSTM', 'AT-LSTM model', 'AT-LSTM method', 'LSTM 5', 'SA-LSTM model', 'ied_lstms (', 'On-Lstm', 'ON-LSTM architectures', 'ON-LSTM models', 'LSTM-on-LSTM', 'LSTM ( 20M )', 'C-LSTM )', 'C-LSTM *', 'C-LSTMs models', 'C-LSTM models', 'LSTM-pre', 'HP-LSTM )', 'LSTM-10k – 10k', 'LSTM j', 'd-LSTM-n-I', 'LSTM H-LSTM 1.1', 'LSTM ( LSTM D )', 'node LSTM', 'LSTM+PAR architecture', 'sc-LSTM 1 )', 'LSTM++', 'LSTM- ` 2', 'LSTM - 2', 'LSTM 2 )', 'LSTM-based S2S approach', 'LSTM-based S2S', 'ODE-LSTM †', 'w/ LSTMs', 'LSTM ( +GEF )', 'SC-LSTMs', 'SC-LSTM approach', 'SC-LSTM system', 'SC-LSTM models', 'sc-LSTM models', 'SC-LSTM cell', 'lstm−b i', 'lstm−b', 'LSTM+SA models', 'LSTM ( WP )', 'LSTM-N W', 'AST-based LSTM', 'LSTM-flat )', 'LSTM-flat models', 'LSTM-flat model', 'LSTM-XF1 8', 'LSTM-based ST model', 'ST-LSTM', 'LSTM—or', 'DPM LS', 'DPM LS models', 'F1 LSTM', 'CEFR-LS data', 'LS_score', 'LS_Score', 'LS-Score', 'LSTM-XF1 1', 'LSTM-T', 'LSTM - T', 'LS T', 't-LSTM', 'T-LSTMs', 'IBFP-LSTM', 'IBFP-LSTMs', 'LSTM ” model', 'LSTM+AD model', 'LSTM+AD', 'LSTM ( None )', 'LSTM - 99', 'LSTM U', 'LSTM 9', 'F 1 μ LSTM', 'LSTM ( 192K )', 'LSTM ( 545K )', 'HM LSTM', 'LSTM-Only', 'DB-LSTM model', 'FS-LSTM-4', 'LSTM - L', 'LSTM l', 'LSTM ( L )', 'f LSTM 1', 'LSTM-dist', 'LSTM-dist model', '+LSTM+WS', 'LSRC/LSTM', 'AF-LSTM method', 'ATE-LSTM', 'LSTM 20', 'LSTM ( 1L )', 'LSTMs 4', 'LSTM networks 4', 'LSTM + I', 'PA-LSTMs', 'PA-LSTM models', 'PA-LSTM *', 'LSTMs—one', 'LSTM-based ones', 'EF-LSTM ( ⋆ )', 'LSTM-xs', 'LSTM + SC', 'FP+LSTM', 'FP + LSTM', 'h lstm', 'H-LSTM )', 'LSTM ( 200d )', 'LSTM-based ACD model', 'LSTM↓', 'LSTM↑', 'Res-LSTM', 'LSTM based model ( HN )', 'RAS-LSTM models', 'LSTM−large', 'LSTM/MHSA', 'MV-LSTM model', 'MH-LSTM', 'LSTM ( k )', 'LSTM-k', 'DCU-LSTM models', 'LSTM-IBA', 'B i LSTMs', 'oh-LSTM', 'ON-LSTM‡', 'LSTM [ w+s ]', 'N term memory networks ( LSTMs )', 'LSTM- n', 'n -LS classifiers', 'edit LSTM', 'LSTM-Edited', 'LSTM-n-k', 'λ 4 LSTM', 'Nested LSTM', 'nested LSTMs architectures', 'nested LSTMs models', 'BPC LSTM', 'LSTM ( WC-LSTM', 'WC-LSTM model', 'R-LSTM', 'SA-LSTM-P model', 'LSTM ( bw )', 'LSTM ( f w )', 'LSTM model 3', 'LSTM 227K', 'LSTM †', 'LSTM + LSTM', 'LSTM LSTM +', 'LSTM 22M', 'MM-LSTM )', 'MM-LSTM', 'LSTM T & N models', 'LSTM FC', 'SDT-LSTM', 'SDT-LSTM model', 'EF-LSTM )', 'EF_LSTM', 'LSTM ( EF-LSTM )', 'LSTM ( LF-LSTM )', 'VSE-LSTM', 'kil ls', 'LSTM ( SFRN )', 'SEDT-LSTM models', 'LA-LSTM', 'LSTM 16', 'LSTMs 1', 'LSTM 1 )', 'LSTM ( TD-LSTM', 'LSTM-FLT', 'bc-lstm']"
3,11,Method,26.2604,2006,"{'2006': -0.1572, '2007': -0.1271, '2008': -0.3149, '2009': -0.2817, '2010': -0.2612, '2011': -0.3403, '2012': -0.1166, '2013': -0.1274, '2014': -0.1948, '2015': 0.3489, '2016': 8.0978, '2017': 14.0546, '2018': 21.9032, '2019': 26.2604, '2020': 21.8076, '2021': 19.9133, '2022': 17.2833}","['attention', 'attention mechanism', 'attention mechanisms', 'Attention', 'attention model', 'attentions', 'attention module', 'attention models', 'attention network', 'attention-based models', 'attention-based model', 'attention-based', 'attention networks', 'structured attention', 'attention modules', 'Attention mechanism', 'Attention Mechanism', 'attention-based methods', 'ATTN', 'AT & T', 'Attention mechanisms', 'ATT', 'attention-based approach', 'Attn', 'attention-based mechanism', 'attn', 'attention-based approaches', 'attention based models', 'attention method', 'attention methods', 'attention based model', 'attention-based method', 'attention strategies', 'Att', 'attention layers', 'Attentions', 'Attention-based models', 'Attention models', 'attention analysis', 'S2S-Attn', 'att', 'Attention Model', 'Attention model', 'structured attention mechanism', 'layer attention', 'attention framework', 'ATT method', 'attention-based architectures', 'attention strategy', 'structured attention networks', 'attention architecture', 'I-Attention', 'attention architectures', 'attention based', 'Attention Networks', 'attention techniques', 'attention )', 'attention-based mechanisms', 'Attention Network', 'ATT model', 'attention modeling', 'C-Attention', 'attention based methods', 'attention-based strategy', 'attention-based system', 'attention-mechanism', 'ATTENTION', 'Structured Attention', 'Attention Mechanisms', 'attention-based network', 'Attention module', 'Attention-based', 'Att-based', 'attention-based systems', 'Attention method', '-Attention', 'attention-based architecture', 'attention based approaches', 'attention-based networks', 'attention approach', 'gold attentions', 'attention technique', 'attention-based classifier', 'attention-based framework', 'cs-ATTN-ATTN', '+H-Attention', 'attention ”', 's2s-att', 'Attention based models', 'model attention', 'Attention-based methods', 'Att strategy', 'attention values', 'attention approaches', 'structured-attention model', 'Ours-Att', 'C attn', 'KB attention', 'I-Attention model', 'attention mechanism model', 'Attention Module', 'attention based approach', 'attention classifier', 'Attention Models', 'attention-based module', 'structure-based attention', 'attention attention', 'attention structures', 'attention layer', 'attention structure', 'structured attention mechanisms', 'S-Att', 'Att 2', 'biattention', 'attention ” mechanism', 'Attention U', 'attention-mechanisms', 'Attention-Based Model', 'Attention-based model', 'ATT models', 'attention based method', 'Attention Strategies', 'attention based mechanism', 'Attention-based approaches', 'Attention Strategy', 'attention algorithms', 'Attention )', 'attention-', 'Attention *', 'metric-based attention method', 'model attentions', 'Attention analysis', 'attention system', 'Attention Modeling', 'ATTN-ATTN', 'Attention Attention', 'structured attention model', 'structure attention', 'structure attention mechanism', 'attention analyses', 'attention value', 's attentions', 'Attention 2', 'biattention mechanism', 'C-attn', 'V-Attn', 'ATT ( v )', '“ attention ” mechanism', 'attention Att II', '+Attention', 'base+att', 'base+att model', 'attention ) mechanism', 'structured-attention', 'S-Att.', 'Attention Architectures', 'Attention-Based Models', 'Att-based models', 'attention-based analysis', 'attention based analysis', 'attention-network', 'Attn model', 'Attention-based Approach', 'Attention Based', 'Attention-Based', 'Attention based', 'Attention classifier', 'Attention networks', 'Attention based methods', 'Attention -based methods', 'attention Models', 'ATTENTION ( ATT )', 'attention ( Attention )', 'attention ( ATT )', 'Attention ( ATTN )', 'ATTN method', 'attn method', '- Attention', ') attention', 'attention based architectures', 'attention based architecture', 'Attention methods', 'Attention Methods', 'attentions ( attn )', 'Attention-based method', 'Attention strategies', 'network attention', 'attention-base model', 'Attention techniques', 'Attention Layers', 'attention-based frameworks', 'Attention-based Strategy', 'Attention (', 'attention (', 'Att )', 'attn -', 'Att.', 'Att-', 'Layer Attention', 'attention based module', 'attention-based classification', 'Attention Setting', 'Attention values', 'attention-based metrics', 'Attention approaches', 'Attention Analysis', 'At & t', 'AT-T', 'attention-based pointing mechanism', 'attention model (', 'over-attention', 'attention based system', 'Attention-based system', 'attention mechanisms Model', 'attention network architecture', ') attention model', 'structured attention approach', 'ATT ATT', 'ATTN-ATTN model', 'Attention architecture', 'attention-layer', 'attention mechanisms )', 'attentions )', 'attention-based )', 'Structured Attention model', 'structured attention method', 'OURS-ATT', 'attention/classification models', 'attention based technique', 'attention-based classification model', 'attention-based structure', 'attention mechanism-based network', 'attention cell', 'Attention Mechanism )', 'attention mechanism )', 'attention based framework', 'structured attentions', 'attention-based network model', 'attention network mechanism', 'Gold Att #', 'Dataset Attention ( Base )', 'attention network model', 'attention over', 'attention measure', 'attention based classification framework', 'attentions—the', 'attention-based structures', 'Attention To', 'attention mechanism Methods', 'attention-based network architecture', 'Structured attention models', 'attention algorithm', 'attention of models', 'ATT 2', 'att2', '“ - Attention', 'C-ATTENTION', 'Att-C', 'cs-ATTN', 'cs- ATTN-ATTN', 'attention ( c )', 'attention 9', 'h att i', 'Attention ( · )', 'attention ( α )', 'attention α', 'KB attention mechanism', 'attention-based system 1', 'attention-based analysis 1', 'Attn ”', 'attention-based au', 'S2S-att', 'S2S-Att', 'model ’ s attention', 'attn ’ model', 'S + I-Attention model', 'S + I-Attention', 'ATT & CK', 'K attention', '• Attention', 'attention 4', 's2s + attention mechanism', 'IAttention', 'Attention v/s', 'v attention', 'ATT ( v ) model', 'I-Attn', 'I-Attention models', 'attention β', 'Attn # 0/1', 'S+I Attention model', 'S + IAttention model', 'S+I-Attention', 'attention mechanism ( 5 )']"
4,67,Method,20.3561,2016,"{'2016': 2.886, '2017': 9.4933, '2018': 14.8464, '2019': 20.3561, '2020': 19.4806, '2021': 18.8707, '2022': 11.7721}","['Adam', 'Adam optimizer', 'ADAM', 'Adam algorithm', 'ADAM optimizer', 'Adam Optimizer', 'adam', 'Adam method', 'adam optimizer', 'Adams', 'ADAM algorithm', 'adams', 'Adam Adam', 'Adam )', 'AdaM optimizer', 'adam algorithm', 'Adam optimizer (', 'Adam Optimizer )', 'Adam framework', 'ADAM method', 'Adam data', 'adams optimizer', 'Adam model']"
5,12,Method,19.569,2003,"{'2003': -0.066, '2004': -0.3438, '2005': 0.13, '2006': -0.0773, '2007': -0.2092, '2009': -0.1635, '2010': 0.3892, '2011': -0.1319, '2012': -0.1966, '2013': -0.0223, '2014': 0.7647, '2015': 5.1323, '2016': 9.8934, '2017': 12.3222, '2018': 17.5955, '2019': 19.569, '2020': 13.8532, '2021': 11.8493, '2022': 9.1646}","['CNN', 'CNNs', 'CNN/DM', 'CNN model', 'convolutional neural network', 'convolutional neural networks', 'convolutional neural network ( CNN )', 'CNN models', 'Convolutional Neural Network ( CNN )', 'convolutional neural networks ( CNNs )', 'convolutional neural networks ( CNN )', 'Convolutional Neural Networks ( CNNs )', 'Convolutional Neural Network', 'CNN-based', 'cnn', 'CNN-based model', 'CNN-based models', 'CNN-DM', 'CNN/DM dataset', 'CNN dataset', 'Convolutional Neural Networks ( CNN )', 'Convolutional Neural Networks', 'CNN classifier', 'CR-CNN', 'CNN architecture', 'ID-CNN', 'CNN-c', 'CNN )', 'CNN architectures', 'convolutional neural network model', 'i-CNN', 'CNN network', 'CNN1', 'CNN-based classifier', 'CNN-MC', 'CNN approach', 'CNN-a', 'Convolutional neural networks', 'HFT-CNN', 'CNN_C', 'Convolutional neural networks ( CNNs )', 'CNN-SC', 'convolutional neural network architecture', 'CNN-DM dataset', 'C-CNN', 'CNN-R', 'a-CNN', 'CNN_D', 'CNN R', 'CNN-Zh', 'CNN-SM', 'NC-CNN', 'Convolutional neural network ( CNN )', 'CNN networks', 'CNN2', 'seq-CNN', 'Convolutional neural network', 'CNN corpus', 'CNN-based architecture', 'MC-CNN', 'CNN/DM data', 'Convolutional Neural Network ( CNN ) model', 'CNN-', 'CNN 3', 'CNN/DM corpus', 'PA-CNN', 'PG-CNN', 'CNN based models', 'CNN-based methods', 'CNN-NTC', 'ID-CNNs', 'CNN-IM', 'EG-CNN', 'Convolutional Neural Network model', 'CNN ( Convolutional Neural Network )', 'convolutional neural network models', 'CNN method', 'convolutional neural network ( CNN ) model', 'convolutional neural networks ( CNNs', 'CNN classifiers', 'CNN-VMASK', 'VD-CNN', 'SAL-CNN', 'J-CNN', 'DM-CNN', 'NSC-CNN', 'CNN based model', 'Cnn', 'convolutional neural network ( CNN', 'CNN data', 'CNN system', 'CNN-base', 'CNN-based classifiers', 'CNN-based method', 'AT-CNN', 'CNN-DM data', 'CNN-DM corpus', 'Convolutional neural networks ( CNN )', 'convolutional neural networks ( CNN', 'CNN (', 'CNN methods', 'convolutional neural network ( CNN ) classifier', 'convolutional neural network ( CNN ) architecture', 'CNN-based approaches', 'Convolutional Neural Network ( CNN ) models', 'convolutional ( CNN )', 'S-CNN', 'CNN data set', 'PF-CNN', 'CNN-R.', 'ID-CNN model', 'CNN-PE', 'seq2-CNN', 'Q-CNN', 'HFT-CNN model', 'CNN-1', 'stacked CNNs', 'CNN-5', 'CNN-9', 'Syl-CNN', 'M-CNN', 'CNN-STC', 'V-CNN', 'Convolutional neural network model', 'CNN Model', 'convolutional neural network ( CNN ) -based', 'convolutional network ( CNN )', 'CNN-based networks', 'CNN corpora', 'CNN algorithm', 'CNN approaches', 'CNN framework', 'CNN-based system', 'convolutional neural network ( CNN ) models', 'Convolutional Neural', 'convolutional neural architecture', 'CNNs ( Convolutional Neural Networks )', 'convolutional neural architectures', 'CNN-CNN-S', 'CNN base model', 'MP-CNN', 'MC-CNN model', 'CNN-3', 'J-CNN *', 'CNN/DM )', 'MV-CNN', 'SL-CNN', 'Syl-CNN-3', 'SM CNN', 'CNN-APLN', 'SECT-CNN', 'SEDT-CNN', 'Convolutional NNs', 'CNNS', 'convolutional NN', 'Convolutional Neural Network Model', 'CNN-CNN', 'CNN-based Models', 'CNNs )', 'CNN *', 'CNN * *', 'CNN-based neural networks', 'CNN-based architectures', 'CNN based architectures', 'Convolutional Neural Network ( CNN ) classifier', 'convolutional neural model', 'convolutional networks ( CNNs )', 'convolutional neural', 'CNN-based approach', 'convolutional neural network classifiers', 'Neural Network ( CNN )', 'convolutional neural network ( CNN ) architectures', 'convolutional neural models', 'CNN systems', 'CNN-dev', 'CNN-QA dataset', 'Convolutional Neural Networks ( PF-CNN )', 'CNN ( 2D )', 'F-Cnn', 'CNN ( TK-5k )', 'SAL-CNN-L', 'CNN-CNN-C', 'SAL-CNN-V', 'SAL-CNN-A', 'CNN+AVE', 'R-CNNs', 'CNN-R model', 'SCA-CNN', 'CNN/DN', 'CNN-PE model', 'CNN ( All )', 'hard-CNN', 'CNN 2', 'CNN-DM model', 'CNN 4', 'T-CNN', 'CNN-SEQ', 'CNN-ASP', 'CNN task classifier', 'm-CNN', 'HMTL-CNN', 'CNN W O', 'F1 CNN', 'ST-CNN', 'D-CNN', 'CNN ( CNN-10k )', 'CNN-10k', 'CNN ( HCTI )', 'CR-CNN model', 'NC-CNN - -', 'GMT-CNN', 'CR-CNN 1', 'GTS-CNN', 'CNN-based Model', 'CNNs based model', 'convolutional ) Neural Network', 'convolutional NNs', 'CNNs model', 'CNN-model', 'convolutional neural network ( CNNs )', 'convolutional neural network ( CNN ;', 'CNN ( CNN )', 'CNNs models', 'CNN CNN', 'CNN & CNN', 'Convolutional Neural Network ( CNN', 'convolutional neural NETworks', 'convolutional neural network classifier', 'CNN based architecture', 'Convolutional Neural Network based', 'CNN Networks', 'convolutional neural network ( CNN ) based', 'Convolutional NN ( CNN ) model', 'Convolutional Neural Networks CNN', 'Convolutional neural networks CNN', 'Convolutional Neural Networks ( CNN', 'Convolutional Neural Networks - CNN', 'Convolutional neural network )', 'CNN based neural networks', 'CNN based classifier', 'Convolutional Network ( CNN )', 'convolutional neural network architectures', 'Convolutional neural network architectures', 'CNN Architectures', 'CNN-Based Architectures', 'neural networks ( CNNs )', 'neural networks ( CNN )', 'Neural Networks ( CNN )', 'Convolutional Neural Network architecture', 'Convolutional neural network architecture', 'Convolutional Neural Networks ( CNN ) architecture', 'convolutional neural network ( CNN ) -based classifier', 'convolutional neural network approach', 'Convolutional Neural Network ( CNN ) architecture', 'convolutional neural network approaches', 'convolutional neural layer', 'cnn_dataset', 'convolutional neural network system', 'convolutional neural network based approaches', 'CNN based system', 'CNN-based neural network classifier', 'CNN-based techniques', 'convolutional neural networks architectures ( CNNs )', 'Convolutional Neural Network model ( CNN )', 'convolutional neural network model ( CNNs )', 'convolutional neural network model ( CNN )', 'method ( CNN )', 'convolutional neural network based approach', 'CNN based approach', 'S-CNN model', 'CNN-layers', 'CNN classification model', 'CNN classifier )', 'Convolutional Network ( CNN ) models', 'CNN ( Convolutional Neural Networks )', 'CNN base classifiers', 'neural network ( CNN )', 'CNN-based neural network', 'network ( CNN )', 'convolutional neural network ( CNN ) -based model', 'Convolutional Neural Network ( CNN ) -based model', 'CNN-based Modeling', 'convolutional network CNN', 'CNN neural network', 'convolutional neural network based method', 'Convolutional Neural Networks ( CNNs ) model', 'CNN-based classification method', 'Convolutional Neural Network ( CNN ) method', 'convolutional neural networks ( CNN ) -based', 'convolutional neural networks )', 'CNN mechanism', 'convolutional neural networks ( CNN layer )', 'convolutional neural network ( CNN ) classifiers', 'small-CNN', 'CNNs Convolutional neural networks', 'CNN Convolutional Neural Networks', 'CNN module', 'CNN-based network', 'CNN ( ours )', 'CNN base architecture', 'Classifier ( CNN )', 'Convolutional Neural Networks ( CNN ) architectures', 'CNN-based )', 'Convolutional Neural Networks ( CNN ) models', 'Convolutional Neural Networks models', 'CNN-based systems', 'convolutional neural network ( CNN ) -based methods', '@ CNN', 'CNN-based ( Convolutional Neural Network ) technique', 'CNN-based technique', 'CNN-based structure', 'convolutional neural structures', 'convolutional neural network ( CNN ) -based architectures', 'CNN-based Classification Model', 'CNN-based classification model', 'convolutional neural network ( CNN ) -based models', 'MG-CNN', 'QA-CNN', 'CNN ( 400 )', 'CNN-NSU', 'W CNN', 'CNN-HTCI model', 'CNN6', 'CNN 6', 'cnn 6', 'Convolutional Neural Network 6 ( CNN )', 'QA-CNN ( A )', 'f cnn', 'CNN classification model 10', 'convoluational neural network', 'CNN ( TK-10k ) *', 'CNN ( TK-10k )', 'CNN c', 'convolutional ( C ) neural networks', 'CNN ( G )', 'CNN ( x )', 'CNN-x', 'CNN-x model', 'CNN-pre', 'CNN/ABC', 'CNN-5k – 5k', 'CNN ( TK )', 'DM-CNN ∗', 'convolutional neural networks ( 1D CNNs )', 'CNN R model', 'CNN R models', 'CNN ( 90K )', 'AC CNN', 'MIT ( CNN )', 'CNN PE', 'CNN- src', 'AST-based CNN', 'Syl-CNN-2', 'CNN- ` 2', 'CNN-2', 'CNN S3D-G', 'Syl-CNN-4', 'Syl-CNN 7', 'CNN-SC −', 'CNN-SC +', 'CNN-MC model', 'seq2- CNN', 'CNN ( HR-CNN )', 'VD-CNN models', 'CNN-n', 'n CNNs', 'CNN model 1', 'CNN ( 1 )', 'CNN AVE', 'DE-CNN 10', 'f CNN 2', 'AS-CNN', 'CNN-a )', 'CNN ( a )', 'CNN - a system', 'CNN-CNN-A', 'CNNs A-network', 'CNNs—as', 'J-CNN * model', 'J-CNN model', 'CNN - 64', 'CNN / DM', 'CNN/DM Dataset', 'CNN DM dataset', 'CNN/DM data Models', 'CNN/DM Model', 'stacked convolutional neural network', 'stacked CNN', 'stacked convolutional neural networks', 'stacked Convolutional Neural Networks', 'HFT-CNN ( B )', 'HFT-CNN ( M )', 'CNN based Model ( LC )', 'CNN package 4', 'convolutional neural networks ( CNNs ) 4', 'convonlutional neural network', 'CNN-Zh models', 'my_cnn', 'CNN-based AA model', 'CNN 19', 'w cnn i', 'convolutional neural networks—we', 'CNN-128 model', 'CNN task', 'CNN task classifiers', 'CNN CLS', 'Convolutional Neural Network ( TCN )', 'CNN ’ s', 'CNN ( m )', 'm -CNN', 'S & M CNN model', 'F 1 CNN', 'Adv-CNN', 'CNN-ADV', 'CNN 7', 'VC-CNN', 'CNN-10k – 10k', 'CNN ( CNN-10k', 'CNN-10K', 'CNN†', 'convolutional neural sub-networks', 'Ada-CNN', 'CNN-SMs', 'f θ CNN', 'V-CNN model', 'V-CNN models', 'CNN ( CR-CNN )', 'CR-CNN architecture', 'CR-CNN approach', 'convolutional neutral network', 'CNN-pair', 'PAR CNN']"
6,26,Method,18.8672,2019,"{'2019': 0.0365, '2020': 8.4236, '2021': 15.3012, '2022': 18.8672}","['RoBERTa', 'RoBERTa-large', 'RoBERTa model', 'RoBERTa-base', 'ROBERTA', 'RoBERTa-Large', 'RoBERTa models', 'Roberta', 'RoBERTa LARGE', 'RoBERTa BASE', 'RoBERTa large', 'RoBERTa base', 'roberta-base', 'RoBERTa-Base', 'RoBERTa-base model', 'RoBERTa-large model', 'RoBERTa Large', 'roberta-large', 'RoBERTa-based models', 'RoBERTa large model', 'RoBERTa-Large model', 'RoBERTa base model', 'RoBERTa-based classifier', 'RoBERTA', 'RoBERTa_BASE', 'RoBERTa classifier', 'RoBERTa-based model', 'RoBERTa-base models', 'RoBERTa-based', 'Roberta-base', 'Roberta-Large', 'RoBERTa Base', 'RoBerta', 'RoBERTa architecture', 'ROBERTA-LARGE', 'RoBERTa BASE model', 'RoBERTa )', 'ROBERTa', 'Roberta model', 'roberta-large model', 'Roberta-large', 'ROBERTA-BASE', 'S-RoBERTa', 'RoBERTa ( base )', 'RoBERTa-ν', 'Roberts', 'roBERTa', 'ROBERTA-Large', 'ROBERTA model', 'RoBERTa ( large )', 'RoBERTa-ζ', 'ROBERTA BASE', 'RoBERTa LARGE model', 'roberta-base model', 'RoBERTa-based architecture', 'RoBERTa- ζ', 'RoBERTa †', 'RoBERTA-large', 'ROBERTA-base', 'RoBERTa Large model', 'ROBERTA-base model', 'RoBERTa-Large models', 'RoBERTa-large models', 'RoBERTa *', 'Roberta base )', '+RoBERTa', 'RoBERTa- ν', 'ROBERT', 'RoBerTa', 'ROBERTA-large', 'RoBerta-large', 'Roberta-Base', 'RoBERTa-Base model', 'RoBERTa LARGE models', 'ROBERTA-large models', 'RoBERTa based model', 'RoBERTa-base architecture', 'RoBERTa networks', 'base RoBERTa', 'RoBERTa network', 'RoBERTa system', 'RoBERTa 4', 'RoBERTa+Classifier', 'RoBERTA model', 'RoBERTa Model', 'RoBERTA-base', 'Roberta base', 'RoBERTa-BASE', 'ROBERTA-large model', 'ROBERTA-Large model', 'Roberta-large model', 'RoBERTa ( large ) model', 'ROBERTA models', 'RoBERTa base models', 'RoBERTa BASE models', 'RoBERTa ( base ) models', 'Roberta-base model', 'RoBERTa large models', 'RoBERTa our', 'RoBERTa-', 'RoBERTa-base )', 'RoBERTa-large-based model', 'RoBERTa framework', 'RoBERTa RoBERTa', 'Roberta-Roberta', 'RoBERTa-Large classifier', 'RoBERTa-large classifier', 'ROBERTA-large classifier', 'RoBERTa-based classifiers', 'base RoBERTa model', 'RoBER Ta', 'RoBERTa-based classification model', 'RoBERTa-S', 'S-RoBERTa ( base )', 'cased RoBERTa base model', 'RoBERTa ∗', 'roberta-based ” models', 'roberta-based ” model', 'RoBERTa-base ”', 'RoBERTa + Classifier', 'RoBERTa +', 'S + RoBERTa', 'roberta-base 9', 'ROBERTA 9', 'ROBERTA X', 'RoBERTa 7', 'RoBERTa-base 7', 'RoBRTa', 'RoBERTa+', 'RoBERTa†', 'RoBERT model', 'roberts', 'ROBERT dataset', 'RoBERT-Large model', 'RoBERT-large', 'RoBERT-Large', 'RoBERT-based models', 'RoBERTa Y', 'roberta', 'RobERTA', 'RobERTa', 'RoBertA', 'RoBERta', 'RoBERTA large', 'Roberta large', 'RoBERTa ( large', 'ROBERTa-large', 'RoBERTa_large', 'ROBERTA LARGE', 'RoBERTa -large', 'ROBERTA-model', 'ROBERTa model', 'RoBERTa * model', 'RoBERTa_base', 'RoBERTa : base', 'Roberta-Large model', 'ROBERTA LARGE model', 'RoBERTA-Large model', 'RoBERTa -large model', 'ROBERTA-LARGE model', 'RoBerta-large model', 'RoBERTa-LARGE model', 'RoBERTa ) models', 'roberta-base models', 'RoBERTa-Base models', 'RoBERTa Base model', 'RoBERTa-BASE model', 'RoBERTa ( base ) model', 'ROBERTA-BASE model', 'ROBERTA base model', 'RoBERTa-large (', 'RoBERTa-Classifier', 'roberta-large models', 'RoBERTa based models', 'ROBERTA-based models', 'RoBERTA our', 'RoBERTa (', 'RoBERTa base )', 'RoBERTA architecture', 'Roberta architecture', 'RoBERTa approach', 'Roberta-large based model', 'RoBERTa-based classification models', 'ROBERTA-based model', 'Roberta based model', 'Roberta-based model', 'RoBERTa-base Module', 'RoBERTa BASE in', 'ROBERTA base/large', 'RoBERTa Base/Large', 'RoBERTa base/large', 'RoBERTA ( base ) architecture', 'RoBERTa BASE architecture', 'RoBERTa ( base ) architecture', 'RoBERTa-base framework', 'RoBERTa-large framework', 'RoBERTa case', 'RoBERTa based', 'ROBERTA-based classifier', 'RoBERTa based model )', 'RoBERTa-large ( roberta-large )', 'RoBERTa LARGE classifier', 'RoBERTa ( base/large )', 'RoBERTa ( roberta-base )', 'roberta-base based model', 'RoBERTa classifiers', 'large RoBERTa model', 'RoBERTa approaches', 'RoBERTa classification model', 'ROBERTA-large classifiers', 'RoBERTa dataset', 'RoBERTa-Large ( Classification )', 'RoBERTa S', 'RoBERTa s', 'roberta-base classification model', '=RoBERTa', 'RoBERTa-based modeling', 'RoBERTa-Small model', 'RoBERTa-Small', 'RoBERTa small', 'cased RoBERTa base', 'RoBERTa-based architectures', 'RoBERTa layer', 'RoBERTa corpus', 'RoBERTa-large base', 'Model RoBERTa', 'RoBERTa-large based models', 'RoBERTa-base classifier', 'RoBERTa-Large architecture', 'roberta-large♢', 'ROBERTa 4', 'RoBERTa model 4', 'RoBERTa-base 4 model']"
7,8,Dataset,17.4187,2005,"{'2005': -0.2896, '2006': 0.2073, '2007': 1.2189, '2008': 2.4604, '2009': 4.0791, '2010': 5.8443, '2011': 5.8018, '2012': 6.6714, '2013': 8.0137, '2014': 9.4119, '2015': 11.7332, '2016': 10.5812, '2017': 11.3389, '2018': 12.1848, '2019': 14.1436, '2020': 16.6423, '2021': 16.6457, '2022': 17.4187}","['Wikipedia', 'Wikipedia corpus', 'Wikipedia data', 'Wiki', 'Wikipedias', 'Wikipedia dataset', 'wikipedia', 'WIKIPEDIA', 'WIKI', 'Wikipedia corpora', 'Wikipedia 2', 'Wiki80', 'wiki', 'Wikipedia )', 'Wiki dataset', 'Wikipedia-based', 'Wikipedia-based approaches', 'Wiki data', 'Wikipedia ( Wiki )', 'Wikipedia 4', 'Wikipedians', 'Wikipedia 3', 'Wikipedia-based dataset', 'Wikipedia 6', 'Wiki corpus', 'Wikipedia Corpus', 'Wikipedia data set', 'Wikipedia 8', 'WIKI dataset', 'of Wikipedia dataset', 'Wiki4M', 'Wikipedia-based corpus', 'Wikipedia Wikipedia', 'WIKI-2', 'wikipedia data', 'WikiPedia', 'wikipedia corpus', 'wikipedias', 'Wikipedia Data', 'Wikipedia ( wiki )', 'Wikipedia models', 'WIKIPEDIA dataset', 'Wiki )', 'Wikipedia dev set', 'Wikipedia data sets', 'Wiki-Small corpus', 'Wikipedia-based approach', 'Wikipedia model', 'Wikipedia 9', 'Wiki_data', 'wiki data', 'wiki corpora', 'Wikipedia ( WIKI )', 'Wikipedia-based data', 'WIKI-', 'Wikipedia-', 'Wiki-Large', 'Wikipedia set', 'gold Wikipedia', 'Wiki Model', 'Wikipedia-based measures', 'wiki80', 'Wikipedia corpus 3', 'MMS ( Wiki )', 'Wikipedia ” corpus', 'Wiki ’ s', 'Wikipedia-data', 'WIKIPEDIA data', 'Wiki system', 'Wikipedia system', 'wikipedia system', 'wiki corpus', 'Wikipedia-based Dataset', 'Wikipedia [ Wiki ]', 'WIKI ( WIKI )', 'Wiki-based', 'wikipedia models', 'wikipedia dataset', 'Wikipedia Dataset', 'wiki dataset', 'Wiki Dataset', 'wiki=', 'Wiki-', 'Wikipedia–', 'WIKI )', 'wiki )', 'Wikipedia dev', 'Wikipedia-dev', 'W IKI dataset', 'Wiki-s', 'Wiki-Wikipedia', 'WIKI data sets', 'to Wiki )', 'Wiki set', 'Gold Wiki', 'Wikipedia dataset The Wikipedia dataset', 'WIKI–Small', 'Wikipedia algorithms', 'WIKI model', 'Wiki model', 'wiki model', 'Wikipedia-based model', 'Wiki data set', 'large Wikipedia dataset', 'Wikipedia/Wikipedia dataset', 'of Wikipedia', 'Wikipedia methods', 'W iki', '-Wiki', 'Wikipedia-based algorithm', 'structured Wikipedia data', 'Wikipedia ( Gold )', 'Wikipedia2', 'Wikipedia 2 corpus', 'Wikipedia ( Wiki ) 2', 'Dataset WIKI-2', 'WIKI-2 dataset', 'Wikipedia corpus 2', 'Wiki-39', 'Wiki-39 dataset', 'Wiki 8', 'Wikipedia , 8', 'Wikipedia data 9', 'Wikipedia2 6', 'WIKIPEDIA-12', 'Wikipedia 12', '+wiki', '+Wiki', 'Wiki0', 'WIKI-02', 'wikipedia 4', 'Wikipedia corpus 4', 'Wiki80 dataset', 'wikipdedia', 'WIKIPEDIA-08', 'WIKIPEDIA-09', 'Wikipedia 6 data', 'Wikipedia corpus 6', 'Wikipedia 6 corpus', 'Wiki 3', 'Wikipedia [ WIKI ] 3', 'Wikipedia dataset 3', 'Wiki data 3', 'Wkipedia', 'Wikipedia ∗', 'Wikipedian']"
8,20,Method,17.2739,2012,"{'2012': -0.0015, '2013': 0.7507, '2014': 2.181, '2015': 7.1392, '2016': 13.1406, '2017': 14.2614, '2018': 17.2739, '2019': 14.494, '2020': 10.4206, '2021': 6.576, '2022': 4.0378}","['RNN', 'RNNs', 'RNN model', 'RNN models', 'recursive neural networks', 'recursive neural network', 'RNN-based models', 'RvNN', 'RNN-based model', 'RNN architecture', 'Recursive Neural Networks', 'recursive neural network ( RNN )', 'Recursive Neural Network', 'Recursive neural networks', 'recursive neural network model', 'recursive neural models', 'C-RNN', 'Recursive Neural Network ( RNN )', 'RNN-based architectures', 'TD-RvNN', 'recursive neural networks ( RNNs )', 'RNN ( f )', 'RNN method', 'RNN systems', 'RNN @ 1', 'RNN classifier', 'Recursive neural models', 'RvNN-based models', 'rnns', 'RvNN models', 'recursive neural network architecture', 'recursive neural network models', 'recursive neural networks ( RNN )', 'RNN approach', 'RNN-based classifiers', 'RNN @ 5', 'Recursive NN', 'Recursive Neural Network ( RvNN )', 'recursive neural network ( RvNN )', 'RvNN model', 'recursive neural model', 'Recursive NNs', 'Recursive neural network', 'RNN ( s )', 'RNN algorithm', 'recursive neural networks ( RvNN )', 'Recursive Neural Network architecture', 'recursive neural network approach', 'structured RNNs', 'structured RNN', 'RvNN based models', 'Recursive Neural', 'RNN ( o )', 'DC-RNN', 'RNN @ 50', 'C & C + RNN', 'TD RNN', 'TD-RvNN model', 'recursive NN', 'recursive NNs', 'RNN ( Recursive Neural Network )', 'Recursive neural network ( RNN )', 'Recursive Neural Network model', 'Recursive Neural Network Model', 'Recursive neural network models', 'Recursive Neural Network models', 'model—RNN', 'Recursive Neural Networks ( RNN )', 'Recursive Neural Models', 'RvNN-based model', 'Recursive Neural Model', 'recursive neural', 'recursive neural networks model', 'recursive neural network based method', 'recursive neural network based methods', 'RvNN-based methods', 'recursive neural architectures', 'recursive neural network based approaches', 'Recursive Neural Networks models', 'CVG ( RNN )', 'RNN ‘ CVG ( RNN )', 'ral Network ( RNN )']"
9,3,Metric,15.9303,2002,"{'2002': 0.0108, '2003': 1.5851, '2004': 1.8239, '2005': 2.8031, '2006': 4.0442, '2007': 5.8353, '2008': 5.5892, '2009': 7.9038, '2010': 6.5303, '2011': 8.1417, '2012': 8.7794, '2013': 10.1646, '2014': 7.9065, '2015': 7.2782, '2016': 7.5373, '2017': 7.5579, '2018': 10.4179, '2019': 13.0923, '2020': 14.3901, '2021': 14.4457, '2022': 15.9303}","['BLEU', 'Bleu', 'bleu', 'ble', 'BLEUs', 'S-BLEU', 's-BLEU', 'BLEU-based', 'BLEU )', 'B LEU', 'corpus-BLEU', 'BLEU-', 'BLE', 'BLEUS', 'BLEU-S', 'BLEU-s', '/BLEU', 'BLEU ( % )', 'BLEU BLEU', 'BLEU algorithm', 'BLE methods', 'BLEU System', 'BLEU ( BLEU )', 'BLEU=', 'BLEU-point', 'BLEU-metric', 'System BLEU', '% BLEU', '( BLEU', 's-BLEUs', 'Bleu-S', 'BLEU ( BLE )', 'ble )', 'Bleu )', 'BLEU.', 'BLEU %', 'BLEU method', 'BLEU measures', 'BL EU', '( BLEU )', 'BLEU ( dev )', 'BLEU Model', 'Metrics BLEU', 'BLEU—and', 'dev-BLEU', 'BLEU-Based Method', 'BLEU BASE', 'BLEU-cased']"
10,57,Method,13.8425,2020,"{'2020': 1.7106, '2021': 6.1673, '2022': 13.8425}","['T5', 'T5 model', 'T5-base', 'T5-large', 'T5 models', 'T5-11B', 'T5-Large', 'T5-3B', 'T5-Base', 'T5-small', 'T5-large model', 'VL-T5', 'T5-base model', 'T5 base', 'NQG-T5', 'T5-seq', 'QAGen-T5', 'CG-T5', 'T5-3B model', 'T5-BASE', 'T5-Large model', 'T5-11B model', 't5-base', 'T5 transformer', 'T5-small model', 'CAE-T5', 'T5-3b', 'T5-11B-poem', 'T5-based models', 'T5- 11B', 'T5-Small', 'T5 architecture', 'T5-11b', 'SED-T5', 'T5-L', 'T5-FT', 'T5-Base model', 'T5 framework', 'T5-large models', 'T5-v1.1-base', 'TAB-T5', 'T5 1.1', 'T5 BASE', 'T5-based model', 'T5 FLUTE', 'T5-ind', 'T5-style models', 'NTR ( T5 )', 'T5- base', 'T5-based', 'T5-xl', 'T5 11B', 'CNL-T5', 'T5-3B-poem', 'PLOG ( T5-base )', 'T5 ∗', 'T5- small', 'T5-LARGE', 't5-large', 'T5-base models', 'T 5 -XL model', 'CG-T5 model', 'T5 + MF model', 'T5-FID', 'PLOG ( T5-large )', 'T5 + I3D', 'T5-CLAPS', 'NL-T5', 'T-5 model', 'T5 large', 'T5- large', 'T5 LARGE', 'T5 base model', 't5-base model', 'T5-Base models', 'T5 2', 'text-to-text transfer transformer', 'T5- Large', 'T5 )', 'T5- base model', 't5-base models', 'T5-large )', 'T5 large model', 'T5-Base ]', 'T5-XL', 'T5 3', 'T5-O-K', 'GenMC T5', 'T5 3B', 'T5-3B models', 'T5-3B *', 'T5-C', 'T5 + MF', 'T5-Post', 'RA-T5', 'VL-T5 models', 'T5-base-prompt', 'T5-m', 'T5-OOV', 'T-5', 'text-to-text-transfer-transformer', 'T5-models', 'T-5 models', 'T5- *', 'T5-', 'T5 BASE model', 'T5-LARGE model', 'T5-Large models', 'T5 based', 'T5 transformers', 'T5-v1.1-Base', 'T5- XL', 'T5-XL model', 'T5-3B-All', 'VL-T5 C', 'T5-O-1', 'T5 + 1f I3D', 'T5-Large 770M', 'MinTL ( T5-small )', 'NQG-T5 approach', 'T5- 3B model', 'T5-LB', 'T5-O-5', 'T5-3BDef', 'SE T5', 'T5- 11B-poem', 'T5-11B + SSM', 'T5-base ROC', 'T5-base 223M', 'T5 + C', 'T5-11B-MOD', 'T 5 -XXL ( 11B )', 'T5 LF', 'text-to-text transfer transformer model—which', 'T5 FT', 'T5- FT', 'T5 + RGX', 'T5- m', 'T5 M', 'T5-based text2text transformer', '“ T5 for QA ” model', 'GenMC T5-U', 'T5 LARGE + GenMC', 'T5 b-1m', 'T5 S + M', 'T5-Large size', 'QA model ( T5 )', 'T5-base QA models', 'T5 b-2m', 'T5-V1.0', 't5', 'Text-to-Text Transfer Transformer', 'T 5', 'T5-model', 'T 5 model', 'text-to-text transfer transformer model', 'T5- BASE', 'T5 Base', 'T5- Base', 'T5- * models', 'T5 Models', 'T5_small', 'T5- Small', 't5-small', 'T5 SMALL', 'T5- LARGE', 't5- large', 'T5 Transformer', 'T5 *', 'T5 ( base ) model', 'T5 BASE models', 't5- base models', 'T5 base models', 'T5- BASE models', 'T5-Large *', 'T5-large -', 't5-large model', 'T5- large model', 'T5 Small model', 'T5 transformer model', 't5- large models', 't5-large models', 'T5- based model', 'T5 system', 'T5 based models', 'T5 S', 'T5 method', 'T5 ( small & base )', 'T5-base systems', 'T5-small systems', 'T5-small models', 'T5-small system', 'base T5 model', 'T5 BASE based models', 'T5 LARGE based models', 'T5-based approach', 'T5 Base/Large', 'T5-based method', 'base-T5', 'T5-Base model architecture', 'T5-base/large model', 'model—T5-base', 'RAG-T5-Large', 'T5 FLUTE models', 'T5 FLUTE model', 'T-20 T-5 DAR', 'T5 + Ours', 'T5-based EL model', 'T5- v1.1-base', 'T5-v1.1-large', 'T5-v1.1- large', 'T5-V1.1', 'VL-T5- * ” models', 'T5 task', 'T5 enc Tagging', 'T5 XL', 'T 5 -XL', 'T5-XL models', 'T5 enc', 'Q-TOD ( T5- Base )', 'Q-TOD ( T5-Large )', 'T5 ( Text-ToText Transfer Transformer )', 'Text-toText Transfer Transformer ( T5 )', 'text-totext transfer transformer ( T5 )', 'Text-toText Transformer ( T5 )', 'T5-3B-All model', 'T5-small model 3', 'T5 O-1', 'T5 model ( Textto-Text Transfer Transformer ) 5', 'T5-DE', 'T5 + CBS', 'T5-ConFiT', 'T5-base ( 220 )', 'T5-like', 'T5-like architecture', 'T5 ( Raf', 'T5 model ( Raf', 'T5 GEN-IN', 'T5- seq', 'T5 ( 11B', 'T5- 11B model', 'T5-11b ) model', 'T5-11B 11B', 'T5 ( 11B )', 'T5 ( FT both )', 'T5-3B† 3B', 'T5-base model 5', 'T5 model 5', 'T5-Base 220m', 'T5 Base 220M', 'T5-XL 3B', 'T 5 -XL ( 3 B )', 'T5-Large 770m', 'T5-3B-List', 'T5- style', 'textto-text transfer Transformer', 'textto-text transfer Transformer ( T5 )', 'NQG-T5-Base', 'T5- 3B', 'Ours ( T5-3b', 'T5 ( 3B )', 'T5-3B †', 'T5-base 12', 'T5 ( T5QR )', 'SSR-base T5-large', 'T5 223M-2.8B', 'T5 C', 'T5 O-5', 'T5-O-5 model', 'T5 Small 60M', 'T5-Large model ( + MF )', 'T5-Base + MF', 'T5- Base + MF model', 'T5-Base + MF model', 'T5-Large + MF model', 'T5-FID base', 'FiD ( T5-base )', 'T5-base Swifts', 'T5-BLEU', 'set—T5-11B-poem', 'T5 LMA 770M', 'T5-SmNQ', 'T5-lmadapt', 'T5-L model', 'T5-based dialog model', 'SE16-T5', 'T5-base 6', 'T5-Base 6', 'T5 6', 'T5-O-3', 'all-t5-base-v1', 'T5 ( 512 )', 'T5 ” models', 'BC-T5', 'T5-based joint models', 'NQG-T5-3B', 'T5-Small + HLSTM', 'FLAN-T5-11B', 'T5-base ( C2S-T )', 'T5 ( adapted )', 'VL-T5 model', 'VL-T5 ( Base )', 'T5 + C model', 'T5-Base + C model', 'Q-TOD ( T5-3B )', 'T5DG T5-DG', 'T5-DG', 'T 5 labels', 'T5 ans', 'T5 eq', 'GPT/T5', 'T5-CSL', 'T5-base +17.95', 'NQG-T5 19', 'T5-base-cont', 'T5-cont', 'T5-large-cont', 'T5-small-cont', 'T5-blocks', 'T5 + I3D models', 'T5 PT+FT model', 'T5 PT+FT models', 'T5-base JPR', 'bio-T5 models', 'T5 B', 'T5-base ( 120M )', 'T5-11B-MOD model', 'T5-11B †']"
11,24,Method,13.2745,2015,"{'2015': 0.4013, '2016': 3.5073, '2017': 5.5601, '2018': 11.29, '2019': 13.2745, '2020': 9.0288, '2021': 6.4541, '2022': 4.512}","['BiLSTM', 'BiLSTMs', 'BLSTM', 'biLSTM', 'BiLSTM model', 'BILSTM', 'bidirectional LSTMs', 'bidirectional LSTM', 'bidirectional LSTM ( BiLSTM )', 'BiLSTM models', 'HR-BiLSTM', 'biLSTMs', 'BLSTMs', 'B-LSTM', 'bilstm', 'bidirectional LSTM model', 'BiLSTM network', 'biLSTM model', 'BLSTM model', 'Bidirectional LSTM', 'BiLSTM architecture', 'BiLSTM-Max', 'BLSTM-BLSTM', 'BiLSTM-max', 'ASP-BiLSTM', 'bidirectional-LSTM', 'BILSTMs', 'LSTM ( BiLSTM )', 'BiLSTM networks', 'BiLSTM-A', 'BILSTM+BIA', 'DR-BiLSTM', 'BiLSTM +', 'bidirectional LSTM ( biLSTM )', 'T-biLSTM', 'BiLSTM )', 'MS-BiLSTM', 'PGN-BiLSTM', 'bidirectional LSTMs ( BiLSTMs )', 'BiLSTM-based', 'BiLSTM-based models', 'L-biLSTM', 'Bidirectional LSTMs', 'BILSTM model', 'Bidirectional LSTM ( BiLSTM )', 'BiLSTM *', 'BiLSTM classifier', 'S-BiLSTM', 'C2F-BiLSTM', 'biLSTM models', 'BiLSTM-based systems', 'meta-BiLSTM', 'BILSTM+ARE', 'stacked BiLSTMs', 'T-biLSTMs', 'TC-BiLSTM', 'BiLSTM MLMET', 'b-LSTM', 'BiLSTM-based model', '=BiLSTM', 'LSTM/BiLSTM', 'BiLSTM ( G )', 'H-biLSTM', 'GTS-BiLSTM', 'MTIN-BiLSTM', 'AT-BiLSTM', 'Ext-BiLSTM', 'BILSTM-SEQ', 'BILSTM classifier', 'TUPA BiLSTM', 'CIFG-BLSTM', 'J-BiLSTM', 'H-biLSTMs', 'BiLSTM parser', 'BiLSTM-Fea', 'stacked BiLSTM', 'M-BILSTM', 'BLSTM-local', 'BiLSTM-concat', 'bLSTM', 'Bidirectional LSTM ( BLSTM )', 'BLSTM models', 'BiLSTM architectures', 'BiLSTM Max-out', 'A-BiLSTM', 'HA-BiLSTM', 'BILSTM-GLOVE', 'Flat-BiLSTM', 'BiLSTM-LSTMd', 'meta-BiLSTM model', 'SA-BiLSTM', 'CA-BILSTM-BILSTM', 'C-BiLSTM', 'BiLSTM-sum', 'BiLSTM-LVM', 'BiLSTMS', 'bidirectional LSTMs ( BiLSTM )', 'bidirectional LSTM ( BLSTM )', 'BiLSTMs )', 'BiLSTM system', 'BiLSTM method', 'C-BLSTM100', 'A-BiLSTM model', 'HA-BiLSTM model', 'BiLSTM 2', 'BiLSTM-self', 'HR-BiLSTM model', 'BiLSTM-UANet', 'U2P-BiLSTM', 'Char BiLSTM', 'shared BiLSTM', 'Shared BiLSTM Model', 'BiLSTM-Mean', 'BILSTM-CTX', 'C2P-BiLSTM', 'BiLSTM * ‡', 'BiLSTM-T', 'BiLSTM-C', 'deep BiLSTMs', 'Deep BiLSTM', 'deep BiLSTM', 'B-LSTM p', 'STL-BiLSTM', 'BiLSTM-AT', 'BiLSTM-EuroEmb', 'BiLSTM Concat', 'BilSTMs', 'bidirectional LSTMS', 'BiLstm', 'Bidirectional LSTM model', 'bidirectional LSTM ( B-LSTM )', 'Bidirectional LSTM network', 'biLSTM network', 'bidirectional LSTM network', 'bidirectional LSTM networks', 'BILSTM-based model', 'bidirectional LSTM models', 'bidirectional LSTM architecture', 'BLSTM-BLSTM model', 'biLSTM system', 'LSTM-BLSTM', 'BiLSTM approach', 'DAM BiLSTM', 'BiLSTM-WC', 'GloVe-BiLSTM model', 'BiLSTM2', 'BiLSTM ( src )', 'BiLSTM-WSD4', 'BiLSTM-WSD1', 'BiLSTM 1', 'BiLSTM-we', 'BiLSTM + all', 'Ptr BiLSTM', 'Meta-BiLSTM', 'BILSTM+CON', 'BiLSTM-based parsers', 'L-biLSTM ( 2 ) -S', 'L-biLSTM ( 2 )', 'BiLSTM-Fea models', 'BiLSTM-last', 'DL-BiLSTM model', 'PG + BLSTM', 'stacked BiLSTM model', 'QA-BILSTM', 'L-biLSTMs', 'BAbI 1 ( BiLSTM )', 'bAbI 1 ( BiLSTM )', 'BiLSTM-based parsing models', 'ST-BLSTM', 'BiLSTM-O', 'Deep BiLSTMs', 'BiLSTM ( B )', '+BiLSTM', 'BiLSTM + BiLSTM', 'BiLSTM 3', 'AT-BiLSTM model', 'local classifiers ( BiLSTM )', 'Ext-BiLSTM model', 'Feature-BiLSTM', 'BiLSTM+SWN-Lex model', 'bidirectional LSTM ( BILSTM-SEQ )', 'BiLSTM BCN', 'BiDirectional LSTM', 'BILSTMS', 'Bidirectional-LSTM', 'BiLSTm', 'bidirectional lstm', 'B-LSTM model', 'BiLSTM Model', 'bidirectional-LSTM model', 'LSTMs ( B-LSTM )', 'LSTMs ( biLSTMs )', 'LSTMs ( BLSTM )', 'bidirectional LSTMs ( BLSTM )', 'bidirectional-LSTM ( BiLSTM )', 'bidirectional-LSTM ( BLSTM )', 'BiDirectional LSTM ( BiLSTM )', 'Bidirectional LSTM ( b-LSTM )', 'Bidirectional LSTM ( biLSTM )', 'bidirectional LSTM ( BILSTM )', 'bidirectional LSTMs ( BLSTMs )', 'BiLSTM-network', 'BLSTM network', 'BILSTM-', 'BILSTM )', 'BLSTM ( )', 'bidirectional LSTM )', 'biLSTM )', 'BiLSTM- ,', 'BiLSTM - -', 'biLSTM (', 'BiLSTM-based system', 'biLSTM ) networks', 'biLSTM networks', 'biLSTM classifier', 'BiLSTM based model', 'biLSTM-based model', 'B-LSTM models', 'bidirectional LSTM Models', 'bidirectional LSTM network ( biLSTM )', 'bidirectional LSTM network ( B-LSTM )', 'Bidirectional LSTM network ( BiLSTM )', 'BILSTM-based', 'BILSTM-BASED', 'biLSTM-based', '-biLSTM', 'bidirectional LSTM-based approach', 'BiLSTM-based approach', 'BiLSTMs architecture', 'BiLSTM s', 'BiLST M', 'biLST M', 'bidirectional LSTM ( BILSTM ) model', 'bidirectional LSTM ( BLSTM ) model', 'Bidirectional LSTM ( B-LSTM ) model', 'bidirectional LSTM ( BiLSTM ) model', 'BiLSTM-based methods', 'BiLSTM based models', 'biLSTM-based models', 'BiLSTM-based networks', 'BILSTM system', 'bidirectional LSTM models ( BiLSTM )', 'bidirectional LSTM ( biLSTM', 'bidirectional LSTM ( BiLSTM', 'layered BiLSTM models', 'BiLSTM based systems', 'BiLSTM methods', 'bidirectional LSTM methods', 'bidirectional LSTM ( BILSTM ) classifier', 'short-term memory ( BiLSTM )', 'bidirectional LSTM architectures', 'LSTMs ( BILSTMs', 'bidirectional BiLSTM architecture', 'LSTM networks ( BiLSTMs )', 'BiLSTM-based method', 'BiLSTM classification model', 'BiLSTM classifiers', 'bidirectional LSTM model ( BLSTM )', 'BLSTM approach', 'BiLSTM ( Ours )', 'BILSTM approaches', 'base BILSTM model', 'bidirectional LSTM networks-based model', 'BiLSTM-layer', 'bidirectional LSTM ( biLSTM ) models', 'BiLSTM based architecture', 'BiLSTM-based architecture', 'BiLSTM-based architectures', 'layered biLSTM model', 'BiLST Ms', 'BiLSTM-based approaches', 'BiLSTM-layers', 'BiLSTM-based network', 'DR-BiLSTM ( DR ( S ) )', 'BiLSTM-4096', 'BiLSTM max-out model', 'BiLSTM Max-out method', 'BiLSTM-M3T', 'GloVe-BiLSTM Model', 'BiLSTM-GloVe', 'H-biLSTM ( 2 ) -S', 'H-biLSTM ( 2 )', 'BLSTM2', 'bidirectional LSTM ( BiLSTM2 )', 'bidirectional LSTM ( BiLST M )', 'BiLSTM-pre', 'BiLSTM sent', 'BiLSTM para', 'BiLSTM+2ATT model', 'biLSTM task models', 'multilayer BiLSTM', 'multilayer biLSTMs', 'multilayer BiLSTMs', 'multi-layer BiLSTMs', 'GloVe BiLSTM-Max', 'bidirectional graph LSTM model', 'BiLSTM ( LS T M ( wc ) )', 'BiLSTM ( LS T M ( sc ) )', 'bidirectional LSTM-Jump', 'BiLSTM-based LASER', 'BILSTM+', 'bidirectional LSTMs ( J )', 'BiLSTM ’ s', 'BiLSTM models 1', 'biLSTM 1', 'BLSTM1', 'DET-BLSTM', 'LW-BiLSTM', 'flat BiLSTM', 'BiLSTM ranking model', 'S2 rel ( BLSTM )', 'BiLSTM-based text model', 'BLSTM-global', 'Global BiLSTM', 'Global biLSTM based', 'C2F-BiLSTM-B', 'T-biLSTM ( 2 ) -S', 'T-biLSTM ( 2 )', 'T-biLSTM ( 2 ) -S model', 'T-biLSTM-S ( 2 ) )', 'BiLSTM ( Chiu', 'BiLSTM ( 128 )', 'bidirectional LSTM en', 'UFET-biLSTM', 'UFET-biLSTM †', 'BILSTM-ATTG', 'BiLSTM-rand', 'B-LSTM rand', 'uni-directional LSTM model', 'BiLSTM ⇧ G 100d', 'CA-BILSTM-BILSTM @ N', 'CA-BILSTM-BILSTM @ C', 'biLSTM 9', 'BiLSTM 9', 'SL-BiLSTM', 'SL-BiLSTM models', 'BiLSTM-Max model', 'BiLSTM-Max (', 'X-BILSTM-LAST', 'X-BiLSTM-last', 'Meta-BiLSTM Model', 'Meta-BiLSTM model', 'Meta-BiLSTM architecture', 'BiLSTM + Glove ( z )', 'HR BiLSTM', 'HR-BiLSTM *', 'BiLSTM ( Φ )', 'BiLSTM ( tgt ) BiLSTM ( src )', 'BiLSTM ( tgt )', 'Src BiLSTM W', 'BiLSTM-based parser', 'BiLSTM parsers', 'global biLSTM based GLE model', 'BiLSTM ( +up )', 'BiLSTM–depth', 'BiLSTM–hidden', 'BiLSTM-mix', 'L-biLSTM-S ( 2 )', 'BILSTM+BIA models', 'BILSTM+BIA model', 'DL-BiLSTM Architecture', 'U2P-BiLSTM models', 'BiLSTM+GRN', 'bilstm-num', 'Char-BiLSTM', 'char-BiLSTM', 'BiLSTM Char', 'bilstm-shape', 'shared bidirectional LSTM ( BiLSTM )', 'shared BiLSTM model', 'stacked BILSTMs', 'stacked BiLSTM models', 'stacked BiLSTM network', 'SC-BILSTM', 'SC-biLSTM model', 'BiLSTM-based MIMO', 'BiLSTM W N', 'BiLSTM MAST', 'BiLSTM-MAST', 'BiLSTMs + EMLoL', 'BiLSTMs + EMLo', 'BiLSTM ( 512d )', 'MS-BiLSTM )', 'BILSTM m', '“ + BiLSTM', 'BiLSTM-last/max', 'BILSTM-CTX )', 'BILSTM-CTX model', 'PGN-BiLSTM model', 'L-biLSTM model', 'BiLSTM COL )', 'BiLSTM * ‡+CG', 'F1 BiLSTM', 'Glove+BiLSTM', 'BiLSTM †', 'JL BiLSTM', 'multilayer BILSTM ’ S', 'MP-BiLSTM', 'bidirectional short-term memory networks', 'BILSTM m=2', 'bidirection LSTM', 'BiLSTM A-TCN', 'biLSTM ( T-LSTM )', 'T-biLSTM models', 'T-biLSTM )', 'bidirectional LSTMs ( BL )', 'MS-BiLSTM ( C1 )', 'BILSTM @ C', 'C-BiLSTM *', 'C-biLSTM', 'C-BLSTM', 'BLSTM ( C-BLSTM )', 'DR-BiLSTM (', 'DR-BiLSTM )', 'DR-BiLSTM model', 'DR-BiLSTM models', 'BiLSTM bg', 'deep bidirectional LSTMs ( BiLSTMs )', 'deep bidirectional LSTM ( BiLSTM )', 'deep BILSTM', 'deep biLSTM', 'deep biLSTM architectures', 'deep bidirectional LSTM model', 'deep BiLSTM architecture', 'BiLSTM 8000', 'biLSTM ~w + ~c', '+BiLSTMs', 'BILSTM +', 'BiLSTM-XR-Dev', 'BiLSTM-XR', 'BiLSTM 78.1', 'BiLSTM 13', 'BiLSTM 1k', 'BiLSTM 2k', 'BiLSTM 4k', 'BiLSTM 8k', 'BiLSTM 16k', 'BiLSTM 32k', 'F-BiLSTM )', 'F-BiLSTM', 'bidirectional chain-LSTM', 'LCAM-BiLSTM', 'X-BiLSTMs', 'X-BiLSTM-max', 'BiLSTM3', 'BLSTM p', 'B-LSTM p model', 'bilstm-aux', 'Mul-BiLSTM', 'bidirectional LSTM ( STL )', 'ABCD biLSTM', 'L-biLSTM ( 2 ) -G', 'BiLSTM-MAMT', 'bidirectional LSTMs 4', 'bidirectional LSTM ( BiLSTM ) 4', 'BiLSTM ( L15 )']"
12,68,Metric,13.1292,2005,"{'2005': -0.0633, '2006': 0.1177, '2007': 0.2728, '2008': -0.0736, '2009': 0.2083, '2010': 0.061, '2011': 0.1015, '2012': -0.0175, '2013': 0.2579, '2014': 1.4226, '2015': 1.7432, '2016': 3.4541, '2017': 5.1036, '2018': 7.7886, '2019': 9.9206, '2020': 11.9847, '2021': 13.1292, '2022': 12.483}","['cross-entropy', 'cross entropy', 'crossentropy', 'Cross-Entropy', 'Cross-entropy', 'Cross Entropy', 'CrossEntropy', 'cross entropy ( CE )', 'cross-entropies', 'cross-entropy method', 'Cross entropy', 'cross-entropy model', 'ce', 'Crossentropy', 'CROSS-ENTROPY', 'cross-entropy approach', 'cross − entropy', 'Cross-Entropy method', 'cross-entropy )', 'cross-entropy models', 'crossentropy approach', 'cross-entropy based method', 'cross\ue010entropy', 'H CE', 'cross-entropy ( M', 'cross-entropy ( CCE )', 'Cross entropy ( 2M )', 'cross-entropy based models', 'crossentropy model', 'Cross-entropies', 'crossentropy based methods', 'cross-entropy Method', 'cross-entropy (', 'cross-entropy ( ` )', 'cross-entropy measure', 'cross-entropy based model', 'cross entropy models', 'Cross-entropy Model', 'crossentropy-based method', 'cross entropy classification', 'cross-entropy network', 'cross-entro py', 'cross entropy based methods', 'cross-entropy based methods', 'cross-entropy principle', 'Cross -The Cross Entropy Method Entropy Method', 'cross-entropy-based', 'Cross Entropy Entropy', 'R F ce', 'cross-entropy—are', 'cross entropy 4', 'crossentopy']"
13,21,Method,13.1167,2015,"{'2015': -0.0158, '2016': 2.3976, '2017': 6.7092, '2018': 10.3758, '2019': 13.1167, '2020': 11.8215, '2021': 12.666, '2022': 11.7323}","['Seq2Seq', 'seq2seq model', 'seq2seq', 'sequence-to-sequence model', 'Seq2Seq model', 'seq2seq models', 'sequence-to-sequence models', 'sequence-to-sequence', 'Seq2Seq models', 'SEQ2SEQ', 'Seq2seq', 'sequence-tosequence model', 'SEQ2SEQ model', 'SEQ2SEQ models', 'sequenceto-sequence model', 'Seq2seq model', 'sequence-to-sequence architecture', 'Seq2seq models', 'sequenceto-sequence models', 'sequence to sequence model', 'seq2seq framework', 'sequence-tosequence models', 'Seq2Seq framework', 'sequence to sequence models', 'Sequence-to-sequence models', 'seq2seq architecture', 'sequence-to-sequence framework', 'seq2seq approach', 'sequenceto-sequence', 'seq-to-seq model', 'Seq2Seq architecture', 'sequence to sequence', 'seq-to-seq models', 'seq2seq approaches', 'sequence-tosequence', 'seq2seq architectures', 'seq-to-seq', 'seq2seq methods', 'sequence-to-sequence approach', 'sequence-to-sequence modeling', 'sequence-to-sequence architectures', 'Seq2Seq Model', 'Seq2Seq methods', 'sequence-to-sequence approaches', 'sequenceto-sequence architecture', 'sequence-tosequence framework', 'Seq2Seq Models', 'seq2seq systems', 'sequence-to-sequence methods', 'seq2seq modeling', 'seq2seq network', 'sequence-tosequence architecture', 'Sequence-to-Sequence Model', 'seq2seq-based models', 'sequence-to-sequence ( S2S ) models', 'seq2seq-based model', 'seq2seq frameworks', 'seq-to-seq architecture', 'Seq2Seq-based models', 'sequence-to-sequence network', 'sequenceto-sequence framework', 'sequence-to-sequence ( S2S ) model', 'Seq2seq Model', 'Sequence-to-Sequence model', 'Sequence-to-sequence model', 'Seq-to-Seq', 'Sequence-to-Sequence', 'seq2seq-based', 'Seq2seq approach', 'seq2seq system', 'sequenceto-sequence approach', 'sequence-tosequence architectures', 'sequence-tosequence approach', 'sequence-to-sequence ( S2S )', 'Sequence to Sequence', 'Sequence to sequence', 'sequence to sequence framework', 'Seq2Seq based models', 'seq2seq-based methods', 'Seq2Seq architectures', 'Seq2Seq - base model', 'SEQ2SEQ approach', 'SEQ2SEQ systems', 'Seq2Seq network', 'sequence-to-sequence method', 'seq2seq point', 'sequenceto-sequence ( S2S )', 'sequenceto-sequence approaches', 'sequenceto-sequence network', 'sequence-tosequence methods', 'sequence2sequence model', 'Sequence-to-sequence', 'Sequence-to-Sequence models', 'sequence to sequence approach', 'Seq2Seq method', 'seq2seq )', 'Seq2Seq )', 'seqto-seq model', 'sequence to sequence architectures', 'sequence-to-sequence networks', 'seq2seq networks', 'sequenceto-sequence architectures', 'Sequenceto-Sequence models', 'Sequenceto-sequence models', 'sequenceto-sequence networks', 'sequence-tosequence modeling', 'sequence-tosequence network', 'sequence-to-sequence ( S2S ) architecture', 'sequence-tosequence ( S2S ) models', 'Sequence to Sequence Model', 'Sequence to Sequence Models', 'Sequence-to-Sequence Models', 'Seq2seq architecture', 'Seq2Seq-based model', 'Seq2Seq based model', 'Seq2Seq frameworks', 'Seq2Seq approaches', 'seq2seq method', 'seq2seq based models', 'Seq2seq methods', 'SEQ2SEQ methods', 'Seq2Seq-based methods', 'Seq2seq architectures', 'Seq2Seq-based', 'seq2seq-based systems', 'base sequence-to-sequence model', 'base seq2seq model', 'base SEQ2SEQ model', 'seq2seq2seq', 'seq2seq based approach', 'seqto-seq models', 'sequence to sequence network', 'seq2seq-based approaches', 'seq-toseq models', 'Seq2Seq system', 'sequence-to-sequence system', 'seq2seq point models', 'Sequenceto-Sequence', 'sequenceto-sequence modeling', 'Sequenceto-sequence methods', 'sequenceto-sequence ) methods', 'sequenceto-sequence systems', 'sequence-to-sequence ”', 'sequence-to-sequence ” models', 'Sequence-tosequence models', 'Sequence-toSequence models', 'Sequence-toSequence model', 'sequence-tosequence method', 'base sequence-tosequence', 'sequence-tosequence approaches', 'sequence-tosequence based', 'sequence-tosequence frameworks', 'S2S Sequence-to-sequence framework', 'S2S Sequence-to-sequence model', 'S2-S1', 'sequence to sequence ( S2S ) model', 'Sequence-to-Sequence ( S2S ) model', 'sequence-to-sequence ( s2s ) model', 'sequence-2-sequence framework', 'Sequence-to-Sequence ( S2S )', 'Sequence-to-sequence ( S2S )', 'sequence-to-sequence model ( S2 )', 'Sequence2Sequence', 'sequence to sequence models ( S2S )', 'sequence-to-sequence MMT models', 'sequence-tosequence ( S2S ) methods', 'seq2Seq model', 'SEQ2SEQ Model', 'seq2seq ) model', 'Seq2Seq ) model', 'Sequence to sequence model', 'Sequence to Sequence model', 'sequence-to-sequence Model', 'Seq-to-Seq Model', 'Sequence-to-sequence Model', 'seq2seq-models', 'Sequence-To-Sequence', 'SEQ 2 SEQ', 'Seq-to-seq', 'Seq-To-Seq', 'Sequence to Sequence models', 'Sequence to Sequence Approach', 'Sequence-to-Sequence Approach', 'seq2seq models )', 'seq-to-seq framework', 'Seq2seq method', 'Seq2seq-based models', 'Seq2seq based models', 'Seq2Seq based methods', 'Seq2seq-based methods', 'seq2seq based methods', 'SEQ2SEQ Framework', 'SEQ2SEQ framework', 'Seq2seq framework', 'SEQ2SEQ architectures', 'seq2seq based', 'seq2seq-based system', 'Sequence-to-Sequence Modeling', '( Seq2Seq )', 'Sequence-to-sequence methods', 'sequence-to-sequence Dataset', 'sequence-to-sequence dataset', 'sequence-to-sequences models', 'layered seq2seq architectures', 'Seq2Seq modeling technique', 'Seq2Seq pointing framework', 'Seq2Seq Approach', 'SEQ2SEQ )', 'seq2seq *', 'Seq2Seq (', 'Seq2seq )', 'seq2seq2seq models', 'Seq2Seq modeling', 'Seq2seq modeling', 'SOTA seq2seq', 'SOTA Seq2Seq', 'large seq2seq models', 'Seq2Seq based approach', 'Seq2seq systems', 'SEQ2SEQ network', 'Seq2seq network', 'seq-to-seq strategy', 'Seq2Seq framework ( Seq2Seq )', 'sequence-to-sequence model ( dev )', 'sequence to sequences', 'sequence to sequence based architecture', 'Seq2Seq-based architecture', 'base SEQ2SEQ', 'base seq2seq', 'seq2seq structure', 'architectures—sequence-to-sequence models', 'sequence-to-sequence based approaches', 'Seq2Seq modelling', 'sequence-to-sequence modelling', 'seq2seq modeling framework', 'seq2seq based framework', 'seq2seq-based framework', 'seq2seq network architecture', 'Seq2Seq module', 'sequence-to-sequence structure', 'sequence-to-sequence model-based method', 'sequence-to-sequence techniques', 'seq2seq modelling approach', 'seq2seq techniques', 'sequence-to-sequence systems', 'seq2seq point model', 'based seq2seq models', 'Seq2Seq ( SS )', 'sequence-to-sequence QA models', 'sequence-to-sequence QG model', 'sequence-tosequence model ( DR )', 'sequence-tosequence model 13']"
14,42,Method,12.8073,2020,"{'2020': 1.9102, '2021': 8.0675, '2022': 12.8073}","['BART', 'BART model', 'BART-large', 'BART-base', 'BART models', 'BART-Large', 'S-BART', 'BART-large model', 'BART-based models', 'Bart', 'BART-base model', 'BART large', 'BART base', 'BART-LARGE', 'BART-Base', 'BART )', 'Structured-BART', 'BART LARGE', 'BART Large', 'BART-Large model', 'BART-based model', 'BART classification', 'Bart-large', 'BART-BASE', 'BART-based', 'Bart-base', 'Bart-Large', 'BART large model', 'BART architecture', 'BART-base )', 'BART Base', 'BART-large )', 'bart', 'BART ( large )', 'BART base model', 'BART BASE', 'BART-based systems', 'BARTs', 'BART-large models', 'bart-base model', 'BART-Base model', 'Bart-Base', 'BART ( base )', 'BAR T', 'Bart model', 'Bart models', 'bart-large', 'BART large architecture', 'BART BART', 'BART Large model', 'BART LARGE model', 'bart-large model', 'bart-base', 'BART-S', 'BART-Base/Large', 'BART-', 'BART ( ours )', 'large BART model', 'BART large models', 'BART-based methods', 'BART classification model', 'base BART', 'BART-base models', 'S-BART models', 'base BART model', 'BART - Ours', 'BART classifier', 'BART framework', 'bart.large', 'BART _large', 'BART-based method', 'BART Large architecture', 'Bart-base model', 'BART-Large *', 'BART-based architectures', 'BART architectures', 'BART based', 'BART based models', 'BART based systems', 'BART-base/large', 'BART (', 'BART=', 'Ours BART', 'BART-based system', 'BART-based architecture', 'large BART models', 'BART cased base model', 'BART cased large model', 'BART-Large models', 'BART-LARGE models', 'Bart-large models', 'BART-Large-based', 'BART-Large based', 'BART systems', 'BART ( bart-large ) model', 'BART technique', 'Base BART', 'BART system', 'S-BART model', 'BART module', 'Structured-BART models', 'BART-base architecture', 'BART-small']"
15,54,Method,12.772,2014,"{'2014': 0.0092, '2015': 2.4827, '2016': 4.4022, '2017': 5.6927, '2018': 10.3769, '2019': 12.772, '2020': 8.4974, '2021': 7.2587, '2022': 4.2231}","['GloVe', 'Glove', 'GLOVE', 'GloVe model', 'glove', 'GloVE', 'GloVe models', 'GloVe )', 'GLoVe', 'glove/', 'gloves', 'GloVe algorithm', 'GLoVE', 'GloVe 7', 'Glove model', 'GloVe 4', 'Glove )', 'GloVe-based', 'GloVe-based models', 'GloVe-based model', 'GloVe 3', 'Glove 7', 'Glove models', '-Glove', 'glove dataset', 'Glove 3', 'glove,3', 'glove package 4', 'Glove 4', 'GloVe , 4', 'GloVe framework 4', 'GlOVE', 'GLOVE model', 'glove/ Model', 'GloVe-based methods', 'Glove-based methods', 'GloVe package', 'glove package', 'glove algorithm', ':glove', '- GloVe', 'Model GloVe', 'glove/ Model Corpus', 'GloVe Ours', 'GloVe method', 'GLoVe methods', 'GloVe-based system', 'GloVe data set', 'GloVe approach', 'Glove Classifier', 'glove 3', 'GloVe3']"
16,30,Method,12.4753,2016,"{'2016': -0.289, '2017': -0.1738, '2018': 4.3099, '2019': 11.0166, '2020': 11.2072, '2021': 12.4753, '2022': 11.4181}","['self-attention', 'self-attention mechanism', 'SAN', 'SANs', 'Self-Attention', 'Self-attention', 'self attention', 'self-attention networks', 'self-attention network', 'self-attentions', 'SANS', 'self-attention model', 'self-attention mechanisms', 'self-attention models', 'Self Attention', 'self-attention module', 'self attention mechanism', 'Self-attention mechanism', 'SAN model', 'Self-Att', 'Self-Attention Mechanism', 'self-attention based models', 'self-attention architecture', 'gated self-attention', 'Self-attention networks', 'self-attention strategy', 'self-attention networks ( SANs )', 'self attentions', 'structured SANs', 'self-attentional model', 'Self-Attn', 'Self-Attention Networks', 'Self-attention networks ( SANs )', 'Self-Attention Network ( SAN )', 'SAN-2', 'gated self-attention mechanism', 'SELF-ATT', 'self-attn', 'Self-attention mechanisms', 'self-attention maps', 'self-attentional models', 'self-attentional networks', 'self-att', 'self attention module', 'Self-Attention Network', 'self-attention-based', 'self-attention network ( SAN )', 'SAN-based models', 'self-attention based model', 'HTT SAN', 'CT-SAN', 'XL SANs', 'self-attention-B', 'self-attention-K', 'self -attention', 'Self-attn', 'Self attention', 'self- attention mechanism', 'Self-Attention mechanism', 'self attention network', 'Self-Attention model', 'self-attention-based models', 'self-attention classifier', 'self-attention method', 'self-attention based architecture', 'self-attention based', 'Self-attention Network ( SAN )', 'self-attention-based approaches', 'self-attention-based model', 'structured self-attention mechanism', 'self-attention architectures', '-Self-attention', 'SAN-based', 'self-attentional approach', 'self-attentional', 'SELF-ATT ( · )', 'Gated Self Attention', 'gated self-attention network', 'gated self-attention modeling mechanism', 'gated self-attention architecture', 'attention + SELF', 'MG-SANs', 'SAN 1', 'Self Attn', 'Self-ATT', 'SELF ATTENTION', 'self Attention', 'self- attention', 'Self-Attention module', 'self-attention ) mechanism', 'Self-attention Mechanism', 'Self Attention mechanism', 'Self-attention Networks', 'Self-Attention Models', 'Self-attention Networks ( SANs )', 'Self-attention networks ( SAN )', 'self-attention networks ( SAN )', 'Self-Attention network', 'Self-attention network', 'Self-Attention Model', 'Self-Attn model', 'self attention mechanisms', 'self-attention ( s )', 'self-attention-based )', 'self-attention approaches', 'self-attention based classifier', 'Self-Att ( Self-attention Networks )', 'Self-Attention Based', 'self-attention ( SAN )', 'structured self attention technique', 'Self-attention network ( SAN )', 'SAN SAN', 'self-attention model )', 'Self-attention based model', 'structured self attention network', 'Self-Attention Networks ( SANs', 'attention networks ( SANs )', 'self-attention approach', 'SAN methods', 'self-attention-based classification', 'Structured Self Attention', 'Structured-Self-Attention', 'Self-attention architectures', 'structured self-attention mechanisms', '=Self-Attention', 'self-attention frameworks', 'self-attention network models', 'Self-attention-based model ( Self-attn )', 'SANs-based', 'Self-Attention-based method', 'self-attention classifiers', 'self-attention-based networks', 'Self-Attention module ( Self-ATT )', 'Self-attention ( Base )', 'self-attention methods', 'self-attention structures', 'self-attention strategies', 'self-attention )', 'self-attention-based framework', 'attention-to-self', 'Self Attention ( a )', 'QA self-attention model', 'self attention gen', 'SAN 2', 'self-attention network ( 6 \ue00d )', 'self-attetion', 'self-attentional mechanisms', 'self-attentional modeling', 'self-attentional architectures']"
17,13,Method,10.8358,2012,"{'2012': 0.0529, '2013': 0.5685, '2014': 1.391, '2015': 3.9093, '2016': 10.8358, '2017': 10.1585, '2018': 9.1464, '2019': 7.8238, '2020': 5.3701, '2021': 3.8512, '2022': 2.5834}","['RNN', 'RNNs', 'recurrent neural networks', 'recurrent neural network', 'RNN model', 'recurrent neural network ( RNN )', 'RNN models', 'recurrent neural networks ( RNNs )', 'recurrent neural networks ( RNN )', 'Recurrent Neural Network ( RNN )', 'RNN-based models', 'Recurrent Neural Networks ( RNNs )', 'RNN-based', 'RNN-based model', 'Recurrent Neural Network', 'Recurrent Neural Networks', 'rnn', 'Recurrent Neural Networks ( RNN )', 'RNN architectures', 'RNN-T', 'RNN architecture', 'S-RNN', 'Recurrent neural networks ( RNNs )', 'FA-RNN', 'Recurrent neural networks', 'RNN1', 'RNN_HG', 'RNNS2S', 'RNN HG', 'recurrent neural network model', 'recurrent neural models', 'a-RNN', 'RNN-based methods', 'recurrent neural network models', 'set-RNN', 'TG-RNN', '∆-RNN', 'T-RNN', 'recurrent neural network architectures', 'DD-RNN', 's-RNN', 'Recurrent neural network ( RNN )', 'RNN+FC', 'd-RNN', 'RNN2', 'RNN-based approaches', 'recurrent neural network architecture', 'RNN-based architectures', 'FA-RNNs', 'RNN 0', 'Recurrent neural network', 'RNN based models', 'RNN )', 's-RNNs', 'Recurrent neural networks ( RNN )', 'recurrent neural architectures', 'recurrent neural network ( RNN ) model', 'VSA-RNN', 'RNN 4', 'MD-RNN', 'B-RNN-A', 'Rnns', 'RNN method', 'RNN network', 'big RNN model', 'RNN systems', 'RNN-VGS model', 'LC-RNN', 'recurrent neural networks ( RNNs', 'RNN-', 'RNN methods', 'RNNs approach', 'RNN approaches', 'RNN system', 'RNN classifier', 'RNN @ 1', 'RNN +', 'MD-RNNs', 'B-RNN', 'rnns', 'S-RNNs', 'RNN-based approach', 'RNN networks', 'RNN-based method', 'RNN framework', 'd-RNN model', 'MM-∆-RNN', 'RNN-QA', 'recurrent NN', 'Recurrent neural networks ( RNNs', 'rnn model', 'recurrent neural model', 'recurrent neural', 'RNN approach', 'recurrent neural network ( RNN ) models', 'RNN-based classifiers', 'recurrent networks ( RNNs )', 'recurrent neural architecture', 'recurrent neural networks approach', 'RNNS2Ss', '∆-RNNs', 'RNN 1', 'RNN1 system', 'RNN-80', 'RNN-320', 'RNN ( Q )', 'RNN R', 'RNNS', 'recurrent neural network-based models', 'RNNs )', 'RNN-based neural network', 'rnn models', 'recurrent ) neural network models', 'RNN-small', 'recurrent neural network ( RNN', 'recurrent neural network ( RNN ) architectures', 'Recurrent Neural Network ( RNN ) model', 'recurrent neural network ( RNN ) architecture', 'RNN-GOLD', 'RNN cell', 'RNN ’ s', 'm-RNN', 'm-RNNs', 'RNN-VGS', 'RNNS2S-s', 'RNN-xF1', 'RNN2- ( W2V )', 'RNN-HG', 'RNN1- ( W2V )', 'C & C ( + RNN )', 'RNN CLM', 'RNN-C', 'HL-RNN', 'Recurrent Neural network', 'Recurrent NNs', 'RNNs-based models', 'RNN based model', 'recurrent neural network-based model', 'RNN ( recurrent neural network )', 'Recurrent Neural Network ( RNNs )', 'RNN-based systems', 'recurrent NN architecture', 'Rnns.', 'RNN based methods', 'recurrent neural network based architecture', 'RNN-based architecture', 'Recurrent Neural', 'RNN algorithm', 'recurrent neural network approach', 'RNN-based neural network models', 'RNNs ( Recurrent Neural Networks )', 'Recurrent Neural Network ( RNN ) models', 'Recurrent networks ( RNNs )', 'neural network ( RNN', 'structured RNNs', 'Recurrent Neural Network ( RNN ) -based methods', 'neural recurrent models', 'RNN ( RNN S )', 'recurrent neural networks model', 'RNN-based classifier', 'RNN frameworks', '=set-RNN', 'DD-RNN model', 'RNN u', 'RNN-W', 'V1 RNN', 'V2 RNN', 'RNN_CLS', 'CH-RNN', 'RNN ( D )', 'RNN 3', 'RNN-2', 'RNN2 system', 'RNN ( RNN2 )', 'RNNS2S model', 'RNNS2S s', 'RNNS2S models', 'RNN ( V )', 'RNN-1', 'RNN1 systems', 'RNN-T.', 'FB-RNN', 'recurrent neural network-based ones', 'RNN HG model', 'RNN HG )', 'JTR RNN models', 'rnn +C', 'Recurrent Neural Networks ( R NN )', 'R-RNN', 'RNN 2d', 'RNN ( C )', 'RNN ( RNN C )', 'Recurrent Neural Networks ( c )', 'I-RNN I-RNN', 'APC RNN', 'recurrent neutral network', 'RNN @ 4', 'RNN NLU', 'RNN PSG', 'RNN ( b )', 'recurrent ( b ) neural networks', 'RNN-LG', 'recurrent NNs', 'RECURRENT NEURAL NETWORK', 'Recurrent Neural Network-based approaches', 'recurrent neural network based models', 'Recurrent Neural Network based models', 'rnn-based models', 'RNN -based models', 'recurrent neural network based model', 'Recurrent neural network ( RNNs )', 'recurrent NNs ( RNNs )', 'recurrent-neural-network-based', 'recurrent ) neural networks', 'RNN Architecture', 'RNN-architecture', 'Recurrent neural network architecture', '( RNN ) )', 'RNN ( )', 'RNN (', 'RNN -', 'Recurrent Neural Network model', 'recurrent neural-network model', 'RNN Model', 'Recurrent Neural Network Model', 'RNNs model', 'RNNs models', 'RNN Models', 'Recurrent Neural Network models', 'Recurrent Neural Network Models', 'Recurrent neural network architectures', 'recurrent NN architectures', 'recurrent neural network-based approach', 'recurrent neural network based approach', 'RNN-based system', 'Recurrent neural models', 'Recurrent Neural Models', 'RNN Networks', 'RNN based method', 'recurrent neural network architectures ( RNNs )', 'RNN-S', 'RNN S', 'RNN-based network architecture', 'RNN-big', 'small RNN model', 'RNN-big models', 'RNN-big systems', 'RNN based architecture', 'Recurrent ( RNN )', 'recurrent ( RNN )', 'neural networks ( RNN )', 'RNN-based network', 'recurrent neural network methods', 'recurrent neural network ( RNN ) -based models', 'Recurrent neural network ( RNN ) -based models', 'recurrent neural network-based ( RNN )', 'Recurrent Neural Network-based ( RNN )', 'RNN NN', 'Recurrent Neural Network ( RNN', 'RNN-RNN', 'Recurrent Neural Network RNN', 'RNNs RNNs', 'RNN RNNs', 'recurrent neural system', 'Recurrent Neural Network Approach', 'recurrent neural network approaches', 'Recurrent neural network ( RNN ) architectures', 'RNN-based neural network model', 'Recurrent neural network models ( RNNs )', 'recurrent neural network models ( RNN )', 'Recurrent Neural Network ( RNN ) Model', 'RNN ( recurrent neural network ) model', 'Recurrent neural network based architectures', 'recurrent neural networks ( RNN ) models', 'Recurrent Neural Networks ( RNNs ) models', 'network ( RNN )', 'recurrent neural networks ( S-RNNs )', 'Structured RNN', 'recurrent neural network ( RNN ) -based methods', 'RNN-based model architectures', 'RNN-based models )', 'RNN-based modeling', 'RNN-Base', 'recurrent neural networks ( RNN s )', 'recurrent based neural network models', 'Recurrent Neural Network ( RNN ) Dataset', 'RNN cells', 'RNN based )', 'RNN techniques', 'NN-based recurrent models', 'neural network ( RNN )', 'RNN-gold', 'recurrent neural network ( RNN ) -based approaches', 'RNN-based networks', 'recurrent neural cells', 'Classifier ( RNN )', 'Recurrent Neural Modeling', 'recurrent neural network frameworks', 'Recurrent Neural Networks Approach', 'recurrent neural methods', 'RNN-layers', 'recurrent neural network model ( RNN )', 'RNN-based structures', 'RNN-based frameworks', 'recurrent neural network classifiers ( RNN )', 'RNN classifiers', 'recurrent neural network ( RNN ) classifier', 'RNN-based framework', 'set-RNN method', 'set-RNN )', 'w rnn i', 'RNN @ 8', 'RNNs ’', 'RNN-based mapping', 'w/ RNN', 'rnn w/', 'RNN+FC method', 'RNN + FC', 'P RNN', 'θ rnn', 'RNN ( a )', 'RNN a-RNN', 'k-layer RNN', 'RNN M', 'M-RNN', 'M RNNs', 'RNN R 2', 'RNN R 3', 'RNN R M', 'L-layer recurrent neural networks', 'RNN PKU', 'FA-RNN systems', 'FA-RNN system', 'RNN w2v', 'non-recurrent neural network', 'non-recurrent neural architecture', 'non-recurrent neural model', 'non-RNN models', 'non-recurrent neural networks', 'RNN-based tagging model', 'RNN ( PG ) models', 'PG model ( rnn )', 'd-RNNs', 'DM-RNN', 'RNN2 0', 'α-RNN', 'RNN 2', 'RNN2-', 'RNN @ 2', '∆-RNN model', 'RNN ( RNN1 ) models', 'RNN ( RNN1 )', 'rnn +C h', 'RNN-IW', 'RNN ∗', 'p-RNN ( V + C )', 'f RNN', 'RNN-F', 'GF-RNN', 'RNN-T models', 'RNN-T T-T', 'RNN2- ( W2V ) models', 'RNN QA model']"
18,40,Method,10.6631,2019,"{'2019': 0.5024, '2020': 5.7703, '2021': 8.4583, '2022': 10.6631}","['GPT-2', 'GPT2', 'GPT-2 model', 'GPT-2 models', 'GPT2 model', 'GPT2 models', 'GPT2-large', 'GPT - 2', 'GPT2-Large', 'GPT2-small', 'GPT2-base', 'GPT-2 Small', 'GPT-2 Large', 'GPT-2 small', 'gpt2', 'GPT-2 )', 'GPT-2-based model', 'GPT-2 based model', 'GPT-2s', 'GPT-2 large', 'GPT2-based models', 'GPT-2 based models', 'GPT2-Small', 'GPT-2 small model', 'GPT-2S', 'GPT-2 architecture', 'GPT2-small model', 'GPT- 2', 'GPT-2-based models', 'GPT-2 ( large )', 'GPT2 base', 'gpt2-large', 'GPT2-LARGE', 'GPT2 architectures', 'GPT2-base model', 'GPT-2 ( small ) model', 'GPT-2 ( small )', 'gpt-2', 'GPT- 2 model', 'GPT-2 systems', 'GPT2 )', 'GPT/GPT-2', 'GPT-2-Small', 'GPT2-based model', 'GPT2-Base', 'GPT2- Large', 'GPT2 dataset', 'models–GPT2', 'GPT-2-based', 'GPT 2', 'gpt2 model', 'GPT - 2 models', 'gpt2 models', 'GPT2-large model', 'GPT-2 LARGE model', 'GPT2-Large model', 'GPT-2 large model', 'GPT2 Large model', 'GPT-2 Large model', 'GPT-2 SS', 'GPT-2-based strategies', 'GPT2 *', 'GPT-2-', 'GPT2-', 'GPT/GPT2', 'GPT2 ( Large )', 'GPT-2 network', 'GPT2 architecture', 'GPT-2 SMALL', 'GPT2 small', 'GPT2 Small', 'GPT-2-small', 'gpt2-base', 'GPT-2-Base', 'GPT-2-base', 'GPT-2 Base', 'GPT-2 base', 'GPT2- base', 'GPT2 LARGE', 'GPT2- large', 'GPT2 large', 'GPT-2 LARGE', 'GPT-2 architectures', 'GPT-2 base model', 'GPT-2 ( base ) model', 'GPT-2 ( base )', 'gpt-2 small model', 'GPT-2 Small model', 'GPT2 small model', 'GPT-2 large models', 'GPT-2 ( gpt-2 )', 'GPT-2 base models ( Base )', 'GPT2-base models', 'GPT-2-based methods', 'GPT2 data', 'models—GPT-2', 'GPT2-S models', 'GPT2-S model', 'GPT-2 based approach', 'GPT2-based', 'GPT-2 Large based models', 'GPT-2 classifier', 'GPT2 models—small', 'GPT-2—the', 'GPT-2-based classifier']"
19,78,Method,10.5735,2011,"{'2011': -0.107, '2012': 0.0241, '2013': 0.4984, '2014': 1.8966, '2015': 3.8589, '2016': 7.2675, '2017': 8.6212, '2018': 9.5544, '2019': 10.5735, '2020': 9.2266, '2021': 7.0342, '2022': 7.113}","['softmax', 'Softmax', 'softmax classifier', 'sof tmax', 'softmax model', 'softmax classifiers', 'SoftMax', 'Softmax classifier', 'SOFTMAX', 'F 2 -Softmax', 'h-softmax', 'softmax method', 'softmax )', 'softmax-based models', 'Softmax Softmax', 'Softmax Softmax Softmax', 'softmax-based classifier', 'F 2 Softmax', 'Softmax model', 'Sof tmax', 'softmax classification', 'softmax approach', 'softmax methods', 'softmax-based', 'h-softmax model', 'Softmax Classifier', 'softmax-based methods', 'Softmax )', 'sof tmax classifier', 'softmax models', 'softmax architecture', 'F 2 -Softmax Models', 'softmax ”', '\ue000 softmax', 'Softmax-τ', 'softmaxτ', 'h-softmax models', 'softmax ( · )', '\uf8ff Softmax', 'Softmax ( RS )', 'softmax ࢝', 'softmax ( f )', 'p softmax', 'Softmax Model', 'softmax ) classifier', 'SoftMax classifier', 'softmax softmax softmax softmax', 'Softmax Softmax Softmax Softmax', 'softmax/softmax', 'softmax softmax', 'softmax softmax softmax', 'Softmax classifiers', 'Softmax method', 'Classification Softmax classifier', 'softmax ( . )', 'Softmax ( . )', 'softmax ( )', 'softmax algorithm', 'Softmax ( s )', 'softmax-based classifiers', '=softmax', '-Softmax', 'Softmax-based approaches', 'softmax-based model']"
20,61,Method,10.4007,2014,"{'2014': 1.9857, '2015': 7.1996, '2016': 10.4007, '2017': 8.8767, '2018': 8.0854, '2019': 7.9368, '2020': 5.052, '2021': 3.8835, '2022': 2.6928}","['word2vec', 'Word2Vec', 'Word2vec', 'WORD2VEC', 'word2vec model', 'word2vec models', 'Word2Vec model', 'word2vec/', 'word2vecf', 'word2vec algorithm', 'word2vec-D', 'word2vec 5', 'word2Vec', 'word2vec 3', 'Word2Vec method', 'Word2vec model', 'word2vec approach', 'word2vec method', 'WORD2VEC models', 'word2vec package', 'word2vec )', 'Word2Vec models', 'Word2Vec 3', 'K word2vec', 'Word2Vec approach', 'Word2Vec )', 'word2vec-based', 'word2vec algorithms', 'word2vec ( r )', 'word2vec 8', 'Word2VecF', 'word2vec 7', 'Word2Vec *', 'Word2vec models', 'Word2Vec algorithm', 'word2vec corpus', 'Word2Vec 5', 'Word2vec 8', 'Word2vecf', 'word2vecf 5', 'word2vecf 7', 'word2vecf 7 model', 'Word2Vec 7', 'Word2vec 7', 'Word2vec ’', 'WORD2VEC *', 'Word2vec method', 'word2vec Method', 'WORD2VEC method', 'word2vec/ Model', 'WORD2VEC model', 'WORD2VEC-based methods', 'word2vec-based methods', 'Word2Vec-based methods', 'WORD2VEC package', 'Word2vec algorithm', 'WORD2VEC dataset', 'word2vec dataset', 'word2vec system', 'Word2Vec system', 'Word2Vec technique', 'word2vec technique', 'Word2Vec approaches', 'word2vec-based system', 'word2vec-based algorithm', 'word2vec-based approaches', 'WORD2VECF 3', 'word2vec3', 'WORD2VEC 3', 'word2vec package 3', 'Word2Vecs', 'word2vec package 5', 'Word2Vec 8', 'Word2vec model 8']"
21,45,Method,10.3091,2019,"{'2019': 0.9186, '2020': 5.7447, '2021': 8.6713, '2022': 10.3091}","['MLM', 'MLMs', 'masked language model', 'masked language modeling ( MLM )', 'masked language models', 'masked language model ( MLM )', 'masked language modeling', 'Masked Language Modeling ( MLM )', 'Masked Language Model ( MLM )', 'mlm', 'Masked Language Model', 'masked language modelling ( MLM )', 'masked language models ( MLMs )', 'MLM model', 'masked LMs', 'Masked Language Modeling', 'MC-MLM', 'MLM models', 'C-MLM', 'O-MLM', 'Masked Language Models', 'masked language modelling', 'Masked Language Models ( MLM )', 'CA-MLM', 'Masked language modeling ( MLM )', 'Masked Language Models ( MLMs )', 'masked language modeling ( MLM', 'Masked Language Modelling ( MLM )', 'MLM-based', 'M-MLM', 'masked language models ( MLM )', 'mask language model', 'D-MLM', 'MLM-based models', 'Masked LM', 'Masked Language Modelling', 'MLM method', 'L MLM', 'MLM 3', 'MLM12', 'masked LM ( MLM )', 'mask language modeling', 'Masked language model', 'mask language model ( MLM )', 'mask language modeling ( MLM )', 'MLM- CLS', 'MLM12-LM', '+NT-MLM', 'Masked language modeling', 'Masked LM ( MLM )', 'MLMS', 'masked LM', 'masked-LM', 'Masked language models ( MLMs )', 'masked language model ( LM )', 'MLM )', 'MLM approach', 'masked language', 'language mask', 'MLM methods', 'MLM- [ CLS ]', 'MLM- [ CLS ] model', 'ID-MLM', 'masked LM ” ( MLM )', 'masked-language modeling', 'Masked language model ( MLM )', 'Masked LMs', 'Masked-Language Model', 'Mask Language Model', 'mask-LM', 'Mask-LM', 'masked-language modelling', 'masked language model ( MLM', 'MLM classifier', 'Language Modeling ( MLM )', 'masked language model ( masked LM )', 'MLM-based model', 'AC-MLM', 'L mlm', 'D-MLM ( MLM', '- - D-MLM', 'w/o MLM', 'MLM ’', 'masked language models 8', 'mlm + L', 'Masked Language Modeling ( MLM ) 1', 'masked language model ( MLM ) 10', 'masked langauge modeling', 'MLM + MLM', 'MLM +', 'u-MLM', 'MLM12 model', 'MLMs 2', 'C-MLM approach', 'C-MLM model', 'C-MLM method', 'w/ MLM', 'AD-MLM', 'MLM—we', 'mask language model ( MLM ) M', 'O−MLM', 'masked language model ” ( MLM )', 'MLM + QT', 'masked-language-modeling', 'Masked language models', 'masked-language-model ( MLM )', 'Masked Language model ( MLM )', 'Masked-Language Model ( MLM )', 'Masked LMs ( MLM )', 'mlms', 'masked-language modeling ( MLM )', 'masked-language model', 'masked-LMs', 'Masked-LMs', 'masked-language models ( MLM )', 'masked-language-models ( MLMs )', 'Masked language models ( MLM )', 'Masked Language Modeling ( MLM ) model', 'Mask-Language-Model', 'Masked Language Modeling ( MLM', 'Masked language modeling ( MLM', 'Masked-Language Modelling ( MLM )', 'Mask Language Model ( MLM )', 'MLM-', 'MLM-based methods', '-MLM', '/mlm', 'MLM framework', 'masked language model )', 'Masked Language Model )', 'masked language models ( MLM', 'Masked LM ( MLM', 'MLM ( masked language modeling )', 'MLM-based approach', 'masked language modeling ( Masked LM )', 'masked language modeling techniques', 'base MLMs', 'Mask Language Modeling ( MLM )', 'masked language modeling )', 'MLM ( masked language model )', 'mask language models', 'masked language modeling ( MLM ) classifier', 'masked language modelling framework', 'Masked Language Modeling ( MLM ) methods', 'language modeling ( MLM )', 'masked language model LM', 'MLM technique', 'masking language model ( MLM )', 'masked language ( MLM )']"
22,66,Method,9.8776,2018,"{'2018': 1.6526, '2019': 9.8776, '2020': 7.6552, '2021': 3.8569, '2022': 1.9659}","['ELMo', 'ELMO', 'ELMo model', 'ELMo models', 'Elmo', 'ELMo2', 'ELMo1', 'ElMo', 'elmo', 'CS-ELMo', 'T-ELMo', 'ELMo )', 'SRC-ELMO', 'ELMo language model', 'NE-LM', 'ELMo CI', 'CS-ELMo model', 'Txt-ELMo', 'R-ELMo', 'wl ( ELMo )', 'ELMO0', 'ELMO model', 'ELMo-based model', 'ELMo ( 4-layer )', 'ELMo-m', 'ELMo-a', 'ELMo-0', 'ELMO models', 'language embedding', 'ELMo-based models', 'TGT-ELMO', 'ELMo 4', 'ELMo LAC', 'ELMo†', 'O-ELMo', 'R-ELMo ( 7 )', 'ELMo 3', 'Language Models ( ELMo )', 'ELMo (', 'ELMo-based', '-ELMo', 'ELMo methods', 'ELMO1', 'F1 ELMo', 'QM + ELMo', 'ELMo-QG', 'ELMo models 7', 'ELMo 7', 'ELMo∗', 'ELMo LAC model', 'ELMo L', 'ELMo.We', 'ELMo tagging model', 'ELMo0', 'ELMO + P', 'ELMo:3', 'ELMo model 3', 'ELMo model ( v2 )', 'eLMo', 'ELMo-based metric', 'language models ( ELMo )', 'ELMo architecture', 'ELMo ?', 'ELMo *', 'ELMo.', 'ELMo ]', 'ELMo-', 'ELMo model—the', 'ELMO-based', 'Embedding/LM', 'ELMo-based Model', 'ELMo small model', 'ELMo-Small model', 'ELMo-based method', 'ELM o', 'Language embedding', '–ELMo', 'embedded language', 'ELMO ( small )', 'ELMo small', 'language embedding network', 'ELMo-based Models', 'ELMo systems', 'Language embedding based method', 'language model-based embedding model', '( ELMo )', 'ELMO S', 'ELMo-based approach', 'ELMo-based system', 'ELMo method', 'ELMo approach', 'ELMo small++', 'ELMo L 0', 'ELMo—at', 'ELMo 6', 'C ELMO', 'C ELMo', 'CS-ELMo models', 'ELMo:1', 'ELMo 1', 'ELMo - 89.3', 'ELMo 5', 'ELMo ⋄', 'ELMo – 78.2', 'LM ( emb )', 'ELMo 2', 'ELMo model 2', 'ELMo-4-layers', 'ELMo ( 4- layer )', 'ELMO T', 'Large + ELMo', 'ELMO +', 'ELMo ELMo +']"
23,31,Method,9.2901,2001,"{'2001': 0.482, '2002': 0.3911, '2003': 1.5785, '2004': 0.8615, '2005': 0.9409, '2006': 2.2089, '2007': 2.2757, '2008': 1.4779, '2009': 2.5313, '2010': 1.7552, '2011': 2.1333, '2012': 2.9831, '2013': 2.8829, '2014': 4.0914, '2015': 3.9314, '2016': 6.669, '2017': 7.9559, '2018': 7.9095, '2019': 8.9027, '2020': 9.1299, '2021': 9.2901, '2022': 8.8591}","['beam search', 'beam-search', 'beam search algorithm', 'Beam search', 'Beam Search', 'beamsearch', 'beam search strategy', 'beam-search algorithm', 'beam search method', 'Beam-search', 'beam search approach', 'beam search methods', 'beam search algorithms', 'beam-search strategy', 'Beam-Search', 'beam search strategies', 'beam search technique', 'beam search mechanism', 'beam-search framework', 'beam search ( BS )', 'beam-search approach', 'beam-search method', 'beamsearch algorithm', 'beam search based methods', 'beam-search model', 'beam search )', 'beam searching', 'BEAMSEARCH', 'beam search models', 'beam search based', 'beam-search approaches', 'beam searches', 'beam-based search', 'ST beam search', 'beam search ( α )', 'beam search 3', 'beam searcher', 'BeamSearch', 'beam-search technique', 'beam search model', 'Beam Search method', 'beam-search models', 'beam-search-based', 'beam search-based methods', 'large-beam beam search', '+ Beam search', 'beam search ( BS', 'Beam search ( BS )', 'Beam Search ( BS )', 'Beam Search ( B )', 'beam search 4', 'beam-search ”', 'beam search 8', 'beam A * search', 'A * beam search', 'beam search—a', 'beam-4 search', 'Beam-10 search', 'beam-searchers', 'beam search ( tg )', 'beam search—are', 'BEAM SEARCH', 'beam ) search', 'beam– search', 'BEAM-SEARCH', 'beamSearch', 'Beamsearch', 'Beam-Search algorithm', 'Beam Search Algorithm', 'Beam search algorithm', 'Beam Search algorithm', 'beam-search strategies', 'beam searched models', 'BeamSearch ( )', 'Beam Search Model', 'beam search Method', 'Beam-Search methods', 'beam-search methods', 'Beam Search )', 'beam-search architecture', 'beam search approaches', 'beamsearch method', 'beamsearch-based methods', 'Beamsearch based methods', 'beam-search algorithms', 'beamsearch approaches', 'beam-search based methods', 'beam-search-based methods', 'beam search-based approaches', 'beamsearch framework', 'beam Beam search', 'beam-searched model', 'beam search based method', 'beam-search based systems', 'beam-search based technique', 'beam-search system', 'beam search based approach', 'beam-searched', 'Value-based Beam Search', '\ue737 Beam Search', 'beam ( =5 ) search', 'beam search ( 5 )', 'beam-1 search ( Beam 1 )', 'beam-1 searches', 'beam-search ( y )', '“ beam search ” technique', 'beam search 7', '\ue02e Beam search', 'beam search 2']"
24,32,Method,8.7896,2007,"{'2007': -0.0557, '2008': -0.3451, '2009': -0.299, '2012': -0.315, '2013': -0.3166, '2014': -0.2235, '2015': -0.2858, '2019': -0.2196, '2020': 0.1419, '2021': 2.3447, '2022': 8.7896}","['PLMs', 'PLM', 'language models ( PLMs )', 'PLMs )', 'KP-PLM', 'language model ( PLM )', 'PLM-based', 'H-PLM', 'Language Models ( PLMs )', 'PLM model', 'V-PLM', 'PLM models', 'PLM-based models', 'PLM-NR', 'T-PLM', 'language models ( PLM )', 'PLM-based metrics', 'PLM M', 'PLM )', 'PLM-based approaches', 'PLM-FT', 'GTS-PLM', 'Pretrained language models ( PLMs )', 'PLM method', 'PLM-based methods', 'PLM-QA', 'FT-PLM', 'HN-PLM', 'NRMS-PLM', 'M-PLM', 'plm', 'Language Models ( PLM )', 'Pretrained language models ( LMs )', 'Language Model ( PLM )', 'PLMs models', 'PLMs-based methods', 'PLM s', 'Pretrained Language Models ( PLMs )', 'PLMs-based', 'PLM-based model', 'PLM-D', 'MPO-based PLM', 'w/o PLM', 'PLM FL', 'MBRpretrained language', 'PLM-based CTG approaches', 'PLM-based GET', 'PLM-based GET framework', 'PLM-based GET model', 'T0 PLMs', 'PLM-based CCF model', 'PLM-G', 'V-PLM model', 'PLM ( CLS )', 'w/ PLM', 'Pretrained-LM', 'pLM', 'LMs ( PLMs )', 'PLMs Models', 'PLM Model', 'PLMs model', 'PLM *', 'LM-pretrained models', 'Pretrained language models ( PLM )', 'languages models ( PLMs )', 'PLMs architectures', 'PLM OURS', 'base PLM', 'PLM-based metric', 'Uni-PLM models', 'PLM 5', 'NAR PLMs', 'KP-PLM framework', 'KP-PLM models', 'mBART PLMs', 'PLM-based DG', 'language models ( PLM ) 1', 'PLM 2', 'PLM y']"
25,51,Method,8.5181,2004,"{'2004': -0.3736, '2005': -0.3084, '2006': -0.3299, '2007': -0.1414, '2008': -0.302, '2009': -0.2961, '2010': 0.0788, '2012': -0.315, '2013': -0.02, '2014': 0.104, '2015': 0.5, '2016': 2.6598, '2017': 5.0216, '2018': 7.29, '2019': 8.4466, '2020': 7.616, '2021': 8.4999, '2022': 8.5181}","['MLP', 'MLPs', 'multi-layer perceptron', 'multi-layer perceptron ( MLP )', 'multilayer perceptron', 'MLP classifier', 'multilayer perceptron ( MLP )', 'Multi-Layer Perceptron ( MLP )', 'MLP model', 'multi-layer perceptrons', 'MLP network', 'Multilayer Perceptron ( MLP )', 'MLP-1', 'MLP models', 'multilayer perceptrons', 'MLP )', 'Multi-Layer Perceptron', 'ABC MLP', 'MLP-IQA', 'Multilayer Perceptron', 'Multi-layer Perceptron ( MLP )', 'mlp', 'multi-layer perceptron ( MLP ) classifier', 'MLP classifiers', 'MultiLayer Perceptron ( MLP )', 'MLP-2', 'MLP-QA', 'MLP-IA', 'MLP-A', 'MultiLayer Perceptron', 'multi-layered perceptron ( MLP )', 'multilayer perceptron ( MLP ) classifier', 'MLP (', 'MLP ( · )', 'multilayer perceptron classifier', 'Multi Layer Perceptron ( MLP )', 'multi-layer perceptron )', 'multi-layer perceptron ( MLP ) network', 'multi-layered perceptron', 'MLP +', 'MLP 1', 'Multi-layer Perceptron', 'multi-layer perceptron classifier', 'MLP Network', 'MLP ( Multilayer Perceptron )', 'MLP S', 'MLP-based', 'MLP systems', 'MLP networks', 'S2S-MLP', 'MLP M', 'MLP 1NN', 'MLP 2', 'MLP ( H n )', 'Multi-Layer Perceptrons', 'MLP-20', 'MLP1', 'MLP ( H )', 'MLP ( q )', 'Multi-layer perceptron', 'Multi Layer Perceptron', 'multi layer perceptron', 'multilayer-perceptron', 'multilayer perceptron network', 'multi-layer perceptron network', 'MLP ( Multi-Layer Perceptron )', 'MLP ( multi-layer perceptron )', 'multi-layer Perceptron ( MLP )', 'Multilayer perceptron ( MLP )', 'MLP mechanism', 'Multi-Layer Perceptron ( MLP ) classifier', 'MLP ( . )', 'MLP s', 'Multi-Layer Perceptron ( MLP ) network', 'MLP architecture', 'multi-layer perceptron methods', 'Multi-layered Perceptron', 'layer perceptron ( MLP )', 'multi-layer perceptron ( MLP ) model', 'MLP-based classifier', 'multilayer perceptron model ( MLP )', 'MLP SG', 'MLP -32', '\ue000 MLP', 'MLP G', 'MLP 3', 'multi-layer perceptron ( “ MLP ” ) model', 'MLP l', 'MLP L', 'Multi Layer Perceptrons', 'Multi-layer perceptrons', 'multi-layer perceptrons classifier', 'multi-layer perceptrons network', 'multi-layered perceptrons', 'all-mlp architecture', 'F 1 MLP', 'MLP ( r )', 'MLP lbl', 'MLP-based ID', 'MLP ( ID )', 'MLP \ue010 h K', 'MLP ( h CLS )', 'MLP-1 model', 'MLP ( 1 )', 'MLP I', '\uf06d MLP Network', 'MLP ( h )', 'MLP ( h t )', 'SP-MLP', 'SP-MLP model', 'MLP q', 'multi-layer-perceptron', 'MLP ) classifier', 'Multilayer Perceptron classifier', 'Multi-Layer Perceptron classifier', 'MLP-classifier', 'MLP ( to )', 'Multi-Layered Perceptron ( MLP )', 'Multilayer Perceptron network', 'Multi-layer Perceptron network', 'MLPs ( multilayer perceptron )', 'MLP ( Multi-layer Perceptron )', 'MLP ( Multi Layer Perceptron )', 'MLP ( multilayer perceptron )', 'multilayer-perceptron ( MLP )', 'Multi-layer perceptron ( MLP )', 'MLP ( MultiLayer Perceptron )', 'MLPs ( multi-layer perceptron )', 'Multilayer Perceptron model', 'multilayer perceptron model', 'multi-layer perceptron model', 'Multi Layer Perceptron ( MLP ) classifier', 'MLP ( multi layer perceptron ) classifier', 'Multilayer Perceptron ( MLP ) classifier', 'Multi-layer Perceptron ( MLP ) classifier', 'multilayer perceptron models', 'MLP method', 'multi-layer perceptron method', 'multilayer perceptron classifiers', 'multi-layer perceptron classifiers', 'MLP :', 'multi-layer-perceptron-based approaches', 'MLP-based approaches', 'layer perceptron ( MLP ) classifiers', 'Multi-Layer Perceptron ( MLP ) model', 'multilayer perceptron ( MLP ) model', 'Multi-Layer Perceptron ( MLP ) networks', 'multilayer perceptron ( MLP ) networks', 'Multi-Layer-Perceptron ( MLP ) Networks', 'multilayer perceptron networks ( MLP )', 'MLP-based systems', 'MLP-based models', 'MLP classification', 'multi-layer perceptron ( MLP ) -based', 'MLP-based method', 'multi-layer perceptron classifier ( MLP )', 'multi-layer perceptron ( MLP ) classifiers', 'MLP ( multi layer perceptron ) classifiers', 'Multi-layer perceptron ( MLP ) based model', '-multilayer perceptron ( MLP )', 'MLP system', 'Multilayer perceptron based approach', 'multilayer perceptron ( MLP', 'base multi-layer perceptron ( MLP ) model', 'MLP approach', 'ABC MLP model', 'MLP classifier model ( CLF )', 'MLP ’ s', 'MLP/LNN', 'θ -MLP', 'MLP 32', 'MLP + +', 'MLP f θ MLP', 'MLP ( v n )', 'MLP ( BoR )', 'MLP-based mapping', 'MLP 6', 'f c−mlp', 'MLP g', 'MLP ( g )', 'MLP 7', 'MLP 9', 'Meta-MLP', 'MLP 4', 'multi layer perceptrons 4', 'φ -MLP', 'MLP3', 'MLP -64', 'MLP 64', 'MLP m', 'MLP k', 'MLP §4', 'exp-MLP', 'z vis layer perceptron ( MLP )', 'ABCD mlp', '\ue010 MLP', 'MLP 2 (', 'MLP2', 'MLP-2 model', 'MLP ( ρ )', 'MLP N', 'multi-layer perception', 'multilayer perception', 'MLP ( ∗ )', 'MLP ∗']"
26,90,Method,8.4416,2012,"{'2012': -0.315, '2013': 0.4914, '2014': 2.2414, '2015': 2.8111, '2016': 5.2192, '2017': 5.7298, '2018': 8.4416, '2019': 8.1767, '2020': 7.2715, '2021': 7.1705, '2022': 6.8162}","['deep learning', 'deep learning models', 'deep learning methods', 'deep learning model', 'deep learning techniques', 'deep learning approaches', 'Deep Learning', 'deep learning architectures', 'Deep learning', 'deep learning framework', 'deep learning approach', 'deep learning architecture', 'deep learning systems', 'deep learning based models', 'deep learning based methods', 'Deep learning models', 'deep-learning models', 'deep learning based approaches', 'deep learning-based methods', 'deep learning method', 'deep learning-based models', 'deep learning technique', 'Deep Learning models', 'deep-learning', 'Deep Learning Models', 'Deep learning methods', 'deep learning networks', 'deep Q-learning', 'deep learning based', 'deep metric learning', 'deep-learning approaches', 'Deep learning approaches', 'deep learning system', 'deep learning based approach', 'deep learning-based', 'Deep Learning techniques', 'Deep learning techniques', 'deep learning-based approaches', 'Deep learning architectures', 'deep learning-based approach', 'deep learning )', 'deep learning frameworks', 'deep learning theory', 'deep-learning model', 'deep learning-based model', 'Deep learning based methods', 'deep-learning architectures', 'deep learn', 'deep learning based systems', 'deep-learning-based systems', 'deep learning classifiers', 'deep learning-based techniques', 'deep Q learning', 'Deep Q-Learning', 'Deep Q-learning Network ( DQN )', 'deep Q-learning algorithm', 'deep Q-learning approach', 'deep leaning methods', 'deep-learning-based models', 'deep-learning methods', 'deep-learning techniques', 'Deep learning systems', 'Deep learning-based approaches', 'deep-learning based approaches', 'deep learning based model', 'deep-learning-based methods', 'Deep learning-based methods', 'deep-learning based methods', 'deep learning-based architectures', 'deep learning-based systems', 'deep learning strategies', 'Deep Learning )', 'deep learning network', 'deep learning based techniques', 'deep learning-based system', 'deep learning-based method', 'deep metric learning based model', 'deep learning ” architectures', 'deep Q -learning', 'Deep Q-learning', 'deep Q-learning network ( DQN )', 'Deep Q-learning algorithm', 'deep Q-learning framework', 'deep Q-learning architectures', 'Deep Q-Learning ( DQN )', 'deep Q-learning ( DQN )', 'deep Q-learning method', 'deep Q-learning model', 'deep leaning model', 'deep leaning-based methods', 'deep learning based QA models', 'deep learning-based QA models', 'deep-learning based models', 'Deep-learning-based models', 'Deep Learning-based models', 'deep learning ) models', 'Deep ) learning models', 'Deep Learning ) models', 'deep-learning framework', 'Deep Learning methods', 'Deep Learning Methods', 'Deep Learning approaches', 'deep-learning systems', 'Deep Learning Model', 'Deep Learning model', 'Deep learning model', 'deep-learning approach', 'Deep Learning approach', 'Deep Learning Based Approaches', 'Deep learning based approaches', 'deep-learning-based approaches', 'Deep Learning based approaches', 'deep-learning-based model', 'Deep learning-based model', 'deep-learning based model', 'deep-learning architecture', 'Deep Learning Architecture', 'Deep Learning Architectures', 'Deep learn', 'Deep learning based systems', 'Deep Learning based systems', 'deep-learning based', 'Deep learning based', 'deep learning—', 'deep-learning based techniques', 'deep learning strategy', 'Deep learning method', 'deep-learning-based system', 'Learning deep architectures', 'deep learning-based classifier', 'deep structured learning technique', 'deep learning classification model', 'deep learning module', 'deep learning packages', 'Deep Learning Theory', 'deep learning-based classifiers', 'deep learning based classifiers', 'deep learning classifier', 'deep learning based analysis', 'deep learning mechanism', 'Deep Metric Learning Data Set', 'deep metric learning based approach', 'Q-Learning ( DQN )']"
27,64,Method,8.4276,2015,"{'2015': 0.0023, '2016': 3.3876, '2017': 7.154, '2018': 8.4276, '2019': 8.3726, '2020': 5.5989, '2021': 3.365, '2022': 1.9732}","['GRU', 'GRUs', 'Gated Recurrent Unit ( GRU )', 'gated recurrent unit ( GRU )', 'GRU network', 'GRU model', 'gated recurrent units ( GRU )', 'GRU networks', 'Gated Recurrent Units ( GRU )', 'GRU cell', 'Gated Recurrent Unit', 'GRU cells', 'Tree-GRU', 'gated recurrent unit', 'GRU-2', 'MBi-GRU', 'GRU-based', 'GRU-GRU', 'A-GRU', 'gated recurrent units ( GRUs )', 'Gated Recurrent Units', 'GRU-RM', 'gru', 'gated recurrent units', 'T-GRU', 'C-GRU', 'GRU+A', 'HMBi-GRU', 'Gated Recurrent Units ( GRUs )', 'GRU )', 'GRU models', 'L-GRU', 'HA-GRU', 'HBi-GRU', 'TeMP-GRU', 'Gated Recurrent Unit ( GRU', 'Gated Recurrent Unit ( GRU ) network', 'GRU classifier', 'GRU-384', 'T-GRUs', 'GRU m', 'gated recurrent units ( GRU', 's-GRU', 'Gated Recurrent Unit networks', 'GRU-based model', 'gated recurrent unit ( GRU ) network', 'GRU r', 'GRU b', 'GRUs +', 'Gated Recurrent Units ( GRU', 'Gating Recurrent Units ( GRU )', 'Recurrent Unit ( GRU )', 'gated recurrent unit ( GRU', 'gated recurrent network ( GRU )', 'GRU architecture', '=GRU', 'Gated Recurrent Unit ( GRU ) cell', 'GRU2', 'GRU-layer=2', 'GRU-EHE', 'GRU c', 'GRU w', 'base-UT + GRU', 'FA-GRU', 'GA w/ C-GRU', 'GA w/ GRU', 'Uni-GRU', 'r-GRUs', 'AGG ( GRU )', 'U GRU', 'U GRU system', 'Base GRU-RM', 'GRU-1', 'GRU ( 50 )', 'n -GRU', 'TRNN-GRU', 'GRU-like', 'De-GRU', 'A-GRU/T-GRU', 'GRU ( GTS )', 'HMBi-GRUs', 'GRU Äî', 'grün', 'grünes', 'grünen', 'grünem', 'GRU +', 'Unit ( GRU )', 'Units ( GRUs )', 'f gru t', 'GRU h t', 'STM ( GRU )', 'MAGE-GRU', 'GRU nets', 'gated-recurrent unit', 'GRU Cells', 'Gated-Recurrent Unit ( GRU )', 'gated recurrent unit ( GRUs )', 'Gated recurrent unit ( GRU )', 'Gated-Recurrent Units ( GRU', 's-GRUs', 'GRU based', 'Gated Recurrent Unit model', 'gated recurrent unit model', 'GRU (', 'GRU *', 'GRU-', 'GRU ) network', 'Gated Recurrent Unit Networks', 'GRU-based architectures', 'recurrent unit ( GRU )', 'GRU-networks', 'GRU architectures', 'GRU modeling', 'GRU based model', 'GRU GRU GRU', 'Recurrent Unit ( GRU ) network', '( GRU )', 'Gated Recurrent Unit ( GRU ) networks', 'Gated Recurrent ( GRU )', 'GRUs ( Gated Recurrent Units )', 'gated recurrent unit ( GRU ) -based models', 'GRU GRU', 'gated recurrent networks ( GRU )', 'GRU-based models', 'GRU based models', 'gated recurrent unit ( GRU ) architecture', 'Gated Recurrent Unit ( GRU ) architecture', 'GRU GRU GRU GRU GRU GRU', 'gate-recurrent-unit ( GRU )', 'Gate Recurrent Unit ( GRU )', 'Gated Recurrent Unit ( GRU ) cells', 'gated recurrent unit ( GRU ) cells', 'GRU-GRU model', 'gated recurrent units ( GRU ) architecture', 'gated-recurrent unit mechanism', 'Gated Recurrent Units ( GRU ) networks', 'Recurrent Units ( GRUs )', 'GRU ( gated-recurrent unit )', 'GRU ( Gated Recurrent Unit )', 'GRU-layer', 'cell GRU', 'GRU s', 'gate recurrent unit ( GRU ) cells', 'GRU Gated recurrent units ( GRU )', 'gated recurrent unit ( GRU ) -based', 'gated recurrent units ( GRUs ) network', 'GRU based method', 'GA + C-GRU', 'GRU-GRU ( GG )', 'GRU 2', 'GRUs 2', 'gated recurrent unit ( GRU ) 2', 'Ely GRU 1', 'αEly GRU', 'GRU 0', 'GRU ( 0 )', 'C-GRUs', 'C-GRU model', 'GRU g', 'GRU ( g )', 'FA-GRUs', 'GRU_5', 'r-GRU', 'GRU ( r )', 'GRU t', 'GRU_T', 't GRU', 'GRU T-GRU', 'GRU-EVE', 'GRU iter cell', 'F1 Base model ( GRU )', 'VA GRU', 'GRU s a', 'GRU ( GRU a )', 'GRU 176K', 'MBi-GRU architecture', 'tree-GRU', 'tree-based GRU', 'tree-GRU network', 'tree-GRU based model', 'Tree-GRU method', 'GRU 7', 'GRU ( w it )', 'GRU 17M', 'GRU-RM models', 'GRU ( GRU-1 )', 'GRU ( 500 )', 'GRU-500', 'α-GRU', 'GRU h', 'GRU_H', 'GRU ( h', 'H-GRU', 'GC gated recurrent unit ( GRU ) network', 'TeMP-GRU )', 'TeMP-GRU model', 'IBFP-GRU', 'IBFP-GRUs', 'Q-GRU', 'Gated Recurrent Unit ( BGRU )', 'GT GRU', 'GRU upd cell', 'GRU ’ s', 'GRU ( L-L )']"
28,246,Method,7.9818,2013,"{'2013': 0.0174, '2014': 1.0463, '2015': 3.5466, '2016': 7.2428, '2017': 6.717, '2018': 7.9818, '2019': 5.3497, '2020': 2.4862, '2021': 1.7567, '2022': 0.8287}","['recurrent neural network ( RNN )', 'recurrent neural networks ( RNNs )', 'recurrent neural networks ( RNN )', 'Recurrent Neural Network ( RNN )', 'Recurrent Neural Networks ( RNNs )', 'Recurrent Neural Networks ( RNN )', 'Recurrent neural networks ( RNNs )', 'recursive neural network ( RNN )', 'GRU-RNN', 'Recursive Neural Network ( RNN )', 'recursive neural networks ( RNNs )', 'recursive neural networks ( RNN )', 'Recursive Neural Networks ( RNNs )', 'recurrent neural network ( RNN ) models', 'recursive recurrent neural network ( R 2 NN )', 'recursive recurrent neural network', 'gated recurrent neural network ( GRNN )', 'Recursive neural networks ( RNNs )', 'recursive/recurrent neural networks', 'Recurrent Neural Network ( GRNN )', 'GRNN ( Gated Recurrent Neural Network )', 'Recurrent Neural Networks ( RNN ’ s )', 'recursive deep neural networks ( RNN )', 'recursive neural network ( RNN ) 1', 'Recursive Neural Network ( RNN ) 1', 'recursive neural networks ( Tree-RNNs )', 'Gated Recurrent Units Recurrent Neural Networks RNN', 'Recursive Recurrent Neural Network ( R 2 NN )', 'RNN ( Recurrent Neural Network )', 'Recursive recurrent neural network', 'Recursive Recurrent Neural Network', 'Recursive neural networks ( rnns )', 'recurrent/recursive neural networks ( RNNs )', 'recursive neural network ( rnn ) model', 'recursive neural network ( RNN ) model', 'Recursive Neural Network ( RNN ) models', 'recursive neural network ( RNN ) approaches', 'recursive neural network model ( RNN )']"
29,50,Method,7.925,2001,"{'2001': 0.0459, '2002': -0.0353, '2003': 0.544, '2004': 0.4399, '2005': 1.2803, '2006': 1.4383, '2007': 2.115, '2008': 1.3572, '2009': 3.3384, '2010': 2.9113, '2011': 4.635, '2012': 2.9639, '2013': 5.0828, '2014': 5.2779, '2015': 7.925, '2016': 6.1313, '2017': 5.88, '2018': 5.7443, '2019': 5.8091, '2020': 3.8124, '2021': 3.4833, '2022': 2.2562}","['bag-of-words', 'BOW', 'BoW', 'bag of words', 'bag-of-words model', 'bag-of-word', 'bow', 'bag-of-words approach', 'bag-of-words ( BOW )', 'bag-of-words models', 'Bag-of-Words', 'bag-of-words ( BoW )', 'Bag-of-words', 'BOW model', 'bags-of-words', 'Bag of Words', 'bag of words model', 'BoW model', 'BOW approach', 'bag of words ( BOW )', 'Bag of words', 'BOWs', 'BOW corpus', 'bag of words approach', 'bag-of-words approaches', 'Bag-of-Words ( BoW )', 'BOW models', 'BoW data', 'bag-of-words methods', 'bag-of-word model', 'Bag of Words ( BoW )', 'Bow', 'BoW models', 'bag-of-words classifier', 'bag of words ( BoW )', 'Bag-Of-Words', 'bag-of-word models', 'bag-of-words method', 'bag-of-word ( BOW )', 'Bag of Words ( BOW )', 'Bag-of-Words ( BOW )', 'Bag-of-word', 'Bag-of-Word', 'bag of word', 'bag of words models', 'bag-of-words ( BOW ) model', 'BOW )', 'BoWs', 'BoW-based', 'bag-of-words ( BOW ) approach', 'bag-of-word approach', 'BoW approach', 'Bag-of-words ( BoW )', 'bag-of-words ( BoW ) model', 'bag-of-word ( BoW )', 'bag-of-words model ( BOW )', 'BOW methods', 'BoW-based models', 'bag-of-words )', 'Bag-of-words ( BOW )', 'Bag-of-Words model', 'BOW ( bag-of-words )', 'BoW )', 'bag-of-word approaches', 'BOW method', 'BoW V', 'Bag Of Words ( BOW )', 'bag-of-words ( bow )', 'Bag-Of-Words ( BOW )', 'Bag of words ( BOW )', 'Bag-of-Words Model', 'bag of words ( BOW ) model', 'BoW Model', 'bag-of-words model ( BoW )', 'bags of words', 'bag-of-word methods', 'BOW-based methods', 'bag of words )', 'bag-of-words (', 'bag of words approaches', 'BoW approaches', 'BOW algorithm', 'L BoW', 'bow3', 'BOW+', 'Bag-of-words model', 'bag-of-Words', 'BoW ( Bag-of-Words )', 'BOW ( Bag-of-Words )', 'bag of word models', 'bag-of-words-based', 'bag-of-word classifiers', 'BOW classifiers', 'BOW-based', 'BOW-based models', 'bag of words methods', 'Bag-of-words approaches', 'BOW framework', 'BOW approaches', 'Bag of Words based method', 'BoW method', 'bag-of-words strategy', 'bag-of-words analysis', 'BoW †', 'Q ( BOW )', 'bag-of-words model ( QL )', 'bag-of-words 7', 'BOW-K', 'K BoW', 'K BOW models', 'bag of words 3', 'bow h bow', 'BOW- β', 'bag of words ( bow )', 'Bag of words ( BoW )', 'bag of words ( Bow )', 'bag–of–words model', 'bag-of-words ) model', 'Bag of Words model', 'Bag Of Words', 'bag of-words', 'Bag of Word', 'BOW ( Bag Of Words )', 'BoW ( bag of words )', 'Bag-of-words models', 'bag of words ) models', 'Bag of words models', 'bag of words ( BoW ) model', 'Bag-of-words ( BoW ) Model', 'Bag of Words ( BOW ) model', 'Bag-of-Words ( BoW ) model', 'BoW ) model', 'Bag-of-word Model', 'bag-of-words based', 'bag-of-words ( BoW', 'bag-of-words BoW', 'bag of words method', 'bag-of-words Method', 'bag of word ( bow )', 'Bag-of-word ( BoW )', 'bag-of-word ( bow )', 'Bag-of-Word ( BoW )', 'BOW–', 'BoW ( )', 'bow )', 'Bows', 'Bag-of-Words approach', 'Bag-of-words approach', 'Bag of Words model ( BOW )', 'Bag-of-words model ( BOW )', 'Bags-of-words', 'BoW methods', 'bag-of-word-based', 'BOW based models', 'BoW-based Models', 'BOW based methods', 'BoW-based methods', 'BoW-based model', 'Bag-of-Words Methods', 'Bag-of-words (', ""bag-of-words ''"", 'Bag of Words approaches', 'bag of words ( BOW ) approach', 'Bag-Of-Words ( BOW ) approach', 'bag-of-words ( BoW ) approach', 'Bag-of-Words ( BOW ) approach', 'bag of word approach', '( BOW )', 'BOW corpora', 'bag-of-words system', 'bag-of-words classification', 'bag-of-words ( BOW ) approaches', 'bow approaches', 'bag-of-words-based method', 'bow-words', 'bag-of-words ( BOW ) techniques', 'BOW metrics', 'bag-of-words ( BOW ) models', 'bag-of-words ( BoW ) models', 'BoW-based approaches', 'BOW-based approaches', 'bag of words classifier ( BOW )', 'BOW classifier', 'bag-of-word classifier', 'bag-of-words ( BoW ) methods', 'Bag of words approach ( BOW )', 'bag of words approach ( BOW )', 'bag-of-words based approach', 'bag-of-word ( BoW ) models', 'bag-of-word method', 'bag-of-words classifiers', 'bag-of-words analyses', 'Bag of Words analysis ( BoW )', 'bag-of-word mechanism', 'bag-of-words sets', 'BoW modelling', 'BoW sets', 'bag-of-word technique', 'BoW technique', 'bag-of-words based model', 'Bag-of-word model ( BOW )', 'bag-of-words-based classifiers', 'bag-of-words classification model', 'bag-of-words model-based approach', 'bags-of-words-based', 'bags-of-words-based methods', 'bag of words systems', 'bags-of-words models–', 'bag-of-words measure', 'bag-of-words measures', 'BOW measure', 'Bag-Of-Words words model', 'bag-of-words technique', 'bag-of-words based models', 'BoW system', 'Bag of Words ( BoW ) corpus', 'v ( BoW )']"
30,33,Method,7.8921,2001,"{'2001': -0.6324, '2002': -0.6346, '2004': -0.3885, '2005': -0.2707, '2006': -0.0229, '2007': -0.1842, '2008': -0.052, '2009': -0.1203, '2010': 0.0995, '2011': -0.2034, '2012': 0.4144, '2013': 0.4377, '2014': 0.6383, '2015': 0.7015, '2016': 0.8434, '2017': 3.6832, '2018': 6.7654, '2019': 7.8921, '2020': 6.4165, '2021': 5.6517, '2022': 5.8251}","['reinforcement learning', 'RL', 'reinforcement learning ( RL )', 'Reinforcement Learning', 'reinforcement learning framework', 'RL model', 'Reinforcement Learning ( RL )', 'Reinforcement learning', 'RL framework', 'reinforcement learning algorithm', 'reinforcement learning approach', 'RL models', 'RL method', 'reinforcement learning method', 'RL algorithms', 'RL methods', 'reinforcement learning model', 'RL-based methods', 'RL system', 'reinforcement learning techniques', 'reinforcement learning methods', 'RL algorithm', 'RL approach', 'RL techniques', 'RL-C', 'CMAS-RL', 'RL-based models', 'reinforcement learning approaches', 'Reinforcement learning ( RL )', 'RL approaches', 'reinforcement learner', 'RL-based', 'reinforcement learning technique', 'RL-based model', 'RL systems', 'RL-CLF', 'RL-based method', 'reinforcement learning strategy', 'RL-based systems', 'reinforcement learning mechanism', 'reinforcement learning based approach', 'RL-Q', 'RL-based approaches', 'reinforcement learning based methods', 'reinforcement learning ( RL ) framework', 'reinforcement learning ( RL ) approach', 'reinforcement learning ( RL ) methods', 'RL strategy', 'D-S-RL', 'CR-RL', 'reinforcement learning system', 'reinforcement learning ( RL ) method', 'reinforcement learning models', 'RL strategies', 'RL loss', 'Rl nps', 'reinforcement learning based framework', 'Reinforcement Learning ( RL ) methods', 'RL-based system', 'RL-DG', 'RL-F1', 'RL R+C model', 'reinforcement learning-based approaches', 'rl', 'reinforcement learning-based methods', 'model-based reinforcement learning', 'model-based RL', 'RL )', 'Reinforcement Learning model', 'RL technique', 'reinforcement learning-based', 'reinforcement learning strategies', 'RL-based strategy', 'reinforcement learning ( RL ) approaches', 'RL-QR', 'reinforcement learners', 'RD-RL', 'FD-RL', 'RL-FF', 'reinforcement learning based approaches', 'Reinforcement Learning framework', 'Reinforcement learning methods', 'model-based RL methods', 'RL mechanism', 'reinforcement learning-based models', 'reinforcement learning architecture', 'RL-based framework', 'reinforcement learning-based framework', 'RL-based approach', 'model-based RL approach', 'reinforcement learning ( RL ) techniques', 'RL-based algorithm', 'reinforcement learn', 'reinforcement learning ( RL ) algorithms', 'RL architectures', 'RL-based architecture', 'reinforcement learning frameworks', 'CASS-RL', 'ED RL', 'RL-BM25', 'RL tools', 'RD++-RL', 'reinforcement-learning', 'RL networks', 'reinforcement-learning based methods', 'Reinforcement Learning Framework', 'reinforcement-learning framework', 'Reinforcement Learning system', 'reinforcement learning )', 'RL :', 'Reinforcement Learning method', 'Reinforcement Learning algorithm', 'RL based models', 'Reinforcement Learning Approach', 'reinforcement learning-based algorithms', 'Reinforcement Learning ( RL ) approach', 'reinforcement learning ( RL ) algorithm', 'reinforcement learning-based approach', 'reinforcement learning-based model', 'reinforcement learning based model', 'Reinforcement Learning ( RL ) techniques', 'Reinforcement learning ( RL ) algorithms', 'G2T.RL', 'RL-RR', 'Abs RL', 'reinforcement learning ( CMAS-RL )', 'RL-based QR model', 'ASTE-RL', 'OONP ( RL )', 'RL-based ( PAL )', 'SL+RL', 'model-in-the-loop reinforcement learning ( RL ) approach', 'model-in-the-loop reinforcement learning framework', 'RL 2', 'Reinforcement Learner', 'reinforcement learners )', 'RL1', 'RL-based DMs', 'RL-DM', 'RL R+C', 'reinforcement learning ( 3b )', 'reinforcement learning ( IPS )', 'RL +', 'RD++RL', 'reinforcement-learning based approaches', 'REINFORCEMENT LEARNING', 'reinforcement Learning', 'reinforcement-learning ( RL )', 'reinforcement learning-based method', 'reinforcement-learning based method', 'reinforcement learning based method', 'Reinforcement learning-based methods', 'RL based methods', 'Reinforcement-learning-based methods', 'Reinforcement learning framework', 'Reinforcement Learning System', 'Reinforcement Learning ( RL ) framework', 'model-based reinforcement learning methods', 'Model-based reinforcement learning', 'Reinforcement learning techniques', 'reinforcement learning ( ? )', 'RL * * *', 'RL- *', 'Reinforcement Learning Mechanism', 'Reinforcement learning approaches', 'reinforcement-learning approaches', 'Reinforcement Learning approaches', 'Reinforcement Learning Model', 'Reinforcement-learning model', 'RL architecture', 'Reinforcement Learning approach', 'reinforcement learning based', 'Reinforcement Learning-based', 'reinforcement learning ( RL ) -based', 'reinforcement-learning ( RL ) approach', 'reinforcement-learning-based framework', 'reinforcement learning ( RL ) strategy', 'Reinforcement Learning ( RL ) algorithm', 'Reinforcement learning ( RL ) methods', 'reinforcement-learning based technique', 'RL Models', 'Reinforcement learning models', 'RL based model', 'Reinforcement Learning based model', '( RL )', 'RL Strategy', 'reinforcement learning-based system', 'Reinforcement Learning-based strategy', 'RL-based strategies', 'model-based reinforcement learning techniques', 'Reinforcement Learning Reinforcement Learning ( RL )', 'reinforcement learning ( RL ) mechanism', 'model-based reinforcement learning approach', 'reinforcement learning theory', 'model-based reinforcement learning approaches', 'Reinforcement learning ( RL ) approaches', 'Reinforcement Learning ( RL ) approaches', 'RL learning', 'Value-based RL approaches', 'reinforcement learning ( RL ) models', 'reinforcement learning-based algorithm', 'Reinforcement Learn', 'model-based RL algorithms', 'RL module', 'reinforcement learning based )', 'Reinforcement Learning ( RL ) algorithms', 'value-based Reinforcement Learning ( RL ) model', 'reinforcement learning based techniques', 'reinforcement learning architectures', 'reinforcement learning-based ( RL )', 'reinforcement for learning', 'value-based RL method', 'reinforcement learning ( RL ) system', 'model-based RL system', 'RL dataset', 'model-based reinforcement learning framework', 'model-based reinforcement learning method', 'reinforcement learning ( RL ) based methods', 'reinforcement learning ( RL ) -based methods', 'reinforcement learning-based ( RL-based ) methods', 'reinforcement learning—It', 'T2G.RL models', 'T2G.RL', 'RL-only model', 'RL A2C', 'A2C RL', 'end-to-end reinforcement learning ( RL )', 'end-to-end reinforcement learning ( RL ) model', 'RL/RR', 'reinforcement leaning', 'renforcement learning', 'walk-based reinforcement learning ( RL ) model', 'J RL', 'reinforcement learning 7', 'flat RL', 'RL-CLF model', 'RL paths', 'PPO-based RL algorithm', 'ing ( RL )', 'RL-QR )', 'RL-based QR', 'Reinforcement Learning ( DCA-RL )', 'DCA-RL', 'DCA-RL model', 'reinforcement learning loss', 'RL ( act )', 'RL ( act ) models', 'EL/RL', 'RL-NK', 'RL-NS', 'reinformcement learning', 'reinforcement learning ( HDQN )', 'RL R', 'reinforcement learning ( ED RL )', 'reinforcement learning ERD', 'reinforcement learning ( CM )', 'CB-based RL model', 'RL ”', 'CRN-RL', 'reinforcement learning approach ( PAL )', 'reinforcement learning-based ranking model', 'RL ranking model', 'reinforcement learning at']"
31,44,Metric,7.8905,2004,"{'2004': 0.2117, '2005': 0.361, '2006': 1.1985, '2007': 0.4941, '2008': 0.9693, '2009': 0.8885, '2010': 1.4655, '2011': 0.7643, '2012': 0.3568, '2013': 1.526, '2014': 1.4082, '2015': 1.7734, '2016': 1.2595, '2017': 2.0971, '2018': 3.5522, '2019': 4.8448, '2020': 5.2006, '2021': 6.1838, '2022': 7.8905}","['ROUGE', 'Rouge', 'ROUGE metrics', 'ROUGE-S', 'rouge', 'ROUGE metric', 'ROUGE measures', 'HROUGE', 'ROUGE package', 'ROUGE measure', 'ROUGEs', 'ROUGE-S *', 'ROUGE-based', 'ROUGE )', 'Rouge-S', 'RO UGE', 'ROUGES', 'ROUGE 6', 'Rouge metric', 'ROUGE-', 'Rouges', 'ROUGE model', 'D ROUGE', 'ROUGE-metric', 'ROUGE-metrics', 'ROUGE-based metrics', 'ROUGE-based approaches', 'rouge-', 'ROUGE (', 'RougeS', 'Rouge-S.', 'Rouge package', 'Rouge measures', 'rouge measures', 'ROUGE-based methods', 'ROUGE Model', 'Rouge systems', 'ROUGE method']"
32,102,Method,7.771,2007,"{'2007': 0.0835, '2008': 0.0729, '2009': 0.8366, '2010': 0.6346, '2011': 0.5993, '2012': 0.6959, '2013': 1.3462, '2014': 2.5315, '2015': 5.624, '2016': 7.771, '2017': 4.6978, '2018': 4.3617, '2019': 2.3333, '2020': 1.1509, '2021': 1.0116, '2022': 0.576}","['SGD', 'stochastic gradient descent', 'stochastic gradient descent ( SGD )', 'stochastic gradient ascent', 'Stochastic Gradient Descent ( SGD )', 'stochastic gradient descent algorithm', 'Stochastic Gradient Descent', 'SGD optimizer', 'stochastic gradient descent ( SGD ) algorithm', 'Stochastic gradient descent', 'SGD algorithm', 'SGD-L1', 'stochastic gradient descent method', 'stochastic gradient decent', 'Stochastic gradient descent ( SGD )', 'stochastic gradient descent algorithms', 'SGD algorithms', 'stochastic gradient descent optimizer', 'SGD method', 'SGD )', 'Stochastic Gradient Descent ( SGD ) optimizer', 'stochastic gradient descent ( SGD ) method', 'stochastic gradient descent ( SGD', 'stochastic gradient descent methods', 'stochastic gradient ascent ( SGA )', 'stochastic gradient ascent algorithm', 'Stochastic Gradient Descent optimizer', 'Stochastic Gradient Descent method', 'stochastic gradient descent ( SGD ) methods', 'stochastic gradient descent ( SGD ) optimizer', 'Stochastic Gradient Descent ( SGD ) algorithm', 'stochastic gradient descend', 'stochastic gradient ascent techniques', 'Stochastic gradient decent', 'stochastic gradient decent algorithm', 'sgd', 'SGD-based algorithms', 'stochastic gradient descent ( SGD ) approach', 'stochastic gradient descent framework', 'gradient descent ( SGD )', 'tic gradient descent ( SGD )', 'SGD-L1 ( Naive )', 'stochastic gradient descent ( DPSGD )', 'stochastic gradient descent first', 'Stochastic gradient ascent', 'stochastic gradient ascent algorithms', 'stochastic gradient ascent method', 'stochastic gradient descent ( SGD ) approach 2', 'SGD-type algorithms', 'Stochastic Gradient Decent optimizer', 'stochastic gradient decent optimizer', 'stochastic gradient decent method', 'SGD-RSVM', 'stochastic ( sub ) gradient descent', 'stochastic sub-gradient descent ( SGD ) approach', 'stochastic gradient descent learning method', 'SGD learning method', 'SGD learning', 'stochastic gradient descent learning algorithm', 'SGD ( VW )', 'stochastic gradient descent ( SGD . )', 'Stochastic Gradient Descent algorithm', 'SGD Algorithm', 'Stochastic gradient descent (', 'Stochastic gradient ( SGD ) approaches', 'Stochastic gradient descent ( SGD ) algorithm', 'gradient descent ( SGD', 'SGD model', 'sgd classifier', 'SGD methods', 'SGD models', 'gradient descent ( SGD ) algorithm', 'stochastic gradient descent based optimizer', 'stochastic gradient descent technique', 'SGD approach', 'stochastic gradient descent ( SGD ) framework', 'K -step stochastic gradient descent', 'stochastic gradient descent 5704', 'stochastic gradient descent ( SGD ) 4', 'stochastic gradient descent-like approach', 'stochastic gradient descent 3', 'stochastic gradient descend algorithm', 'stochastic gradient descenttype algorithms', 'Stochasitc Gradient Descent']"
33,88,Method,7.4071,2013,"{'2013': -0.0714, '2014': 0.3223, '2015': 1.0521, '2016': 1.9378, '2017': 2.2064, '2018': 5.1593, '2019': 6.3172, '2020': 6.9078, '2021': 7.4071, '2022': 6.5947}","['FFN', 'feed-forward network', 'FFNN', 'feed-forward', 'feedforward network', 'feed-forward networks', 'FFNs', 'FNN', 'feed forward network', 'feed-forward network ( FFN )', 'feedforward networks', 'feedforward', 'FFNN-based model', 'FNN model', 'feed forward', 'feed forward networks', 'Feed-Forward Network', 'feed-forward model', 'feed-forward layers', 'Feed-Forward', 'feed-forward architecture', 'FFN ( · )', '+FFNN', 'feed forward network ( FFN )', 'FNNs', 'Feed-Forward Network ( FFN )', 'Feed-forward', 'feed-forward layer', 'feed-forward architectures', 'ffn', 'FFN classifier', 'feed-forward models', 'feed-forward classifier', 'feed-forward networks ( FFN )', 'Feed-Forward Networks ( FFN )', 'feedforward layers', 'Feed Forward', 'Feed Forward Network', 'FFN model', 'feed-forward module ( FFN )', 'Feedforward', 'feedforward architecture', 'feedforward architectures', 'feedforward model', 'feedfoward network', 'Feed-Forward-Network', 'FFNN classifier', 'Feed-forward networks', 'Feed-Forward Networks', 'feed forward layer', 'FFNN model', 'feed forward model', 'Feed Forward model', 'Feed Forward Network ( FFN )', 'feed-forward ( FFN )', 'feed-forward module', 'FFNN-based models', 'feed-forward network FFN', 'FFNN models', 'Base Model ( FFN ) Base Model', 'Feedforward network', '\ue000 FFNN', 'Feed\ue003Forward', 'feedforward ( FF )', 'FFN 1', 'C-FFNN', 'FFN 3', 'FFN ”', 'feedfoward', 'feedfoward networks', 'FFN 2', 'FNNs 2', 'FFN 2 )', 'FFN network', 'Feed-forward network', 'Feed-Forward network', 'Feed-forward Networks', 'feed-forward Networks', 'Feed-Forward model', 'Feed Forward Networks ( FFN )', 'feed-forward networks ( FFNs )', 'feed forward network ( FNN )', 'feed-forward network ( FNN )', 'Feed-forward network ( FFN )', 'feed-forward classifier network', 'feed forward layers', 'Feed-forward layers', 'feed-forward network ( FFN ) classifier', 'Feed-Forward ( FFNN )', 'Feed-forward module', 'feed-forward method', 'FFN-based models', 'FFN-based', 'FFNN-based', 'FNN-based', 'FNN architecture', 'FFNN architecture', 'feed forward architecture', 'feed-forward network ( FFN', 'feed-forward network ( FFNN', 'Feed-Forward Network ( FFN', 'FFNN approach', 'feed-forward approach', 'FFNN-based system', 'feed forward network classifier', 'ffn )', 'feed-forward mechanism', 'feed-forwards', 'feed-forward layer ( FFNN )', 'Small Feed-Forward Network Models', 'feed-forward classifiers', 'feed-forward network method', 'feed-forward network )', 'FNN ( Network )', 'feed-forwards networks', 'FFNN b', 'FFN-ft', 'FeedForward', 'Feedforward Network', 'FeedForward network', 'FeedForward Network', 'feedforward classification networks', 'feedforward modules', 'feedforward layer )', 'Feedforward architecture', 'feedforward system', 'feedforward network architectures', 'feedforward models', 'feedforward classifier', 'feedforward network-based', 'feedforward structure', 'layer feedforward network', 'feedforward layer', 'feed-foward', 'feed-foward networks', 'Feedforward ( FF )', 'FFN ( X )', 'feedforward Q', 'FFNN c']"
34,125,Method,7.3265,2020,"{'2020': 0.2121, '2021': 2.7699, '2022': 7.3265}","['GPT-3', 'GPT3', 'GPT-3 model', 'GPT-3 models', 'GPT3 model', 'gpt-3', 'GPT3-', 'GPT-3 )', 'GPT-3 dataset', 'GPT- 3', 'GPT-3-', 'GPT3 models', 'GPT-3 architectures', 'GPT-3 based systems', 'Gpt-3', 'GPT-3 ( )', 'Gpt-3 models', 'GPT-3 architecture', 'GPT-3 base models', 'model–GPT-3', 'GPT-3 systems', 'GPT-3 methods', 'GPT3s', 'GPT3-based', 'GPT-3-Small', 'GPT-3-based models']"
35,165,Method,7.1246,2018,"{'2018': 0.1145, '2019': 2.3244, '2020': 5.0575, '2021': 7.1246, '2022': 6.3616}","['Transformer encoder', 'transformer encoder', 'TE', 'Transformer decoder', 'Transformer encoders', 'Transformer Encoder', 'transformer encoders', 'transformer decoder', 'transformer-based encoder', 'TE model', 'Transformer-based encoder', 'transformer-based encoders', 'Transformer-based decoder', 'Transformer encoder model', 'Transformer decoders', 'Transformer-encoder', 'TE models', 'Transformer-based encoders', 'Transformer Decoder', 'transformer encoder model', 'Transformer encoder models', 'TE-DQS', 'transformer decoders', 'Transformer Encoder ( TE )', 'transformer encoder ( TE )', 'Transformer encoder architecture', 'transformer-based decoder', 'transformer-encoder', 'transformer-decoder', 'Transformer-decoder', 'Transformer decoder model', 'decoder transformers', 'te', 'transformer-based encoder architectures', 'Transformers encoder', 'transformer encoder architecture', 'TE dataset', 'Transformer Encoders', 'deep transformer encoders', 'deep transformer encoder', 'transformer encoding model', 'transformer-based encoding', 'Transformer-Decoder', 'transformer decoder architecture', 'Transformer decoder architecture', 'decoder transformer', 'TRANSFORMER_ENCODER', 'TRANSFORMER encoder', 'Te', 'Transformer encoder architectures', 'Encoder Transformer', 'encoder Transformer', 'Transformer encoder based models', 'transformer encoder models', 'transformer-encoders', 'deep encoder transformer', 'transformer-based encoders 5', 'Transformer encoding', 'transformer encoding models', 'Transformer-based encoding methods', 'Decode Transformer Base', 'TE-DQS models', 'TP-TRANSFORMER encoder', 'polyencoder transformer model', 'T TE', 'B TE', 'L TE', 'HiØTe', 'Transfomer decoder', 'TRANSFORMER decoder', 'Transformer Decoder model', 'transformer decoder model', 'Transformer-based Decoder', 'Transformer-based decoder architecture', 'decoder Transformer', 'transformer-based decoders', 'Transformer-based decoders', 'Transformers-based decoders', 'Transformer decoder models', 'Transformer-decoder network', 'transformer based encoder', 'TE data', 'Transformers-based encoder', 'transformer encoder architectures', 'Transformer encoder-based', 'Transformer encoder-based classifier', 'transformers encoder', 'encoder transformer', 'Transformer ( encoder )', 'TE )', 'Transformer-based encoder network', 'Transformer encoder network', 'transformer encoder base', 'Transformer encoder networks', 'transformer-encoder models', 'transformer encoder-based architectures', 'TE classifier', 'transformer-base encoder', 'Transformer-Base encoder', 'Transformer-base encoder', 'transformer encoder module', 'transformer encoder-based model', 'transformer-encoder based model', 'encoder-based Transformer models', 'TRANFORMER-BASED encoder', 'Transformer biencoder model', 'transformer encoder ( EAE )', 'Transformer decoder 2', 'Transformers encoders', 'Transformers-based encoders', 'deep Transformer encoders', 'transformer decoder as']"
36,580,Method,6.9704,2020,"{'2020': 0.9431, '2021': 4.1129, '2022': 6.9704}","['AdamW', 'AdamW optimizer', 'AdamW Optimizer', 'ADAMW', 'AdamW algorithm', 'AdamW *', 'Adamw optimizer', 'ADAMW optimizer', 'Adam W', 'AdamW method', 'AdamW 4', 'AdamW optimizer 2']"
37,114,Method,6.966,2015,"{'2015': -0.1448, '2016': -0.0838, '2017': -0.1134, '2018': -0.1812, '2019': -0.0139, '2020': 0.7061, '2021': 2.5325, '2022': 6.966}","['contrastive learning', 'CL', 'contrastive learning framework', 'Contrastive learning', 'Contrastive Learning', 'contrastive learning method', 'CL methods', 'B-CL', 'contrastive learning methods', 'contrastive learning ( CL )', 'contrastive learning approach', 'contrastive learning approaches', 'CL approach', 'CL algorithms', 'CL approaches', 'CL models', 'CL framework', 'Contrastive Learning ( CL )', 'contrastive learning module', 'Cl', 'CL strategy', 'contrastive learning models', 'contrastive learning algorithm', 'CL method', 'CL model', 'CL 3 M', 'CL-ED', 'contrastive learning strategy', 'Contrastive learning ( CL )', 'contrastive learning model', 'BASE+CL', 'constrastive learning', 'cl', 'Contrastive learning methods', 'CL corpus', 'contrastive learn', 'CL-Cos', 'TD-CL', 'CL-L 2', 'CL08', 'CL systems', 'Contrastive Learning approach', 'contrastive learning based methods', 'CL techniques', 'Contrastive learning techniques', 'contrastive learning techniques', 'contrastive learning mechanism', 'CL-based models', 'CL classification methods', 'contrastive learning classifier', 'contrastive learning theory', 'GOLD + CL', 'GOLD + CL model', 'CL-MW', 'code contrastive learning', 'cs.CL corpus', 'contrastive Learning', 'contrastive-learning', 'CL strategies', 'contrastive learning strategies', 'Contrastive Learning Approach', 'Contrastive learning approaches', 'CL-based methods', 'contrastive learning-based methods', 'CL algorithm', 'Contrastive Learning Framework', 'Contrastive Learning framework', 'contrastive-learning framework', 'contrastive learning based', 'Contrastive Learning based', 'Contrastive-learning based', 'contrastive Learning ( CL )', 'contrastive learning )', 'Contrastive Learning learns', 'contrastive learning ( CL ) method', 'CL module', 'contrastive learning technique', '* CL', 'cl base', 'CL Corpus', 'Classification based ( CL ) models', 'CL system', 'Contrastive Learning Set', 'CL set', 'contrastive learning frameworks', 'contrastive-learning-based module', 'CL corpora', 'CL-based strategy', 'Contrastive Learning ( CL ) techniques', 'Contrastive learning classifier', 'contrastive learning based classifier', 'contrastive learning based method', 'contrastive learning-based method', 'contrastive learning-based approaches', 'contrastive learning setting', 'VQ-CL', 'TS-CL', 'VQ-CL TS-CL', 'CL/CR', 'CL 3 M model', 'Contrastive Learning ( CLR )', 'PT CL', 'Cl −', 'code contrastive learning method', '• +CL', 'Cl-G', 'Cl opt', 'CL CDS', 'cl HR', 'CL ∗', 'Constrastive learning', 'B-CL system', 'Contrasive learning', 'CL D']"
38,94,Method,6.9652,2013,"{'2013': 0.1645, '2014': 1.3594, '2015': 2.658, '2016': 4.6739, '2017': 5.1406, '2018': 6.6391, '2019': 6.9652, '2020': 5.835, '2021': 6.0017, '2022': 4.1457}","['deep neural networks', 'DNN', 'DNNs', 'deep neural network', 'deep neural models', 'DNN models', 'Deep neural networks', 'deep neural networks ( DNNs )', 'deep neural network models', 'DNN model', 'deep neural network model', 'deep neural model', 'deep neural network architecture', 'deep neural architectures', 'Deep Neural Networks', 'deep neural network ( DNN )', 'Deep neural networks ( DNNs )', 'deep neural architecture', 'Deep Neural Networks ( DNNs )', 'Deep Neural Networks ( DNN )', 'deep neural networks ( DNN )', 'Deep Neural Network ( DNN )', 'deep neural network architectures', 'DNN architecture', 'DNN-based models', 'Deep Neural Network', 'DNN-based', 'Deep neural network', 'DNN approach', 'deeper neural networks', 'DNN architectures', 'DNN *', 'Deep neural models', 'DNN approaches', 'DNN-based approaches', 'deep neural network-based', 'DNN2', 'DNN1', 'deep NNs', 'deep neural', 'Deep neural network models', 'deep neural network approaches', 'deep neural network classifier', 'DNN algorithm', 'DNN methods', 'Deep Neural Network ( DNN ) models', 'DNN systems', 'deep neural network approach', 'dnn', 'DNN Models', 'deep neural network based models', 'Deep neural networks ( DNN )', 'DNN classifier', 'deep neural approaches', 'Deep Structured Neural Network', 'deeper neural network', 'DNN-based methods', 'deep neural network classifiers', 'DNN classifiers', 'deep neural networks ( NNs )', 'deep neural networks ( NN )', 'DNN-based method', 'deep neural classifier', 'deep neural network framework', 'DNN techniques', 'DNN-based model', 'DNN-based approach', 'DNN C', 'NN-DQN', 'DNN * ‡', 'DNN B', 'deep ) neural network', 'deep NN', 'Deep NNs', 'deep Neural Networks', 'Deep Neural Network models', 'deep NN models', 'DNNs models', 'deep ) neural network models', 'Deep Neural Network Models', 'DNN ( Deep NN )', 'DNN ( Deep Neural Network )', 'DNN Approaches', 'Deep neural network approaches', 'deep neural networks method', 'Deep Neural Network Model', 'DNN based models', 'deep neural-network based models', 'deep neural network-based models', 'deep-neural-network-based models', 'Deep Neural networks ( DNNs )', 'deep neural network based approaches', 'deep neural network–based', 'Deep Neural Architectures', 'deep ) neural architectures', 'deep ) neural network classifier', 'Deep neural model', 'deep neural network ( DNN ) based model', 'network ( DNN )', 'Deep neural network ( DNN ) models', 'Deep neural network architectures', 'SOTA deep neural networks', 'deep neural-networks-based methods', 'deep neural network ( DNN ) approaches', 'deep neural network ( NN ) models', 'deep neural network ( DNN ) architectures', 'Deep Neural Network ( DNN ) architectures', 'deep neural modeling', 'deep neural networks based approaches', 'Deep neural network-based methods', 'deep neural network based methods', 'deep neural network-based methods', 'DNN system', 'Deep Neural Network ( DNN ) -based methods', 'deep neural methods', 'DNN-based architectures', 'deep neural approach', 'deep neural-network ( NN )', 'deep-neural-based models', 'deep structured neural networks', 'DNN frameworks', 'deep neural network frameworks', 'deeper neural models', 'DNN-based classifier', 'dnn framework', 'DNN framework', 'deeper neural network architecture', 'DNN-based architecture', 'deep neural network ( NN ) model', 'deep neural network-based model', 'deep neural network based model', 'DNN data', 'DNN ( Deep Neural Network ) model', 'DNN algorithms', 'NN ( deep )', 'deep neural framework', 'DNN method', 'deep neural networks/', 'deep neural systems', 'neural networks ( DNNs )', 'DNN+R', 'QA DNNs', 'deep eural models', 'DNN + R', 'DNN DNN KB', 'DeepCx neural network', 'DNN C model']"
39,36,Method,6.9612,2008,"{'2008': -0.2675, '2009': -0.0569, '2010': -0.0099, '2011': 0.2104, '2012': 0.0177, '2013': 0.2999, '2014': 0.2103, '2015': 1.2475, '2016': 1.5996, '2017': 3.931, '2018': 4.5549, '2019': 6.9612, '2020': 6.6462, '2021': 6.8213, '2022': 6.8784}","['multi-task learning', 'MTL', 'multitask learning', 'multi-task learning framework', 'MTL models', 'Multi-task learning', 'MTL model', 'multi-task learning approach', 'multi-task learning model', 'multi-task learning ( MTL )', 'Multi-task Learning', 'multitask learning framework', 'multi-task learning method', 'MTL framework', 'Multitask learning', 'Multi-Task Learning', 'Multi-task learning ( MTL )', 'multi-task learning models', 'multitask learning model', 'multi-task learning architecture', 'FS-MTL', 'Multitask Learning', 'MTL approach', 'Multi-Task Learning ( MTL )', 'multi-task learning strategy', 'MTL methods', 'multitask learning approach', 'multi-task learning methods', 'MTL method', 'OTE-MTL', 'MTL-based approach', 'multitask learning architecture', 'multitask learning models', 'SP-MTL', 'Multi-task Learning ( MTL )', 'multi-task learning approaches', 'MTL architectures', 'Y & P-MTL', 'MTL-T P', 'MTL architecture', 'multi-task learning ( MTL', 'multi-task learning algorithm', 'FS-MTL model', 'MTL-XLD', 'multitask learning method', 'multi-task learning ( MTL ) approach', 'multi-task learner', 'MM-MTL', 'MTL-T', 'mtl', 'multitask learning approaches', 'multiple-task learning', 'multi-task learning setting', 'MTL-based methods', 'multi-task learning system', 'MTL system', 'MTL techniques', 'MTL-TAP', 'MTL-5', 'MTL2', 'MULTI-TASK LEARNING', 'Multi-task learning framework', 'multi-task learning frameworks', 'MTL approaches', 'multitask learning strategy', 'Multi-task learning model', 'multi-task learning networks', 'multi-tasking learning', 'multi-task learning module', 'H-MTL', 'multi-task learning QE model', 'MTL-WS', 'MultiTask Learning', 'Multi-Task learning', 'MTL frameworks', 'multi-task learning architectures', 'multi-task ( MTL )', 'MTL strategy', 'multi-task learning ( MTL ) model', 'Multi-Task Learning ( MTL ) model', 'multi-task learning strategies', 'multi-task learning systems', 'MTL systems', 'MTL * system', 'MTL )', 'MTL-large', 'multitask learning methods', 'MTL algorithms', 'ED MTL', 'MTL F model', 'multi-task learners', 'MTL2 framework', 'mutli-task learning', 'M MTL', 'MTL-XLD model', 'multitask-learning', 'Multi-task learning approach', 'multi-task learning network', 'Multi-task learning approaches', 'Multi-task Learning methods', 'Multi-Task Learning Model', 'Multi-task ( MTL )', 'multi-task learning based methods', 'multi-task learning mechanism', 'multitask-learning architecture', 'Multitask learning models', 'multi-task learning ( MTL ) method', 'multi-task learning based model', 'multi-task learning based system', 'MTL-based system', 'multi-task learning )', 'MTL *', 'multi-task learning ( MTL ) architecture', 'Multitask learning frameworks', 'multitask learning architectures', 'multi-task cell learning', 'multi-task learning ( MTL ) methods', 'multi-task learning techniques', 'MTL-based', 'multi-task learning ( MTL ) framework', 'multi-task learn', 'multitask learning techniques', 'multi-task learning structure', 'multi-task learning ( ED MTL )', 'multiple task learning', 'H-MTL models', 'MTL-TA †', 'MTL-WS †', 'MTL-16 †', 'MTL-16', 'MTL-T P✗', 'MTL-T P ✗', 'B -mtl', 'MTL F', 'multitask learners', 'w/MTL', 'MTL L21', 'HA-MTL models', 'Multi-task learning ( BGL )', 'Multi-task Leaning Framework', 'MTL ex', '• Multi-task framework ( MTL )', 'MTL ( MTL-XLD', 'MTL-XLD models', 'multi-task learning framework ( OTE-MTL )', 'multi task learning', 'multi-task learning ( mtl )', 'Multi Task Learning ( MTL )', 'Multi-task learning ( mtl )', 'MTL Models', 'Multi-task learning models', 'MTL Method', 'multi task learning approach', 'MTL ) approach', 'MTL=multi-task learning', 'MTL Framework', 'Multi-task Learning Framework', 'multi task learning framework', 'multi-task learning ( MTL ) frameworks', 'MTL-based models', 'multi-tasking learning model', 'Multi-Task Learning model', 'mtl model', 'Multi-task Learning Model', 'Multi-task learning architecture', 'multi-task learning Strategy', 'MTL networks', 'multitask learning systems', 'Multi-Task Learning ( MTL ) method', 'multitask learned models', 'MTL-based model', 'MTL algorithm', 'Multi-task Learning algorithm', 'multi-task learning based method', 'multi-task learning technique', 'multi-tasks learning framework', 'multitask-learning method', 'Multitask Learning Frameworks', 'multitask learning based model', 'multi-task learning ( MTL ) architectures', 'Multitask learning architectures', 'Multi-Task Cell Learning', 'Multi-task learning ( MTL ) methods', 'multitask learning network', 'multi-tasking learning ( MTL )', 'Multi-task Architecture Learning', 'multi-task learning based framework', 'multi-task learning based', 'multitask learning module', 'multitask learning mechanism', 'mtl base model', 'multitask learning technique', 'multi-task learning ( MTL ) approaches', 'Multi-task learning ( MTL ) approaches', 'multi-task learning ( MTL ) models', 'Multi-task Learning for', '( MTL ) model', 'multi-task learning ( MTL ) techniques', 'MTL Dataset', 'multi-task learning dataset', 'base MTL model', 'multitask learning analysis', 'Multi-Task model ( MTL )', 'multitask learning system', 'multi-task learning ( MT )', 'mult-task learning', 'D ed dataset ( multi-task learning )', 'multitasking learning', 'Multiple Task Learning', 'multi-task models ( D-MTL )', 'D-MTL models', 'D-MTL', 'LK-MTL', 'Multi-task Learning Strategy ( Q3 )', 'multi-task learning—we', 'Multi-Task Learning ( BPM_MT )', 'Multi-Task leaning ” model', 'MTL-5 models', 'MTL-TA method', 'Multi-task Learning ( Ta', 'multi-task learning ( §4 )', 'web – MTL', 'multi-task learning model ( CQR )', 'FS-MTL approach', 'Muti-task learning', 'multitask learner', 'multi-task leaning QE model', 'MTL +', 'Multi-Task Learning Framework ( MTL2 )', 'MTL2 model', 'multi-task learning 1', 'L MTL']"
40,115,Method,6.9043,2016,"{'2016': 2.7131, '2017': 4.5183, '2018': 6.5808, '2019': 6.9043, '2020': 5.1322, '2021': 5.8152, '2022': 6.5643}","['encoder-decoder model', 'encoder-decoder architecture', 'encoder-decoder framework', 'encoder-decoder models', 'encoder-decoder', 'encoder-decoder architectures', 'encoder-decoder network', 'encoder-decoders', 'encoder-decoder approach', 'Encoder-Decoder model', 'Encoder-Decoder', 'encoder-decoder structure', 'encoder-decoder frameworks', 'encoder-decoder system', 'Encoder-decoder models', 'encoder–decoder model', 'encoder-decoder networks', 'Encoder-Decoder architecture', 'encoder-decoder approaches', 'Encoder-Decoder framework', 'Encoder-Decoder models', 'encoder-decoder method', 'encoder-decoder based models', 'encoder–decoder architecture', 'encoder–decoder models', 'encoder decoder models', 'Encoder-Decoder Models', 'encoder-decoder based model', 'encoder-decoder methods', 'encoder-classifier/decoder framework', 'encoder–decoder framework', 'Encoder-decoder model', 'Encoder-decoder architecture', 'encoder/decoder models', 'encoder–decoder', 'Encoder-Decoder approach', 'Encoder-decoder architectures', 'encoder/decoder architectures', 'encoder-decoder module', 'encoder-decoder structures', 'Encoder-decoder framework', 'Encoder-Decoder Model', 'encoder/decoder architecture', 'encoder/decoder', 'Encoder-decoder approaches', 'encoder-decoder systems', 'Encoder-Decoder system', 'Encoder-decoders', 'encoder–decoders', 'encoder–decoder network', 'encoder-decoder-based methods', 'encoder-decoder-based model', 'encoder-decoder mechanism', 'encoder/decoder networks', 'Encoder-decoder networks', 'encoder-decoder based architecture', 'encoder-decoder modeling', 'encoder-decoder )', 'encoder-decoder modeling architecture', 'encoding-decoding framework', 'encoding-decoding', 'encode-decoder model', 'encode-decoder framework', 'encoder– decoder model', 'encoder decoder model', 'encoder/decoder model', 'encoder–decoder frameworks', 'encoder decoder architecture', 'Encoder-Decoder Architecture', 'encoder– decoder architecture', 'encoder– decoder', 'Encoder Decoder', 'Encoder-decoder', 'Encoder-Decoder architectures', 'encoder–decoder approaches', 'Encoder-Decoders', 'Encoder Decoders', 'Encoder-Decoder Network', 'encoder decoder network', 'encoder-decoder based methods', 'Encoder-decoder structure', 'encoder–decoder structure', 'encoder-decoder setting', 'encoder-decoder modeling framework', 'decoder-to-encoder', 'Encoder-decoder structures', 'encoder–decoder networks', 'encoder decoder networks', 'encoder-decoder-based architecture', 'encoder–decoder based models', 'encoder–decoder models—', 'encoder-to-decoder architecture', 'encoder-to-decoder model', 'SOTA encoder-decoder model', 'encoder-decoder network models', 'encoder-decoder based framework', 'encoder-decoder network architecture', 'encoder and decoder architecture', 'encoder and decoder architectures', 'encoder-decoder-based', 'encoder-decoder architecture model', 'encoder-decode architecture', 'CC Encoder-Decoder framework', 'encoder-decoder M model', 'Encoder-Decoder ( s2s )', 'encode-decoder models', 'encode-decoder architecture', 'encoder-decoder 3 model', 'Encoder-Decoder ( CED )', 'encoder-decoder ”', 'encoder-decoder ” architecture', 'encoder-decoder ” models', 'encoder-deocder architecture']"
41,79,Metric,6.9009,2004,"{'2004': 0.1324, '2005': 0.361, '2006': 0.1401, '2007': 0.8262, '2008': 0.8271, '2009': 0.8049, '2010': 0.6317, '2011': 0.7891, '2012': 1.0318, '2013': 0.7927, '2014': 1.0233, '2015': 0.9856, '2016': 1.1531, '2017': 1.982, '2018': 3.0978, '2019': 4.5603, '2020': 5.4932, '2021': 5.6367, '2022': 6.9009}","['fluency', 'Fluency', 'FLUENCY', 'fluency )', 'fluency model', 'fluency classifier', 'Fluency ( F )', 'Fluency Model', 'Fluency models', 'F1 Fluency', 'fluency 6', 'R fluency', 'fluency ( F )', 'κ Fluency', 'FLUency', 'Fluency model', 'System Fluency', 'fluency-based metric', 'fluency classifiers', 'fluency-based metrics', 'Fluency ours', 'fluency measures', 'Fluency Fluency', 'Model Fluency', 'fluency 5']"
42,35,Method,6.8927,2001,"{'2001': -0.4709, '2002': -0.1275, '2003': 1.227, '2004': 1.2435, '2005': 1.5632, '2006': 2.1161, '2007': 4.3501, '2008': 3.805, '2009': 4.67, '2010': 3.9166, '2011': 4.9741, '2012': 5.6381, '2013': 6.8927, '2014': 5.6284, '2015': 4.4513, '2016': 4.235, '2017': 3.2189, '2018': 2.6348, '2019': 1.4178, '2020': 1.1254, '2021': 1.0439, '2022': 0.3053}","['SMT', 'SMT system', 'SMT systems', 'statistical machine translation ( SMT )', 'SMT model', 'SMT models', 'statistical machine translation', 'Statistical Machine Translation ( SMT )', 'statistical machine translation system', 'statistical machine translation ( SMT ) systems', 'statistical machine translation systems', 'statistical MT', 'SMT approach', 'statistical MT system', 'statistical machine translation ( SMT ) system', 'Statistical machine translation ( SMT )', 'statistical MT systems', 'SMT approaches', 'SMT methods', 'SMT techniques', 'SMT+WSD', 'Statistical Machine Translation ( SMT ) systems', 'statistical machine translation ( SMT ) models', 'Statistical Machine Translation', 'statistical machine translation model', 'SMT-based', 'SMT + MF', 'SMT-based methods', 'SMT-based approach', 'SMT-based system', 'CAMB16 SMT', 'statistical machine translation models', 'statistical machine translation techniques', 'QT-SMT-form', 'SMT engine', 'smt', 'Statistical Machine Translation ( SMT ) system', 'Statistical machine translation ( SMT ) systems', 'SMT method', 'AMU16 SMT', 'word-based SMT', 'statistical machine translation approaches', 'SMT 4', 'Statistical machine translation', 'SMT System', 'SMT framework', 'statistical MT techniques', 'SMT-based approaches', 'SMT-based models', 'statistical MT ( SMT )', 'base SMT system', 'SMT )', 'statistical machine translation ( SMT ) model', 'smt-eur', 'statistical machine translation methods', 'statistical MT models', 'SMT Systems', 'statistical MT ( SMT ) systems', 'statistical machine translation approach', 'SMT-based systems', 'SMT-based method', 'SMT data', 'classifier-/SMT-based methods', 'Statistical Machine Translation ( SMT ) approaches', 'statistical machine translation ( SMT ) methods', 'Direct SMT systems', 'SMT+BLEU', 'Chr\ue010En SMT', 'word-based SMT system', 'Statistical MT', 'Statistical MT ( SMT )', 'statistical MT model', 'statistical MT ( SMT ) system', 'Statistical MT ( SMT ) systems', 'Statistical machine translation ( SMT ) models', 'statistical machine translation framework', 'statistical machine translation ( MT )', 'statistical machine translation ( SMT ) techniques', 'SMT-based model', 'SM T', 'SMT + WSD', 'SMT + IEM', 'En-Cs SMT', 'SMT engines', 'SMT-CUT', 'SMT ( 1 )', 'statistical machine transliteration model', 'ES SMT systems', 'SMT-eur', 'SMT Models', 'statistical machine translation ( SMT ) approach', 'Statistical Machine Translation Model', 'Statistical Machine Translation systems', 'Statistical machine translation systems', 'statistical ( SMT )', 'statistical machine translation technique', 'statistical machine translation method', 'SMT architectures', 'SMT-based techniques', 'SMT *', 'Statistical Machine Translation ( SMT ) model', 'statistical machine translation ( MT ) systems', 'statistical machine translation ( SMT ) approaches', 'statistical machine translation ( MT ) system', 'statistical machine translation ( SMT', 'SMT dataset', 'dual SMT system', 'HPB SMT', 'T2S SMT', 'de-en SMT', 'En-De SMT', 'en-cs SMT', 'cdec ( SMT )', 'CMU SMT', 'SMT+IR', 'TILB-SMT', '\ue002 SMT', 'SMT 1', 'CAMB16 SMT )', 'SMT/EBMT', 'ES SMT models', 'token-based SMT', 'SMT+bigLM', 'SMT-5', 'statistical machine translation ( MT ) system 5', 'SMT Chr\ue010En', '* -SMT-form systems', 'SMT-derived', 'derived SMT systems', 'EN-NL SMT', 'CS → EN SMT system', 'word-based SMT systems', 'word based SMT system', 'word-based SMT model', 'Word-based SMT', 'word-based statistical machine translation ( SMT ) models', 'statistical machine-translation methods', 'SMT Methods', 'SMT ( statistical machine translation )', 'statistical Machine Translation ( SMT )', 'Statistical Machine Translation Models', 'Statistical machine translation models', 'Statistical Machine Translation ( SMT ) approach', 'Statistical MT system', 'Statistical MT systems', 'MT ( SMT', 'Statistical Machine Translation ( SMT ) models', 'statistical ( smt )', 'statistical-based MT system', 'statistical MT framework', 'SMT-framework', 'MT ( SMT ) systems', 'statistical MT architectures', 'SMT architecture', 'statistical machine translation architecture', 'Statistical Machine Translation techniques', 'Statistical machine translation techniques', 'statistical Machine Translation ( MT )', 'Statistical Machine Translation approach', 'SMT Approach', 'SMT translation systems', 'statistical machine translation ( MT ) models', 'Statistical machine translation ( MT ) models', 'Statistical Machine Translation ( SMT ) techniques', 'SMT mechanism', 'statistical machine translation-based methods', 'based SMT systems', 'statistical machine translation-based approach', 'classifier-/SMT-based method', 'classifier/SMT-based methods', 'SMT corpora', 'statistical machine translation ( SMT ) based method', 'SMT-based :', 'Statistical machine translation ( SMT ) methods', 'SMT algorithms', 'statistical machine translation ( SMT ) -based models', 'Statistical Machines Translation ( SMT )', 'SMT modeling', 'SMT-based model ( SMT )', 'statistical machine translation system ( SMT )', 'Statistical Machine Translation system ( SMT )', 'statistical machine translation ( SMT ) framework', 'SMT SMT', 'machine translation ( SMT )', 'Statistical Machine Translation ( SMT ) architectures', 'statistical machine translation ( SMT ) based methods', 'statistical machine translation framework ( SMT )', 'statistical machine translation ( SMT ) algorithms', 'statistical machine translation ( MT ) model', 'SMT frameworks', 'Statistical machine translation systems models', 'LIMSI statistical machine translation system', 'IBM-style SMT models', 'Subtree Metric ( SMT )', 'statistical MT ( IMT )', 'IBM statistical MT models', 'IBM statistical machine translation ( SMT ) models', 'stem-based SMT system', 'S2E SMT system', 'SMT de', 'QT-SMT-form systems', 'chunk-based statistical machine translation model', 'en-de IT SMT', 'en-cs IT SMT', 'statistical machine translation ( t )', 'SMT +', 'SMT-2', 'SMT-6', 'SMT-7', 'Direct SMT system', 'structure mapping theory ( SMT )', 'Structure Mapping Theory ( SMT )', 'VT16 SMT + classifiers', 'SMT model ( cdec ) Ta', 'SMT IBM model 2', 'SMT ( Row 3 )', 'SMT-based QE system', 'SMT-based PG method', 'SMT-based PG', 'SMT-based PG model', 'SMT-like model', 'SMT-like approaches', 'en-de SMT', 'cdec SMT system', 'CDEC SMT system', 'SMT ( cdec )', 'w smt', 'SMT MT O 1', 'CMU Statistical Machine Translation System', 'SMT-based data-driven approach', 'dth SMT model', 'SMT 3', 'SMT-3', 'SMT+PAT', 'en–ta SMT', 'hobbles SMT', 'end-toend SMT system', 'SMT-style', 'SMT-style framework', 'SMT + IBM 4', 'JT-SMT', 'SMT-dev-BLEU', 'AMU16 SMT system', 'SMT-1', 'SMT systems 1', 'SMT0', 'SMT work', 'forest-based SMT systems', 'forest-based SMT', 'EN-ID SMT', 'Statistical Machine Transliteration System', 'Statistical Machine Transliteration Model', 'Statistical machine transliteration', '\ue01f SMT', 'statistic machine translation model', 'statistical MT 4', 'SMT-4', 'SMT system 4', 'text-based SMT']"
43,74,Dataset,6.7163,2018,"{'2018': 1.0341, '2019': 1.8463, '2020': 3.8443, '2021': 5.1433, '2022': 6.7163}","['MNLI', 'MultiNLI', 'MNLI dataset', 'MNLI-m', 'MNLI-mm', 'MultiNLI dataset', 'MNLI task', 'MultiNLI corpus', 'MNLI model', 'MultiNLI-TR', 'MNLI-m/mm', 'MNLI data', 'MNLI corpus', 'MNLI-MM', 'MULTINLI', 'MNLI dev set', 'MNLI+CB B', 'MNLI 1', 'MNLI-hard', 'MultiNLI data', 'MultiNLI model', 'MNLI-M', 'MNLI B', 'MULTINLI corpus', 'Multi-Genre Natural Language Inference ( MNLI ) dataset', 'Multi-Genre Natural Language Inference ( MNLI )', 'MNLI models', 'MNLI tasks', 'Multi-Genre NLI Corpus', 'MNLI-', 'bart-large-mnli', 'MNLI8.5k', 'BART-L-mnli *', 'MultiNLI tasks', 'MNLI-128', 'MultiNLI→', 'Multi-Genre NLI', 'MultiNLI dev set', 'Multi-Genre NLI ( MNLI ) dataset', 'MultiNLI ( MNLI )', 'Multi-Genre Natural Language Inference ( MultiNLI ) corpus', 'MultiNLI dev', 'MNLI BART', 'MNLI m', 'MultiNLI 2', 'MNLI 2', 'MNLI-1000', 'MNLI train', 'MultiNLI task', 'MNLI+Adv', 'MNLI+Ad', 'MNLI hard set', 'multiNLI', 'Multi-Genre NLI dataset', 'Multi-Genre Natural Language Inference Corpus', 'Multi-Genre Natural Language Inference corpus', 'MNLI )', 'MNLI data set', 'Multi-Genre Natural Language Inference ( MultiNLI ) dataset', 'Multi-Genre Natural Language Inference ( MultiNLI )', 'Multi-Genre NLI ( MNLI )', 'MultiNLI ( MNLI', 'MNLI dev', 'MNLI NLI', 'MNLI mm', 'MNLI Gov', 'MNLI+', 'MNLI head model', 'MNLI prompt model', 'MNLI-512', 'VitC + MNLI', 'MNLI-1', 'MultiNLI 1', 'MNLI w', 'MNLI 4', 'MNLI-2', 'MNLI 8', 'MNLI 6', 'MNLI-M task', 'MultiNLI Train', 'MultiNLI training dataset', 'MNLI training dataset', 'MultiNLI training set', 'MNLI Mism', 'pus ( MNLI )', 'MNLI exam', 'MultiNLI ( Williams', 'MNLI WNLI', 'MNLI NLI 393k', 'MNLI natural language inference task', 'MultiNLI 392k', 'MNLI data protocol', 'MNLI protocol', 'MNLI-128 -', 'MNLI 3', 'MNLI L-L', 'MNLI ( Electra )', 'mnli', 'MultiNli', 'MNLI NLI Classification', 'MultiNLI natural language inference dataset', 'Multi-Genre Natural Language Inference dataset', 'MNLI Dev set', 'Multi-Genre Natural Language Inference data', 'MultiNLI Model', 'Multi-genre Natural Language Inference corpus', 'MultiNLI )', 'multinli/', 'MNLI *', 'Multi-Genre Natural Language Inference (', 'MultiNLI—from', 'MultiNLI data set', 'multi-genre natural language inference ( MNLI ) dataset', 'Multi-Genre NLI ( MultiNLI )', 'MNLI ( MNLI )', 'Multi-Genre NLI ( MNLI', 'Multi-Genre Natural Language Inference ( MNLI ) corpus', 'Multi-Genre NLI ( MULTINLI ) corpus', 'Multi-genre Natural Language Inference ( MultiNLI ) corpus', 'Multi-Genre NLI ( MultiNLI ) corpus', 'Multi-genre NLI ( MNLI ) corpus', 'MNLI Dev', 'MNLI dev.', 'MNLI classifier', 'S/MNLI', 'NLI MNLI', 'NLI Multi-Genre Natural Language Inference', 'Corpus ( MNLI )', 'MNLI Language inference dataset', 'MNLI system', 'MNLI small', 'Multi-Genre Natural Language Inference dataset ( MNLI )', 'Multi-Genre NLI Corpus ( MultiNLI )', 'Multi-Genre Natural Language Inference Corpus ( MultiNLI )', 'Multi-Genre Natural Language Inference Corpus ( MNLI )', 'Multi-Genre NLI corpus ( MultiNLI )', 'MNLI-based', 'MNLI-based model', 'Multi-genre Natural Language Inference Corpus ( MNLI', 'Multi-Genre Natural Language Inference corpus ( Multi-NLI )', 'MNLI set', '-MNLI', '/MNLI', 'MNLI corpus.The', 'MultiNLI corpora', 'MultiNLI-TR dev sets', 'MultiNLI premise', 'SciTail MNLI', 'MultiNLI SciTail Model', 'MNLI-SwDA-IA_QA', 'BART-large-MNLI', 'bart-large-mnli model', 'BART ( bart-large-mnli )', 'MNLI-mm hard', 'Z-Aug MNLI', 'MNLI 433K', 'MNLI ( MNLI-mm )', 'MNLI +', 'MNLI 8.5k', 'MNLI-style', 'MNLI dataset ( U3 )', 'MNLI Dataset 10', 'MNLI 1.0 dataset', 'MNLI ( ID )', 'MNLI Val', 'MNLI-mis', 'MultiNLI 5', 'MNLI-test', 'Self MNLI', 'MNLI BoolQ CB', 'MNLI-512 -', 'MNLI BoolQ', 'MNLI + PMEANS', 'MNLI-m ’', 'MNLI-M.', 'MNLI-m dev', 'MNLI-m dev set', 'MNLI 3 393K', 'MNLI+CB', 'MNLI TPPDB', 'MNLI 7', 'MNLI small S adv train', 'MNLI 5k', 'BART-L-mnli', 'ITFT MNLI', 'ITFT-MNLI', 'MNLI-single MNLI-multi', 'MultiNLI-trained model', 'MultiNLI train set', 'MNLI ( Training Set', 'MNLI Train Model', 'MNLI Dev ( MD )', 'MNLI RPL', 'tor MNLI-m', 'F1 MNLI-m/mm', 'expanded multi-genre natural language inference dataset', 'MNLI - - 19643', 'MNLI MIR', 'MNLI Ori']"
44,39,Method,6.6709,2002,"{'2002': -0.4387, '2003': 1.0347, '2004': 1.3873, '2005': 1.4642, '2006': 2.081, '2007': 2.9184, '2008': 1.8442, '2009': 2.3469, '2010': 3.3844, '2011': 3.045, '2012': 3.8788, '2013': 4.9824, '2014': 3.718, '2015': 4.7414, '2016': 4.4212, '2017': 3.8725, '2018': 5.1021, '2019': 6.6709, '2020': 4.7303, '2021': 6.1142, '2022': 4.2651}","['TF-IDF', 'tf-idf', 'TFIDF', 'tf.idf', 'tfidf', 'tf * idf', 'TF * IDF', 'TF-IDF model', 'tf · idf', 'Tf-Idf', 'TF/IDF', 'TF.IDF', 'TFIDF +', 'Tf-idf', 'tf·idf', 'TF×IDF', 'TF-IDF method', 'tf ∗ idf', 'tf idf', 'TfIdf', 'TFIDF+', 'tf -idf', 'TF IDF', 'TFIDF model', 'TF-IDF )', 'TF-IDF approach', 'TF-IDF algorithm', 'TF·IDF', 'tf–idf', 'TFIDF method', 'tf-idf )', 'TFIDF approach', 'tf-idf model', 'TF-IDF-based', 'TF-IDF *', 'TF-IDF + BART', 'Tfidf-N', 'tf ) -idf', 'TFIDF-based', 'TFIDF+DE', 'TF-IDF†', 'IR-TF-IDF', 'tf∗idf', 'tf/idf', 'Tf-IDF', 'TF-IDF models', 'tf-idf method', 'tf-idf approach', 'TF * IDF system', 'TFIDF framework', 'TF-IDF + GPT2', 'tf.idf1', 'tf × idf', 'TF · IDF', 'TFIDF-COS', 'TF–IDF', 'Tfidf', 'TF-IDF Model', 'tf.idf method', 'TF-IDF approaches', 'TF-IDF GloVe', 'tf − idf', 'MAX+TFIDF', 'tf * idf 3', 'BoW-TFIDF', 'C-TFIDF', 'TF-IDF + COS', 'TF.Delta-IDF', 'TFIDF cosine', 'TFIDF + -NDO', 'Term Frequency-Inverse Document Frequency', 'term frequency-inverse document frequency', 'TFIDF models', 'tfidf-based', 'tf-idf–', 'TFIDF )', 'tf.idf based method', 'tf-idf methods', 'TF-IDF methods', 'TF * IDF approach', 'tfidf approach', 'tf * idf approach', 'tfidf system', 'tfidf algorithm', 'tf-idf-S', 'tf * idf approaches', 'TF-IDF-based methods', 'TF-IDF technique', '=TFIDF', 'TFIDF-based model', 'tf×idf', 'term frequencyinverse document frequency ( TF-IDF )', 'tf-idf-M', 'TF-IDF box', 'bigram tf-idf', 'bigram TF-IDF', 'TF-IDF/BM25', 'ideal-point-tfidf', 'tf-idf scheme', 'BoW TFIDF', 'BoW+TFIDF', 'R-TFIDF', 'TF/IDF IR system', 'tf ⋅ idf', 'tf ∗ idf model', '=tf ∗ idf', 'tfidf * CIG', 'tfidf 1-best model', 'tf-idf 5', 'TF * IDF 2', 'TFIDF-2', 'log ( tf ) -idf', 'Delta tf.idf', 'tf-idf embedding model', 'term frequency × inverse document frequency ( TF×IDF )', 'TF-IDF ( TD )', 'TF-IDF †', 'prior-tfidf', 'TF-IDF-N', 'tf-idf N', 'TFIDF mdoel', 'tf.idf-based cosine', 'tf-idf-cosine', 'Cosine-TfIdf', 'tf ) idf', 'tf-Idf', 'TF - IDF', 'Tf * Idf', 'tf-IDF', 'tf– idf', 'TF ? IDF', 'Tf.Idf', 'term frequency - inverse document frequency', 'tf-idf-based approach', 'tfidf model', 'TF.IDF model', 'Tf-idf model', 'Tf-Idf method', 'tf-idf-based', 'term frequency inverse-document frequency (', 'TF * IDF )', 'TF * IDF (', 'tf.idf=', 'tf / idf )', 'tf.idf )', 'TFIDF *', 'TFIDF based method', 'TF/IDF methods', 'tf.idf approach', 'TF-IDF system', 'TF * IDF : system', 'TFIDF algorithm', 'TFIDF approaches', 'TFIDF measure', 'tf-idf measure', 'TFIDF-based classifier', 'tf-idf mechanism', 'TFIDF-based methods', 'tfidf-based methods', 'TF * IDF technique', 'tf-idf strategy', 'tf-idf techniques', 'tf * idf techniques', 'tf-idf-Over', 'TFIDF-based measures', 'TF-IDF Dataset', 'TFIDF classifier', 'TF-IDF-based models', 'tfidf/Rocchio classifier', 'TFIDF/Rocchio', 'tf idf×', 'Cosine ( tf ∗ idf )', 'Term FrequencyInverse Document Frequency ( TF-IDF )', 'term frequencyinverse document frequency ( tf-idf )', 'TF-IDF ( Term FrequencyInverse Document Frequency )', 'TF-IDF ( term frequencyinverse document frequency )', 'UGR TF-IDF', 'UGR TFIDF', 'TFIDF ( m )', 'ngrams TFIDF', 'MTM + TFIDF', '†TF-IDF', '•Tfidf ( f 3 )', 'TF-IDF PLT-based methods', 'TF-IDF+GPT2', 'tfidf/entity', 'Term Frequency- Inverse Document Frequenc ( c-TF-IDF ) method', 'TF-IDF + WideMLP', 'TFIDF + -N', 'TFIDF + -ND', 'TFIDF + -O.', 'Tf − Idf', 'TF-IDF+iVectors', 'TFIDF ( +OH )', 'SENT+TFIDF', 'TFIDF+WDO', 'tf-idf web', 'TF-IDF words', 'TF-IDF ( MAX )', 'TF-IDF+Visual', 'tfidf-based ranking method', 'TFIDF ranking algorithm', 'TF-IDF ranking algorithm', 'TF-IDF-based Naming', 'MAX+TFIDF+WDO', 'SSCB-TF-IDF', 'TFIDF-like scoring', 'tf-idf 13', 'bigrams tf-idf', 'TFIDF ( bigram )', 'TF.IDF1', 'tfidf1', 'TFIDF-1', 'DR tfidf', 'TFIDF-3', 'Asp-SSCL-TFIDF', 'tf·idf-score', 'TF * IDF 4', 'tf.idf 4', 'TFIDF PBA *', 'BM25 tf.idf', 'F - TF-IDF', 'F tfidf', 'TF-IDF scheme', 'tf· idf', 'tf·idf-based', 'tf·idf-based method', 'tf w idf', 'tfidf ( W )', 'Bag-of-words+TFIDF', 'cpc tfidf', 'TF-IDF BoW approaches', 'BoW/TFIDF', 'BoW tf-idf', 'BoW TF-IDF', 'BOW tfidf', 'bag-of-words ( Tfidf', 'tf-idf bag-of-words model', 'TF-IDF bag-of-words', 'TFIDF bag of words', 'bag-of-words TF-IDF model', 'TFIDF BOW model', 'TFIDF BOW', 'TF-IDF BOW', 'TF∗IDF-1', 'TF∗IDF-2', 'TF-IDF matrix', 'TF-IDF-like', 'TF-IDF like models', 'TF-IDF matching method', 'TF-IDF matching', 'tf-idf match model', 'tf.idf matching technique', 'tf · idf approach', 'Bow+TFIDF', 'BOW+TFIDF', 'tf.idf ( d )', 'ET-RR ( TF-IDF )', 'TF-IDF + EMB', 'TF-IDF KnowMAN', 'KnowMAN TF-IDF', 'KnowMAN TFIDF', 'WSTAT+TFIDF', 'TF-IDF - Top 5', 'TF-IDF - Top 10', 'tfidf ( B )', 'tf∗idf method', 'TF∗IDF', 'TF-IDF * scores', 'TF-IDF language model', 'c-tf-idf', 'c-TF-IDF', 'c-TF-IDF )', 'TF-IDF+WideMLP', 'TFIDF+WideMLP', 'TF·IDF 1', 'Snorkel TF-IDF']"
45,81,Method,6.5679,2003,"{'2003': 0.3318, '2004': 0.6433, '2005': 0.328, '2006': 1.0322, '2007': 0.9761, '2008': 1.9174, '2009': 2.1048, '2010': 3.068, '2011': 2.124, '2012': 2.6824, '2013': 3.1538, '2014': 4.8412, '2015': 5.626, '2016': 5.1071, '2017': 4.7738, '2018': 6.0692, '2019': 5.6343, '2020': 5.4567, '2021': 6.3767, '2022': 6.5679}","['cosine similarity', 'cosine similarities', 'Cosine similarity', 'cosine-similarity', 'cosine similarity measure', 'Cosine Similarity', 'cosine similarity metric', 'cosinesimilarity', 'cosine similarity )', 'Cosine similarities', 'cosine similarity ( COS )', 'cosine-similarities', 'Cosine similarity measure', 'Cosine-similarity', 'Cosine Similarities', 'cosine-similarity metric', 'cosine similarity metrics', 'Cosine Similarity method', 'Cosine Similarity ( CS )', 'cosine-similarity measure', 'cosine similarly', 'Cosine similarity ( COS )', 'Cosine similarity ( cos )', 'cosine dissimilarity', 'cosine similarity 8', 'PB cosine similarity', 'cosine−similarity', 'cosine similarity 5', 'WW cosine similarity', 'cosine_similarity', 'cosine similarity ,', 'CosineSimilarity', 'cosine similarity-based metrics', 'cosine similarity method', 'cosine similarity algorithm', 'cosine similarity based system', 'cosine similarity—to', 'cosine-similarity model', 'cosine similarity model', 'cosine similarity similarity', 'cosine similarity strategy', 'cosinesimilarity metric', 'cosine similarity based method', 'Cosine Similarity approach', 'cosine similarity approach', 'cosine similarity—for', 'Cosine similarity measures', 'cosine similarity measures', 'cosine similarity value', 'cosine similarity-based', 'cosine similarity based', 'cosine similarity ( Similarity )', 'Cosine Similarity ( CP )', 'cosine similarity ( CP )', 'cosine-similarity 2', 'CLS cosine similarity', 'Cosine Similarity ( Cos )', 'cosine similarity ( cos )', 'Cosine similarity ( Cos )', 'cosine similarity ( CS )', 'Cosine Similarity model ( CS )', 'cosine similarity metric CS', '+ cosine similarity', 'cosine similarity K', 'w2v cosine similarity', 'cosine similarity 4', 'consine similarity', 'Cosine Similarity ( f cos )', 'cosine similarity 9', 'cosine 1 similarity']"
46,17,Method,6.2546,2003,"{'2003': 0.113, '2004': 0.4697, '2005': 1.5019, '2006': 1.969, '2007': 2.7613, '2008': 2.7276, '2009': 3.5286, '2010': 3.5499, '2011': 3.2597, '2012': 5.0239, '2013': 5.9656, '2014': 5.5221, '2015': 6.2546, '2016': 5.89, '2017': 4.2627, '2018': 4.2635, '2019': 5.2887, '2020': 4.6382, '2021': 3.5608, '2022': 2.2874}","['CRF', 'CRFs', 'CRF model', 'CRF models', 'conditional random field ( CRF )', 'Conditional Random Field ( CRF )', 'conditional random field', 'O-CRF', 'CRF++', 'crf', 'CRFs model', 'LOP-CRF', 'Conditional Random Field', 'CRF classifier', 'CRF-based', 'CRF approach', 'CRF-based model', 'CRF algorithm', 'CRF-based approach', 'LOP-CRFs', 'R1-CRF', 'conditional random field ( CRF ) model', 'L-CRF', 'CRF2O', 'CRF method', 'conditional random field model', 'CRF-based models', 'CRF-based method', 'CRF systems', 'CRF+GI', 'Tree-CRF', 'tree CRF-based model', 'F-CRF', 'CRF 0', 'PL-CRFs', 'conditional random field ( CRF', 'CRF system', 'H-CRF', 'L2-CRFs', 'PO-CRF', 'CRF )', 'CRF-based methods', 'CRF-OP', 'CRF++ 3', 'L1-CRFs', 'CRF Model', 'CRF-based system', 'conditional random field ( CRF ) models', 'k -CRF', 'R-CRF', 'Conditional random field ( CRF )', 'CRF framework', 'CRF methods', 'CRF-AW', 'F1 CRF', 'SS-CRF-MER', 'CRF+MSR', 'CRF-WORD', 'CRF training', 'Conditional Random Field ( CRF', 'Conditional Random Field model', 'CRF classifiers', 'CRF approaches', 'CRF-based approaches', 'CRF-based systems', 'CRF++ 5', 'CRF-SRC', 'Tri-CRF', 'M-CRF', 'CRF-MF', 'CRF-joint', 'CRF m+R', 'CRF++ 1', 'CRF+Adj', 'NEF-CRF-F', 'CRF r', 'Conditional Random Field ( CRF ) model', 'CRF-layer', 'conditional random field ( CRF ) classifier', 'CRF-S', 'CRF+', 'RNSCN-CRF', 'M+R-CRF', 'CRF m+R system', 'Tree CRF-based model', 'CRF-KNBC', 'CRF mpm', 'CRF-C', 'SA-CRF', 'CRF ( Conditional Random Field )', 'conditional random field models', 'CRF network', 'conditional random field-based', 'CRFs approaches', 'CRF-based Approach', 'CRF package', 'structured conditional random field', 'F-CRF model', 'GS-CRF', 'CRF+GIB', 'LC-CRFs', 'CRF-1', 'CRF+WEB', 'CRF-L2', 'CRF-Early', 'CRF++ 2', 'CRF-FI', 'NEF-CRF-F model', '+CRF', 'CRF model 2', 'Conditional random field', 'CRF ( ` )', 'CRF-', 'Conditional Random Field Model', 'CRFs models', 'Conditional Random Field models', 'CRF Models', 'conditional random field classifiers', 'Layered-CRF', 'CRF based model', 'CRFs approach', 'CRF-modeled', 'CRF-based classifier', 'Conditional Random Field ( CRF ) models', 'Conditional Random Field ( CRF ) classifier', 'CRF-based module', 'CRF based methods', 'CRF 6', 'CRF ”', 'CRF 4', 'crf++', 'CRFs 1', 'non-CRF model', 'L2- CRFs', 'GCNN-CRF', 'CRF v+R system', 'CRF2o', 'CRF+RR', 'Tri-CRF models', 'PL-CRF', 'CRF+R', 'CRF ( PO-CRF )', 'CRF-M', 'CRF-PA', 'CRFs LOP', 'CRF tagging model', 'CRF tagging', 'CRF stacking model', 'CRF m+R approach', 'EF + CRF', 'bagged CRF', 'tree conditional random field', 'DB-CRF', 'CRF-CI', 'CRF++ 4', 'Model+CRF', 'CRF-2', 'CRF Training', 'CRFs trained model', 'crf-train', 'CRF ( d )', 'L1- CRFs 8', 'conditional random field ( CRFs )', 'Conditional Random Field layer ( CRF )', 'crfs', 'CRFS', 'CRFs )', 'CRF *', 'CRF ( } )', 'CRFs Model', 'Conditional random field model', 'CRFs ) model', 'Conditional Random Field Models', 'CRF-models', 'conditional random field ( CRFs ) model', 'Conditional Random Field classifiers', 'CRF-Layer', 'layered-CRF', 'CRF-based architectures', 'conditional random field based model', 'CRF-classifier', 'CRF Classifier', 'Conditional Random Field classifier', 'conditional random field classifier', 'conditional random field ( CRF ) based model', 'conditional random field algorithm', 'CRFs algorithm', 'CRFs-based', 'CRFs-based models', 'conditional random field-based models', 'CRF based models', 'CRF based system', 'conditional random field methods', 'Conditional Random Field methods', 'CRFs algorithms', 'CRF Algorithms', 'CRF algorithms', 'conditional-random-field approach', 'Conditional Random Field approach', 'conditional random field-based approach', 'CRF based casing method', 'CRF model ( CRF )', 'Conditional Random Field Model ( CRF )', 'CRFs method', 'CRF-based algorithms', 'structured conditional random field model', 'conditional random layer ( CRF )', 'CRF ( S )', 'Conditional Random Field module', '-CRF', 'crf fields', 'random field ( CRF )', 'structured conditional random field ( CRF )', 'Conditional Random Field ( CRF ) algorithm', 'Conditional Random Field ( CRF ) approaches', 'conditional random field ( CRF ) approaches', 'CRF model Models', 'CRF-based algorithm', 'Conditional random field ( CRF ) -based', 'conditional random field ( CRF ) approach', 'Conditional Random Field ( CRF ) mechanism', '-Conditional Random Field ( CRF )', 'CRF architecture', 'Conditional random field ( CRF ) classifiers', 'conditional random field techniques', 'CRF+PROB approach', 'CRF++ 6', 'CRF v', 'conditional topic random field', 'CRF 11', 'CRF++0.58', 'CRF-HSCRF', 'CRF-GM', 'CRF-INF', 'CRF-only system', 'conditional random field model ( CRF ) 6', 'R1-CRF system', 'L-CRF algorithm', 'L-CRF Algorithm', 'L-CRF method', 'L-CRF model', 'CRF ( T )', 'Apply-CRF-Model', 'Condition Random Field', 'TAS-CRF', 'CRF-based model 4', 'W CRF )', 'CRF model—which', 'CRF ( LC-LC )', 'TAS-SW-CRF', 'CRF++ package', 'CRF 1', 'Conditional Random Field ( CRF ) 1', 'CRF ( 1 , - )', 'CRF ( 1,1 )', 'CRF model 1', 'CRF r 0.5', 'CRF t 0.5', 'non-CRF-based model', 'non-CRF models', 'CRFs/HCRFs', 'CRF-Mrg', 'CRF++ 7', 'CRF++ 19', 'CRF-based MWER', 'GCNN-CRF model', 'CRF TCD', 'CRF m−R systems', 'CRF m−R', 'Trans-CRFs', 'CRF2O model', 'ADF-CRF', '+CRF+D', 'CRF model 3', 'CRF 3', 'PO-CRF models', 'TR-CRF', 'CRF-based ORL model', 'CRF m', 'CRF m systems', 'Conditional Random Field ( CRF ) 7', 'conditional random field 7', 'L1- CRFs', 'CRF+D', 'k-CRF', 'k -CRFs', 'k CRFs', 'k -CRF model', 'k -CRF systems', 'CRF ( 2,0 )', 'CRF ( 2,1 )', 'CRF ( LC-LC 1 )', 'CRF ( LCFC 1 )', 'CRF + CRF-MF', 'CRF ToBI', 'LOP-CRF approach', 'CRF ( a )', 'CRF ( CRF-joint )', 'QBC-CRF', 'QBC-CRF algorithm', 'CRF++ 8', 'CRF-based tagging', 'CRFs tagging model', 'CRF tagging models', 'Mono-CRF', 'CRF ’ model', 'CRFs ’', 'CRF join model', 'M+R-CRF †', 'M+R-CRF approach', 'M+R-CRF methods', 'CRF-based ones', 'NEF-CRF-S', 'NEF-CRF-S model', 'CRF ( CPU )', 'tree-CRF', 'tree CRFs', 'Tree CRF', 'tree CRF model', 'Tree-CRF model', 'tree-structured CRF', 'tree structured CRFs', 'tree CRF -based model', 'CRF ( PCRF ) model', 'CRF ( PCRF )', 'CRF 12', 'CRFs 12', 'CRF ∼16k', 'CRF ( HUCRF )', 'CRF-C methods', 'DB-CRF )', 'DB-CRF method', 'word-based CRF approach', 'word-based CRF model', 'CRF ( 3.5 ) model', 'CRF task', 'task-crf model', 'task-crf', 'CRF 10 model', 'CRFs 10', 'CRF +', 'CRF + CRF.', 'CRF +h', 'CRF ( 2 , - )', 'CRF ( 2,2 )', 'CRF method 2', 'CRF2', 'CRF 2', 'CRFs 2', 'CRF ( NCRF ) model', 'CRF ( CC )']"
47,86,Method,6.2291,2008,"{'2008': -0.2675, '2009': 0.1362, '2010': 0.1557, '2011': -0.2594, '2012': -0.1486, '2013': 0.2159, '2014': 0.0006, '2015': 1.1468, '2016': 1.2215, '2017': 2.0249, '2018': 2.8226, '2019': 4.6204, '2020': 5.5087, '2021': 5.9694, '2022': 6.2291}","['transfer learning', 'TL', 'Transfer learning', 'Transfer Learning', 'transfer learning approach', 'transfer learning methods', 'transfer learning method', 'transfer learning techniques', 'transfer learning approaches', 'transfer learning framework', 'transfer learning technique', 'transfer-learning', 'transfer learning models', 'transfer learning model', 'transfer learning strategies', 'transfer learning algorithm', 'transfer learning strategy', 'SIR TL', 'transfer learning ( TL )', 'TL corpus', 'TL methods', 'Transfer learning ( TL )', 'TL method', 'TL model', 'TL MFS', 'transfer-learning approach', 'TL-S', 'TL approach', 'TL models', 'transfer learning module', 'tl', 'transfer learning mechanisms', 'TL Method', 'transfer-learning framework', 'TL module', 'transfer learning setting', 'transfer-learning based methods', 'TL17', 'TL-AM', 'Transfer Learning approach', 'Transfer Learning ( TL )', 'transfer learning-based approach', 'transfer learn', 'TL ( ∗ )', 'ES-TL', 'Transfer learning techniques', 'transfer-learning approaches', 'TL approaches', 'Transfer learning methods', 'transfer learning based classification method', 'transfer learning based approach', 'transfer learning architectures', 'transfer learning-based methods', 'SOTA transfer learning approach', 'transfer learning ( TL ) approach', 'TL-dd', 'TL-adv', 'TL ; DR dataset', 'TL ; DR corpus', 'transfer leaning method', 'LIR transfer learning', 'transfer learning Transfer learning is', 'TL-AM dataset', 'Transfer Learning for QA Transfer learning', 'TRANSFER LEARNING', 'TRANSFER_LEARNING', 'Transfer-learning', 'transfer Learning', 'Transfer Learning techniques', 'Transfer Learning approaches', 'transfer-learning algorithm', 'TL algorithm', 'Transfer Learning technique', 'Transfer Learning method', 'Transfer learning models', 'Transfer Learning Models', 'Transfer Learning models', 'Transfer Learning model', 'Transfer learning model', 'transfer learning-based classification method', 'transfer learning based model', 'transfer learning based models', 'transfer learning-based framework', 'transfer learning based framework', 'Transfer Learning strategies', 'transfer-learning-based approach', 'Transfer learning strategy', 'transfer learning mechanism', 'transfer-learning-based', 'transfer-learning based', 'transfer learning-based', 'model ( TL )', 'Transfer Learning module', 'Data Transfer learning', 'transfer learning-based approaches', 'TL based approaches', 'Transfer learning-based methods', 'transfer learning based strategy', 'transfer learning (', 'transfer learning )', 'transfer learning ( TL ) methods', 'transfer learning system', 'transfer learning architecture', 'transfer learning principle', 'transfer-learned', 'transfer learning based algorithms', 'Learning Transfer', 'transfer learning ( TL ) models', 'transfer learning frameworks', 'transfer-learning based SOTA model', 'transfer learning based technique', 'TransR learning', 'transfer learning framework 2', 'transfer learning 1 technique', 'transfer learning 1', 'ZH-TL', 'TL : Adv )', 'TL ; DR', 'tl ; dr', 'TL-F1', 'EN-TL', 'TL ( R-M )', 'TL ( M )']"
48,15,Method,6.2205,2015,"{'2015': 0.2925, '2016': 2.8309, '2017': 4.8558, '2018': 6.1361, '2019': 6.2205, '2020': 6.041, '2021': 5.9149, '2022': 3.9003}","['NMT', 'NMT model', 'NMT models', 'NMT systems', 'NMT system', 'neural machine translation ( NMT )', 'Neural Machine Translation ( NMT )', 'Neural machine translation ( NMT )', 'neural machine translation', 'NMT architectures', 'neural machine translation ( NMT ) models', 'NMT tasks', 'neural machine translation model', 'VAG-NMT', 'neural machine translation system', 'Neural machine translation ( NMT ) models', 'Neural Machine Translation', 'M-NMT', 'neural machine translation ( NMT ) systems', 'NMT task', 'S-NMT', 'NMT Model', 'Neural Machine Translation ( NMT ) models', 'NMT architecture', 'SD-NMT', 'Neural machine translation', 'LGP-NMT', 'TA-NMT', 'neural machine translation models', 'NMT framework', 'SD-NMT model', 'Neural Machine Translation ( NMT ) systems', 'NMT methods', 'NMT data', 'LGP-NMT+', 'deep NMT models', 'NMT-Adapt', 'nmt', 'neural MT', 'NMT approaches', 'UVR-NMT', 'NMT approach', 'M-NMT system', 'NMT Systems', 'neural MT ( NMT )', 'NMT-based systems', 'neural machine translation ( NMT ) model', 'NMT techniques', 'aux-nmt', 'NMT-learned', 'TFA-NMT', 'neural MT models', 'neural MT systems', 'neural machine translation ( NMT', 'TA-NMT ( GI )', 'NMT-based', 'CTRL-NMT', 'Neural machine translation ( NMT ) systems', 'Neural MT', 'neural machine translation systems', 'neural machine translation ( NMT ) system', 'NMT-based methods', 'NMT-based models', 'SS-NMT', 'ABD-NMT', 'ST-NMT', 'nmT5', 'Neural machine translation models', 'neural MT system', 'NMT corpus', 'Neural Machine Translation ( NMT ) model', 'NMT network', 'M-NMT architecture', 'U-NMT', 'NMT-J', 'NMT-HIERO', 'NMT Models', 'NMT dataset', 'NMT-GTM', 'NMT-J-5', 'neural machine translation framework', 'NMT-High', 'NMT 0', 'DC-NMT', 'L NMT', 'nmT5-Large', 'NMT-l2r-5', 'CC-NMT', 'neural machine translation approaches', 'Neural Machine Translation ( NMT ) system', 'In neural machine translation ( NMT )', 'NMT method', 'neural machine translation system ( NMT )', 'NMT-based approaches', 'NN-MT', 'NMT-Keras', 'NMT all', 'M nmt', 'NMT-r2l', 'SD-NMT\\K', 'NMT 4', 'NMT-l2r', 'NMT-r2l-5', 'NMT 1', 'SB-NMT', 'NMT hybrid', 'NMT sys', 'deep NMT architectures', 'deep NMT systems', 'deep NMT model', 'BLT-NMT', 'LTR-NMT', 'CS-NMT', 'NMT-QE', '• Neural machine translation ( NMT )', 'neural MT model', 'Nmt', 'Neural machine translation systems', 'Neural Machine Translation systems', 'Neural Machine Translation ( NMT', 'Neural Machine Translation NMT', 'Neural MT ( NMT )', 'neural MT ( NMT ) system', 'In Neural Machine Translation ( NMT )', 'neural MT architecture', 'neural machine translation approach', 'NMT )', 'NMT corpora', 'NMT networks', 'NMT-based system', 'LC-NMT', 'R2L NMT models', 'r2l NMT models', 'data-nmt.sh', 'FKD-NMT', 'SB-NMT model', 'TED NMT', 'deep NMT system', 'deep neural machine translation system', 'deep NMT', 'VAG-NMT model', 'NMT-r2l-10', 'TA-NMT )', 'neural machine translation ( NMT ) task', 'NMT-based TS systems', 'NMT-J )', 'A neural machine translation ( NMT ) system', 'A neural machine translation system', 'NMT 5', 'C-NMT', '-UVR-NMT', 'NMT filtering methods', 'NMT-like systems', 'NMT-model', 'Neural Machine Translation Model', 'Neural Machine Translation ( NMT ) approaches', 'neural MT ( NMT ) approaches', 'Neural MT ( NMT ) approaches', 'neural MT ( NMT ) systems', 'neural Machine Translation', 'Neural Machine Translation Systems', 'Neural MT system', 'NMT NMT', 'Neural machine translation ( NMT', 'NMT ( Neural Machine Translation )', 'NMT Neural Machine Translation models', 'Neural MT ( NMT ) models', 'Neural Machine Translation architectures', 'NMT - xx', 'Neural Machine Translation based model', 'NMT Methods', 'Neural MT ( NMT ) system', 'Neural MT architecture', 'NMT Architecture', 'neural MT approach', 'neural machine translation systems ( NMT )', 'The Neural Machine Translation ( NMT ) model', 'Base-NMT', 'NMT based models', 'Neural Machine Translation based models', 'neural ( nmt ) machine translation systems', 'neural MT ( NMT ) model', 'For neural machine translation ( NMT )', 'For Neural Machine Translation ( NMT )', 'neural machine translations ( NMT )', 'Machine Translation ( NMT )', 'machine translation ( NMT )', 'NMT translation system', 'NM T system', 'Neural Machine Translation ( NMT ) framework', 'neural machine translation ( NMT ) framework', 'network NMT system', 'network NMT systems', 'NM T systems', 'NN machine translation models', 'Neural machine translation ( MT ) models', 'neural NMT system', 'neural-network-based machine translation ( NMT ) models', 'base neural MT model', 'base NMT model', 'S-NMT )', 'large NMT model', 'neural network-based machine translation system', 'NMT modeling', 'neural machine translation ( NMT ) -based models', '( NMT )', 'NMT Techniques', 'of neural machine translation ( NMT )', 'neural machine translation ( NMT ) techniques', 'Neural network-based MT models', 'neural-based MT methods', 'Our neural machine translation ( NMT ) model', 'neural ( NMT ) systems', 'NMT model—the', 'neural machine translation ( MT ) approaches', 'neural machine translation ( NMT ) approach', 'NMT strategies', 'neural machine translation ( NMT ) based systems', 'SOTA NMT models', 'neural machine translation model ( NMT )', 'NMT algorithm', 'NMT algorithms', 'Neural Machine Translation ( NMT ) methods', 'NMT-2', 'NMT 2', 'T2T NMT', 'light NMT model', 'light NMT models', 'NMT-GMT', 'X-Y light NMT systems', 'CUDA based NMT system', 'NMT 84', 'neural machine translation ( UNMT ) models', 'Building neural machine translation systems', 'NMT-GTM model', 'DPE-NMT', 'Sim-NMT', 'SD-NMT ]', 'M-NMT approach', 'M-NMT architectures', 'r2l NMT model', 'NMT-PL', 'f NMT', 'IT NMT', 'SCI NMT', 'NMT ♦', 'NMT ( 3 \ue00d )', 'NMT-l2r-10', 'NMT + PS', 'l2r NMT models', 'NMT-r2l5', 'NMT-1', 'SD-NMT .html', 'NMT IA', 'hybrid NMT', 'Hybrid NMT', 'hybrid neural machine translation systems', 'hybrid NMT approach', 'SR-NMT', 'NMT 257', 'gated NMT', 'NMT learner', 'NTS NMT-based system', 'SA-NMT', 'fr NMT system', 'NMT×4', 'NMT × 4', 'deep neural machine translation ( NMT )', 'Deep NMT', 'child NMT model', 'Child NMT model', 'VAG-NMT mechanism', 'TA-NMT method', 'ATR-based NMT', 'ATR-based neural machine translation', 'NMT-based P2C module', 'neural machine translation tasks', 'Neural Machine Translation ( NMT ) task', 'neural machine translation ( NMT ) tasks', 'dual-NMT algorithm', 'Early neural machine translation ( NMT ) systems', 'URA-NMT', 'NMT-L', 'NMT-L )', 'DSP-NMT', 'NMT + UNK', 't NMT models', 'J nmt', 'TFA-Nmt', 'TFA-NMT models', 'as neural machine translation ( NMT ) model', 'p T NMT model', 'NMT-Adapt approach', 'gual NMT model', 'gual NMT', 'Graph NMT', 'NMT +', 'nmT5 model']"
49,477,Method,6.2009,2019,"{'2019': 0.0194, '2020': 1.8974, '2021': 4.5138, '2022': 6.2009}","['HuggingFace', 'Huggingface', 'Hugging Face', 'huggingface', 'Huggingface model', 'Huggingface 3', 'HuggingFace 1', 'HuggingFace 4', 'HuggingFace model', 'HuggingFace framework', 'HuggingFace models', 'HuggingFace 5', 'Huggingface 1', 'HuggingFace 2', 'Huggingface 2', 'HuggingFace 3', 'HuggingFace 6', 'Huggingface Model', 'Huggingface models', 'hugginface', 'Hugginface', 'Huggingface 4', 'huggingface 2', 'HuggingFace 8', 'Hugging-Face', 'HUGGINGFACE', 'HuggingFace Dataset', 'Hugging Face model', 'huggingface model', 'Hugging Face *', 'huggingface package', 'Huggingface metrics', 'Huggingface 9', 'HuggingFace 9', 'HuggingFaces', 'Huggingface 7', 'huggingface 4', 'huggingface 3', 'Hugging Face 3', 'Hugging Face 8', 'HugggingFace', 'huggingface.co', 'Huggingface 6', 'huggingface 6', 'Hugging Face Models 6', 'Huggingface ( HF )', 'HuggingFace ( HF ) models', 'hugging face 1', 'huggingface 1', 'HuggingFace base model 1', 'HuggingFace 11', 'Hugging Face ’ s']"
50,206,Metric,6.1762,2003,"{'2003': -0.0329, '2004': 0.4945, '2005': 0.0074, '2006': 0.1753, '2007': 0.3942, '2008': 0.129, '2009': 0.3754, '2010': 0.7086, '2011': 0.6336, '2012': 0.5167, '2013': 1.0449, '2014': 0.7015, '2015': 1.3201, '2016': 1.0201, '2017': 2.1556, '2018': 2.42, '2019': 2.8577, '2020': 3.9555, '2021': 4.6772, '2022': 6.1762}","['standard deviation', 'standard deviations', 'Standard deviation', 'standard deviation ( σ )', 'Standard deviations', 'standard deviation ( std )', 'Standard Deviation', 'standard deviation ( SD )', 'standard deviation σ', 'standard deviations ( std )', 'standard-deviation', 'standard deviations ( sd )', 'Standard Deviation ( σ )', 'standard deviations σ', 'standard deviation ( Std )', 'Standard deviation ( STD )', 'STD - Standard deviation', 'STD - standard deviation', 'Standard Deviations', 'standard deviation )', 'standard deviation measure', 'standard-deviation-based', 'standard deviations )', 'standard deviation metrics', 'standard deviation ( \ue021 )', 'Standard deviation bars', 'standard deviation ( RSD )', 'standard deviation ( F1 )', 'F1 standard deviation', 'standard deviation ( STDEV )', 'Standard deviation ( Stdev )', 'standard deviation ( stdev )', 'standard deviations ( stdev )']"
51,253,Tool,6.1565,2017,"{'2017': -0.0724, '2018': 2.3574, '2019': 4.4318, '2020': 5.8095, '2021': 6.1565, '2022': 5.5153}","['PyTorch', 'Pytorch', 'PyTorch framework', 'pytorch', 'PyTorch 3', 'PYTORCH', 'Pytorch framework', 'PyTorch model', 'PyTorch models', 'Pytorch 3', 'pyTorch', 'PyTorch Framework', 'Pytorch Framework', 'PyTorch-based', 'PyTorch Model', 'PyTorch module', 'Py-Torch', 'PyTorch-based model', 'PyTorch-based framework', 'pytorch optimizer', 'PyTorch dataset', 'PyTorch 0.3', 'Pytorch 3 framework', 'Pytorch 1.3']"
52,140,Method,6.1218,2010,"{'2010': -0.0838, '2011': 0.0144, '2012': 0.3632, '2013': 0.7553, '2014': 3.1578, '2015': 5.0094, '2016': 6.1218, '2017': 4.1144, '2018': 5.0741, '2019': 3.8643, '2020': 3.4405, '2021': 3.1513, '2022': 2.4184}","['backpropagation', 'BP', 'back-propagation', 'back propagation', 'Backpropagation', 'backpropagation algorithm', 'back-propagation algorithm', 'bp', 'back propagation algorithm', 'BP algorithm', 'BP-MLL', 'Back-propagation', 'BP 4', 'backpropogation', 'StackPropagation framework', 'AA-BP', 'BP system', 'backpropagations', 'Back-Propagation', 'backpropagation method', 'back-propagation method', 'BP models', 'Back Propagation', 'Backpropagation algorithm', 'backpropagation mechanism', 'back propagation ( BP ) algorithm', 'backpropagation )', 'back-propagation algorithms', 'backpropagation technique', 'BP BPL', 'StackPropagation', 'Backpropagation ( BKP )', 'BP4', 'backpropagation 3', 'BP1 )', 'back-propagation 1', 'BackPropogation', 'backpropogation algorithm', 'backpropagation algorithm 9', 'bp i', 'dz backpropagation', 'back propagation 5', 'backpropagation 5', 'BackPropagation', 'back propagation mechanism', 'back-propagation mechanism', 'backpropagation-based methods', 'backpropagation network', 'back propagation principle', 'backpropagation techniques', 'back-propagation ( BP ) algorithm', 'backpropagation based technique', 'back-propagation methods', 'backpropagation approach', 'back-propagation )', 'back propagation ( BP ) networks', 'Back-Propagation ( BP )', 'back-propagation strategy', 'back-propagation technique', 'backpropagation-based method', 'backpropation', 'Stackpropagation', 'QA + BP', 'backpropagation ( Eq .', 'BP/PL']"
53,154,Method,5.8577,2013,"{'2013': -0.0597, '2014': 0.9113, '2015': 2.9985, '2016': 4.5238, '2017': 3.4374, '2018': 5.8577, '2019': 5.0985, '2020': 3.0468, '2021': 1.8011, '2022': 2.0477}","['convolutional', 'convolution', 'convolutions', 'convolutional network', 'convolutional networks', 'convolutional layers', 'convolutional layer', 'convolutional architecture', 'convolutional models', 'Convolution', 'convolutional model', 'convolutional architectures', 'Convolutional', 'Convolutions', 'convolutional approach', 'convolution network', 'convolutional module', 'Convolutional networks', 'convolutional structures', 'convolution model', 'convolution layers', 'convolutional structure', 'convolution-based models', 'convolution networks', 'Convolutional Networks', 'Convolutional Layer', 'convolutional framework', 'convolutional method', 'convolutional approaches', 'convolutional modules', 'convolution models', 'convolution-based', 'convolution module', 'Convolutional Layers', 'Convolutional layer', 'Convolutional models', 'convolutional network architectures', 'convolutional-based models', 'convolutional network model', 'Convolution-based methods', 'convolution-based methods', 'Convolution-based', 'convolution layer', 'convolution based model', 'convolution approach', 'Convolutional ( S & Z )', 'con\ue010 volutional networks', 'convolutional Layer-1', 'Convolution Layer 1', 'Convolutional architecture', 'Convolutional architectures', 'Convolutional Network', 'Convolutional network', 'Convolutional Network based approach', 'convolutional cell', 'Convolutional Module', 'Convolutional model', 'convolutional network based models', 'convolutional methods', 'convolutional-based', 'convolutional based models', 'convolutional network models', 'Convolutional-based architectures', 'convolutional system', 'convolutional )', 'convolutional-based methods', '→convolution', 'Convolutional 6/3', 'convoluational model', 'convolutionl', 'Convolutional Q ,', 'Convolutional Q', 'convolution based methods', 'convolution structures', 'Convolution Model', 'Convolution model', 'convolution based', 'Convolution Network', 'convolution-based network', 'Convolution Networks', 'convolution architectures', 'convolution method', 'convolution network model', 'Convolution layer', 'convolution network based models', 'convolution-based model', 'convolution architecture', 'convolution-based approaches', 'convolution based method', 'convolution-based method', ') convolutions', 'convolutions )', 'convolution ]', 'convolutional en']"
54,58,Dataset,5.783,2016,"{'2016': -0.2795, '2017': 1.1548, '2018': 2.4804, '2019': 3.8968, '2020': 5.783, '2021': 5.3334, '2022': 4.7516}","['SQuAD', 'SQuAD 2.0', 'SQuAD dataset', 'SQuAD 1.1', 'SQuAD v1.1', 'SQUAD', 'SQuAD1.1', 'SQuAD2.0', 'SQuAD-Open', 'SQuAD v2.0', 'Stanford Question Answering Dataset ( SQuAD )', 'SQuAD2', 'SQuAD 2.0 dataset', 'SQuAD dev set', 'SQUAD1.1', 'SQuAD data', 'SQuAD Open', 'SQUAD-ADV', 'SQuAD 1.1 dataset', 'SQuAD model', 'SQuAD dev', 'SQuAD models', 'SQUAD 2.0', 'Stanford Question Answering Dataset', 'SQuAD 1.0', 'Squad', 'SQuAD en', 'SQUAD dataset', 'SQuAD k=1', 'SQuaD', 'SQuAD corpus', 'SQuAD-it', 'SQuAD-T', 'SQUAD 1.1', 'Squad1.1', 'SQuAD v1.1 dataset', 'SQUAD 2', 'SQuAD 2.0 models', 'squad', 'SQuAD )', 'SQuAD OPEN', 'SQuAD QA', 'SQUAD1.1 dataset', 'SQUAD1.1 data', 'SQuAD-v2', 'SQuAD v 2.0', 'Spoken SQuAD', 'SQuAD v 1.1', 'SQUAD2.0', 'SQUAD dev set', 'SQuAD & Yes', 'SQuAD-QG', 'SQuAD-1.1', 'SQuAD 1', 'SQuAD-v1.1', 'SQuAD v1', 'SQuAD 2', 'SQuAD-2', 'SQuAD-2.0', 'SQuAd', 'SQuAD data set', 'SQuAD-en', 'XSUM SQuAD 1.1', 'SQuAD QG dataset', 'Squad 1.1', 'SQuAD 1.1 models', 'SQuAD-1', 'SQuAD1', 'SQuAD 1.1 model', 'Stanford Question Answering Dataset ( SQuAD ) 1.1', 'SQuAD-v2.0', 'SQuAD ( v2.0 )', 'Spoken SQuAD dataset', 'SQuaD v1.1', 'SQuAD v1.1 dev set', 'UB SQuAD', 'SQuAD2 models', 'SQuAD2 dataset', 'ITFT-SQuAD', 'SQuAD2.0 dataset', 'SQuaD 2.0', 'Squad 2.0', 'SQuAD 2.0 data', 'Stanford Question Answering Dataset 2.0', 'SQuAD dev dataset', 'Stanford question answering dataset', 'SQuAD Models', 'Stanford Question Answering ( SQuAD ) dataset', 'SQuAD 320', 'D SQuAD', 'SQuAD k', 'BiDAF ( SQuAD ) model', 'SQuAD v1.0', 'SQuAD-open', 'SQUAD-T', 'SQuAD dataset first', 'SQuAD-NQG', 'SQuAD QA model', 'SQUAD QA', 'SQuAD QA system', 'SQuAD V1 4 data', 'SQuAD Q & A set', 'SQuAD & Y', 'biased SQuAD dataset', 'SQuAD * & Yes', 'SQuAD QG', 'SQuaD 1.1', 'SQuAD1.1 Models', 'Squad 1', 'SQuAD1.1 dataset', 'SQuAD-1.1 dataset', 'Stanford Question Answering Dataset 1 ( SQuAD )', 'SQuAD dataset 1', 'Stanford Question Answering Dataset ( SQuAD 1.1 )', 'SQuAD-1.1 data', 'SQuAD 1.1 corpus', 'SQuAD ( 1.1 )', 'SQUAD-ADV dataset', 'SQUAD-ADV Approach', 'SQUAD V2', 'SQuaD v2.0', 'SQuAD v2.0 model', 'SQuAD ( 2016 )', 'SQuAD k ( a )', 'SQuAD ( SQ )', 'SQuAD V1.1', 'SQuAD v1.1 data set', 'SQuAD dataset v1.1', 'SQuAD V1.1 dataset', 'SQuAD ( v1.1 ) dataset', 'SQUAD v1', 'Squad v1', 'SQuAD v1 dataset', 'Stanford Question Answering Dataset ( SQuAD ) v1.1', 'Stanford Question-Answering Dataset ( SQuAD ) v1.1', 'Stanford Question Answering ( SQuAD ) v1.1', 'SQuAD v1.1 corpus', 'Stanford Question Answering Dataset ( SQuAD v1.1 )', 'SQuAD v1.1 2', 'SQuAD 1.1 6', 'SQuAD2-', 'SQuAD-2 dataset', 'Stanford Question Answering Dataset 2', 'SQuAD 2 data', 'SQuAD2.0-Q', 'SQuAD2.0-C', 'SQUAD2.0 dataset', 'SQuad 2.0', 'SQuAD 2.0 dev set', 'SQuAD 2.0 Dev Set', 'SQuAD 2.0 model', 'squad-2.0 model', '–SQuAD2.0', 'SQuAD 2.0 question answering dataset', 'SQuAD 2.0 dev dataset', 'squad_f1', 'Squad-F1', 'F1 SQuAD', 'SQuAD2.0 3', 'SQuAD dev set—87.9', 'SQuAD ( Score', 'SQuaD dataset', 'SQuad dataset', 'SQuAD Dataset', 'SQuAD Dev', 'Stanford Question-Answering Dataset ( SQuAD )', 'Stanford question answering dataset ( SQuAD )', 'SQUAD corpus', '( SQuAD )', 'SQuAD ( S ) dataset', 'SQuAD question answering data', 'SQuAD dev-set', 'SQuAD Dev set', 'SQuAD-based', 'Squad-based', 'SQu AD', 'Stanford Question Answering', 'Squad question answering dataset', 'SQUAD models', ""SQuAD ''"", 'SQuAD *', 'Answering Dataset ( SQuAD )', 'Stanford Question Answering SQuAD', 'SQuAD-model', 'SQUAD model', 'Stanford Question Answering Dataset ( SQuAD', 'Stanford Question Answering Dataset ( SQUAD', 'SQuAD dev sets', 'Stanford Question Answer Dataset ( SQuAD )', 'SQUAD data set', 'SQuAD system', 'Stanford Question Answer', 'SQuAD question answering', 'SQuAD models—', 'SQuAD data sets', '-SQuAD', 'question SQuAD dev dataset', 'SQuAD SQuAD', 'SQuAD in', 'SQuAD architectures', 'SQuAD gold', 'SQuAD—on', 'SQuAD - 305', 'SQuAD - 51', 'SQuAD 3 3', 'SQuAD-only', 'squad.py', 'SQuAD 6', 'SQuAD dataset 6', 'SQuAD 2.0 9', 'SQuAD ( # 9 )', 'SQuAD-Q', 'SQuAD k=1 models', 'SQuAD Open dev 1 set', 'SQuAD 4', 'SQuAD dataset 4', 'SQuAD v1.1 8', 'SQuAD ReQA', 'SQuAD 86588', 'squad ) juge_d', 'SQuAD v1.0 dataset', 'SQUAD v1.0', 'SQuAD open', 'SQuAD-open dev set', 'SQuAD-Open dataset', 'SQuAD OPEN )', 'open SQuAD', 'Open SQuAD']"
55,136,Metric,5.7151,2004,"{'2004': -0.1801, '2005': -0.2283, '2006': -0.0837, '2007': -0.2628, '2008': 0.3487, '2009': -0.0771, '2010': 0.2177, '2011': 0.0828, '2012': -0.1646, '2013': 0.0594, '2014': -0.1919, '2015': 0.1937, '2016': 0.5451, '2017': 0.6729, '2018': 3.2727, '2019': 3.3675, '2020': 3.6875, '2021': 4.3065, '2022': 5.7151}","['ROUGE-L', 'Rouge-L', 'ROUGE-L.', 'ROUGE L', 'ROUGE-L )', 'rouge-L', 'ROUGE-l', 'ROUGE-L measures', 'ROUGE-2/L', 'ROUGE-L metrics', 'ROUGE-1/L', 'Rouge L', 'Rouge-L.', 'ROUGE2/L', 'ROUGE-2 ROUGE-L 2', 'ROUGE-1-2-L', 'ROUGE - L', 'ROUGE_L', 'ROUGE-L *', 'ROUGE -L.', 'ROUGE-L=', 'ROUGE-l.', 'Rouge-L )', 'ROUGE-L value', 'ROUGE-L measure', 'ROUGE-L model', 'ROUGE-L Model', 'Model ROUGE-L', 'ROUGE-L Metric', 'ROUGE1/L']"
56,133,Method,5.5429,2005,"{'2005': -0.134, '2006': 0.3863, '2007': 2.3364, '2008': 2.8914, '2009': 3.5517, '2010': 3.1715, '2011': 4.3612, '2012': 3.9043, '2013': 5.5429, '2014': 3.1348, '2015': 2.5069, '2016': 1.5274, '2017': 0.6534, '2018': 0.1814, '2019': -0.1107, '2020': -0.1306, '2021': -0.1808, '2022': -0.1841}","['MERT', 'minimum error rate training', 'minimum error rate training ( MERT )', 'Minimum Error Rate Training ( MERT )', 'Minimum Error Rate Training', 'MERT algorithm', 'SA-MERT', 'minimum-error-rate training', 'minimum error-rate training', 'MERT training', 'SA-MERT pot', 'Z-MERT', 'Minimum error rate training ( MERT )', 'minimum error rate training ( MERT ) algorithm', 'N-best MERT', 'Minimum error rate training', 'SA-MERT par', 'kb-mert', 'lb-mert', 'N -best MERT', 'n-best MERT', 'MERT approach', 'mert+b', 'MERT-opt', 'Minimum Error Rate training', 'Minimum Error-Rate Training ( MERT )', 'minimum error-rate training ( MERT )', 'minimum error rate training method', 'MERT method', 'minimum error rate training algorithm', 'MERT )', 'MERT model', 'minimum-error-rate trainer', 'mert', 'MERT ( Minimum Error Rate Training )', 'Minimum Error Rate Training algorithm', 'MERT optimizer', 'minimum error rate training ( MERT ) method', 'error rate training ( MERT )', 'MERT framework', 'base MERT', 'MERT-S', 'MERT 2', 'mert-b', 'PMO-MERT 4', 'MERT 3', 'PRO MERT', 'PRO–MERT', 'MERT PRO', 'MERT/PRO', 'minimum error rate ( MER ) training', 'MERT Max', 'Minimim Error Rate Training', 'minimum-error rate trainer', 'minimum-error-ratetraining', 'MERT 6', 'SAMT MERT', 'SA-MERT -0.1', 'MERT code', 'Z-mert 5', 'minimum error rate training 4', 'MERT 4', 'minimum-error-rating training', 'Minimum-error-rate training', 'minimum-error rate training', 'Minimum Error-Rate Training', 'minimum-error rate training ( MERT )', 'Minimum error-rate training ( MERT )', 'Minimum-Error Rate Training ( MERT )', 'Minimum-error-rate training ( MERT )', 'MERT ( minimum error rate training )', 'minimum-error-rate training method', 'Minimum Error Rate Training method', 'minimum error-rate training algorithm', 'MERT ) algorithm', 'Minimum Error Rate training algorithm', 'minimum-error-rate training algorithm', 'Minimum-Error Training ( MERT )', 'Minimum Error Training ( MERT )', 'MERT module', 'Minimum Error Rate Training ( MERT ) algorithm', 'MERT-based', 'MERT system', 'MERT training algorithm', 'minimum error rate training methods', 'MERT trained system', 'MERT data', 'minimum error rate training ( MERT ) approach', 'MERT-trained', 'minimum error rate training ( MERT ) technique', 'minimum error-rate training ( MERT ) technique', 'MERT ( minimum-error-rate training', 'MERT technique', 'minimum error rate ( MERT )', 'minimum error training ( MERT ) algorithm', 'MERT architecture', 'k-best MERT', 'Och ’ s MERT algorithm']"
57,85,Method,5.4869,2004,"{'2004': 0.048, '2005': 0.8655, '2006': 0.7029, '2007': 0.6584, '2008': 0.8056, '2009': 0.9548, '2010': 1.0663, '2011': 1.4332, '2012': 2.5608, '2013': 2.2617, '2014': 3.3675, '2015': 5.4869, '2016': 3.8322, '2017': 3.6598, '2018': 4.0606, '2019': 3.744, '2020': 2.797, '2021': 2.4364, '2022': 1.6322}","['logistic regression', 'LR', 'logistic regression model', 'logistic regression classifier', 'Logistic Regression', 'logistic regression models', 'Logistic Regression ( LR )', 'logistic regression ( LR )', 'logistic regression classifiers', 'Logistic regression', 'LR classifier', 'Logistic Regression classifier', 'Logistic Regression model', 'logistic regressions', 'LogisticRegression', 'logistic regression algorithm', 'logistic regression )', 'LR classifiers', 'LR ( Logistic Regression )', 'gistic regression', 'Logistic regression model', 'Logistic Regression Classifier', 'Logistic regression models', 'Logistic Regression Models', 'logistic regression analysis', 'flat logistic regression', 'logistic-regression', 'logistic-regression model', 'Logistic regression classifier', 'Logistic Regression classifiers', 'Logistic Regression )', 'logisticregression model', 'logistic regression methods', 'logistic regression algorithms', 'logistic regression framework', 'local logistic regression classifiers', 'logistic regression fits', 'logistic regression—can', 'Logistic Regression Classifier 10', 'logistic regression 3', 'logistic regression classifier 3', 'logistic regression classifier 2', 'logistic regression model 2', 'PV Logistic regression', 'batch logistic regression classifier', 'l 1 -logistic regression', 'logistic regression 5', 'F1 Logistic Regression', 'L2 logistic regression classifier', 'Logistic regression-17', 'Logistic Regression ( LGR )', 'logistic regression ( LGR )', 'logistic Regression', 'Logistic Regression Model', 'Logistic Regression models', 'logistic-regression classifiers', 'logistic regression ( LR ) models', 'logistic regression analyses', 'logistic regression classification model', 'logistic regression package', 'logistic regression approach', 'logistic regression network', 'base logistic regression model', 'logistic regression system', 'Logistic Regression Classification', 'logistic regression classification', 'logistic-regression-based approach', 'logistic regression-based technique', 'logistic regression networks', 'logistic regression method', 'LogisticRegression classifier', 'Logistic Regression classification system', 'logistic regression modeling', 'logistic regression model ( LR )', 'gistic Regression model', 'logistic regression ( I-O )', 'logistic regression model 1', 'logistic regression classifier 1', 'Logistic regression 1', 'logistics regression model', 'local logistic regression classifier', 'local logistic regression', 'Logistic Regression ( HI )', 'logisitic regression models', 'logisitic regression']"
58,186,Metric,5.4284,2019,"{'2019': -0.0278, '2020': 1.5164, '2021': 2.7512, '2022': 5.4284}","['BERTScore', 'BertScore', 'BERTSCORE', 'BERTscore', 'BERTScores', 'Bertscore', 'BERTScore ( BS )', 'BERTScore metrics', 'BertScores', 'MLBERTScore', 'R-BertScore', 'BERTScore++', 'BertScore↑', 'BERTScore 4', 'BERTScore (', 'BERTScore )', 'BERTScore model', 'BERTScore 6', 'BLEURT score', 'BERTScore-R.', 'BERTScore-R', 'BERTScore-r', 'BERTScore 7', 'DABERTScore', 'G-BERTScore', 'BertScore ( BS )', 'BERTSCore', 'BERTScore algorithm', 'BERTScore mechanism', 'BERTScore-', 'BertScore )', 'BertScore Model', 'BERTScore Model', 'BertScore Metrics', 'BERTScore metric', 'BERTS core', 'BERTScore-based )', 'BERTScore 2', 'BLEURT scores', 'BERTScore 12']"
59,103,Method,5.3619,2016,"{'2016': 0.1537, '2017': 1.8201, '2018': 3.2068, '2019': 4.7838, '2020': 5.3619, '2021': 4.9928, '2022': 3.4579}","['BPE', 'byte-pair encoding', 'Byte Pair Encoding ( BPE )', 'byte-pair encoding ( BPE )', 'byte pair encoding ( BPE )', 'BPE model', 'byte pair encoding', 'Byte-Pair Encoding ( BPE )', 'bpe', 'BPE models', 'BPE algorithm', 'Byte-Pair-Encoding ( BPE )', 'Byte-Pair Encoding', 'byte pair encoding algorithm', 'BPE ′', 'Byte Pair Encoding', 'Byte-pair encoding', 'byte-pair encoding ( BPE', 'byte pair encoding ( BPE', 'byte pair encoding ( BPE ) algorithm', 'BPE-J90k', 'BPE-30K', 'byte pair encoded ( BPE )', 'BPE system', 'BPE-60k', 'BPE 2', 'byte-pair-encoding ( BPE )', 'Byte pair encoding ( BPE )', 'Byte Pair Encoding ( BPE', 'BPE-based', 'BPE-60K', 'BPE-1K', 'byte-pair-encoding', 'Byte-pair encoding ( BPE )', 'LARGE-BPE models', 'BPE method', 'Byte-Pair-Encoding', 'Byte-Pair-Encoding ( BPE', 'BPE )', 'byte-pair encoding ( BPE ) algorithm', 'BPE 5', 'BPE-60k system', 'BPE 1', 'BPE ( Byte Pair Encoding )', 'Byte-pair encoding model', 'Byte Pair Encoding ( BPE ) algorithm', 'SMALL-BPE', 'LARGE-BPE systems', 'BPE-based model', 'BPE algorithms', 'BPE 64k', 'BPE100', 'Byte Pair Encoding ( BPE ) 7', '+bpe', 'bpe.ww', 'BPE32k', 'byte-pair encoding 3', 'byte pair encoding ( BPE ) 3', 'byte-pair encoding 2', 'Byte-Pair-Encoding 2', 'BPE 2 Base', 'byte-pair encoding 1', 'BPE 6', 'BPE ’ d', 'Byte-pair-encoding ( BPE )', 'BPE ( Byte-Pair Encoding )', 'BPE ( bpe )', 'Byte-Pair encoding ( BPE )', 'bpe model', 'BPE ) model', 'byte-pair encoding – BPE', 'byte-pair-encoding ( BPE', 'Byte-Pair Encoding ( BPE', 'Byte-pair encoding ( BPE', 'BPE framework', 'BPE-', 'byte pair encoding (', 'byte-pair encoding )', '( BPE )', 'byte pair encoding ( BPE ) technique', 'BPE—the', 'LARGE-BPE', 'SMALL-BPE models', 'Byte-Pair Encoding ( BPE ) method', 'byte pair encoding ( BPE ) method', 'byte-pair-encoding ( BPE ) method', 'byte-pair encoding ( BPE ) method', 'byte-pair encoding ( BPE ) approach', 'Bytes-Pair-Encoding ( BPE )', 'Byte-Pair-Encoding based model', 'BPE data', 'BPE-based models', 'BPE based models', 'Byte pair encoding algorithm ( BPE )', 'Byte Pair Encoding modeling', 'pair encoding ( BPE )', 'Byte-pair encoding ( BPE ) algorithms', 'BPE corpus', 'byte-pair encode', 'BPE encoding', 'byte pair encoding ( BPE ) strategy', 'BPE-encoded', 'byte pair encoding technique', 'byte-pair encoding technique', 'BPE Byte Pair Encoding ( BPE', 'BPE-based system', 'byte-paired encoding ( BPE', 'byte-pair ( BPE )', 'BPE ( × )', 'BPE-D', 'H BPE', 'BPE-10K', 'BPE-10k', 'BPE-10k sets', 'BPE1k', 'BPE-100K )', 'BPE-100K', 'BPE-300K', 'BPE 4', 'BPE-9K']"
60,118,Tool,5.3327,2007,"{'2007': 0.2692, '2008': 1.8657, '2009': 3.3326, '2010': 3.0473, '2011': 3.4277, '2012': 3.642, '2013': 5.3327, '2014': 3.1693, '2015': 3.7239, '2016': 3.646, '2017': 2.6629, '2018': 2.0919, '2019': 0.6723, '2020': 1.2603, '2021': 1.1163, '2022': 0.1315}","['Moses', 'MOSES', 'Moses system', 'Moses 5', 'moses', 'Moses 1', 'moses15', 'Moses15', 'Moses )', 'Moses systems', 'MOSES system', 'moses system', 'Moses framework', 'Moses model', 'Moses-based system', 'MOSES 5', 'Moses-1', 'MOSES 1', 'Moses 1 system', 'Moses 15', 'Moses,15', 'Moses-based systems', 'MOSES package', 'Moses models', 'Moses-based', 'Moses packages']"
61,22,Dataset,5.3056,2010,"{'2010': 0.2562, '2011': 1.9871, '2012': 2.1162, '2013': 3.1117, '2014': 4.2322, '2015': 5.3056, '2016': 4.4136, '2017': 4.0461, '2018': 4.7524, '2019': 5.0936, '2020': 4.589, '2021': 4.4148, '2022': 4.3375}","['Twitter', 'TM', 'Twitter data', 'Twitter dataset', 'twitter', 'Twitter corpus', 'TWITTER', 'Twitter corpora', 'Twitter data set', 'twitter data', 'twitter dataset', 'TM system', 'Twitter Corpus', 'twitter corpus', 'Twitter data sets', 'Twitter model', 'Twitter 2', 'Twitter Dataset', 'Twitter Data', 'Twitter )', 'SS-Twitter', 'hTwitter', 'Twitter Model', 'Twitter- #', 'TWITTER data', 'Twitters', 'Twitter Model ( TM )', 'Twitter-2', 'Twitter set', 'Twitter-based dataset', 'Twitter-based', 'Twitter-', 'twitters', 'Twitter models', 'TWITTER-S', 'Twitter- @', 'Twitter system', 'Twitter data 3', 'hTwitter set', 'hTwitter sets', 'hTwitter Sets', 'hTwitter data', 'Twitter 8', 'Twitter ( 20 )', 'Twitter Corpora', 'Twitter-Corpus', 'TWITTER dataset', 'Twitter data-set', 'TWITTER data set', 'Twitter dev set', 'Twitter network', 'Twitter Twitter corpus', 'Twitter ( Twitter Dataset )', 'gold Twitter Dataset', 'Twitter Models', 'twitter models', 'Twitter-based approach', 'Twitter based method', 'Twitter-based approaches', 'T WITTER', 'T WITTER dataset', 'Twitter @', 'Twitter methods', 'Twitter systems', 'Twitter +', 'Twitter data set 3', 'Twitter 3', 'Twitter corpus 2', 'twitter 2']"
62,112,Dataset,5.2605,2009,"{'2009': -0.0511, '2010': -0.1164, '2012': -0.219, '2014': -0.1316, '2015': 0.369, '2016': 0.4254, '2017': 0.0739, '2018': 0.9952, '2019': 1.5357, '2020': 2.869, '2021': 3.9867, '2022': 5.2605}","['SST-2', 'SST', 'SST-5', 'SST2', 'SST-2 dataset', 'SST dataset', 'SST2 dataset', 'SST5', 'SST-5 dataset', 'SST-2 dev set', 'SST )', 'subset tree ( SST )', 'SST dev set', 'SST data sets', 'Subset Tree ( SST )', 'SST model', 'SubSet Tree ( SST )', 'SST-5 5', 'SST-2 SST-5', 'SST2 SST5', 'SST-2 dev', 'SST-B', 'SST K', '+SST-2', 'SST-2 1', 'Subset Tree', 'SST-dataset', 'subset trees', 'SST classifier', 'SST systems', 'SST dataset )', 'SST data', 'SST5 dataset', 'SST5 5', 'SST l models', 'SST-2 4', 'SST2 - 11', 'sst-2', 'SST-2 Dataset', 'SST2 dev set', 'SST2 dev', 'SST-2 models', 'SST-2 dev sets']"
63,152,Tool,5.2089,2003,"{'2003': 0.3318, '2004': 0.9657, '2005': 1.8602, '2006': 1.9435, '2007': 3.3682, '2008': 2.5854, '2009': 3.3903, '2010': 3.683, '2011': 4.0345, '2012': 4.2594, '2013': 5.2089, '2014': 3.8329, '2015': 2.1865, '2016': 1.7383, '2017': 0.3822, '2018': 0.0033, '2019': 0.0755, '2020': 0.2094, '2021': 0.3132, '2022': -0.0306}","['GIZA++', 'Giza++', 'GIZA++ Model 4', 'giza++', 'GIZA++ )', 'GIZA++ 2', 'GIZA++ 3', 'GIZA++ 5', 'GIZA++ Model-4', 'GIZA++ model 4', 'GIZA++ package', 'GIZA++ models', 'GIZA++ system', 'GIZA++ 4', 'giza++ package', 'GIZA++ model', 'GIZA++- ` 0', 'GIZA++ 0', 'Giza++ 2', 'GIZA++ ∗', 'GIZA++ 8', 'Giza++ Model 4', '∗GIZA++', 'GiZA++', 'Giza++ )', 'Giza++ package', 'GIZA++ Models', 'Giza++ models', 'GIZA++ based system', 'GIZA++-based system', 'Giza++ model', 'GIZA++-based method', 'GIZA++-based systems', 'GIZA++ algorithm']"
64,215,Method,5.1907,2019,"{'2019': 1.6455, '2020': 4.2417, '2021': 5.1907, '2022': 4.5011}","['Phrase-BERT', 'Mirror-BERT', 'BERT ranker', 'Lattice-BERT', 'BERT-based graders', 'BERT-RC', 'BERT rankers', 'DeFormer-BERT', 'BERT reader', 'BERT-pair-QA-B', 'BERT random', 'DeFormer-BERT-base', 'Product-BERT', 'BERT-based ranker', 'VisDial-BERT', 'HMEAE ( BERT )', 'BM25+BERT', 'BERT w/ Mean pooling', 'FedRec ( BERT )', 'BERT-Extr', 'BERT+Random', 'BERT-FTC', 'FCA-BERT', 'BERT 12L/768H', 'Soft-Masked BERT', 'BERT ( Frozen )', 'WD-Match ( BERT )', 'BM25 + BERT', 'BERT+APD', 'BERT+DIM', 'BERT-HIGRU-f', 'PaSeR-BERT base', 'BERT + RANKING', 'BERT+AKK ( mono )', 'BERT ( Oracle )', 'BERT-wwm-ext-base', 'BERT-wwm-ext', 'RAT-SQL + BERT', 'T-BERT marked -MP', 'BERT Ranking Parser', 'BERT+PDRMM', 'BERT-DExp + emb', 'BERT-DOC', 'MRQA-BERT', 'CoFEE-BERT', 'MMSD-BERT', 'DNNC-BERT', 'BERT-HIGRU-sf', 'BERT-based MIMO', 'BERT-based parser', 'BERT BASE -Random', 'BERT-4-256', 'MathQA-BERT', 'PACSUM ( BERT )', 'BERT/ELECTRA', 'QR-BERT', 'Lattice-BERT model', 'BERT-Pairwise', 'BERT-based pairwise model', 'BERT-rtv', 'UMS BERT +', 'BERT-pair-QA-M', 'BERT-MCRF-decoding', 'BERT-wwm-open', 'BERT-based ranking models', 'BERT-Ranker', 'BERT-based rankers', 'BERT-PR', 'BERT-QA-Doc', 'Mobile-BERT', 'BERT FC', 'T-BERT marked -TS', 'SOTA Soft-Masked BERT model', 'BERT frozen model', 'BERT frozen', 'BERT-MK', 'BERT-based TDP models', 'BERT + BM25', 'BERT probe', 'ATLOP+BERT', 'DeFormer-BERTlarge', 'DeFormer-BERT-large', 'DeFormer-BERT BERT', 'BERT 108M', 'KnowBert-W+W BERT BASE', 'BERT parameters', 'gual BERT', 'PRGC BERT', 'BERT matching ( BEM )', 'BERT·SC+', 'fin-bert', 'T-BERT marked', 'Mask Filling ( BERT )', 'BERT-Large reader', 'BERT-based reader', 'BERT+DCMN+', 'MN + BERT', 'WPZ + BERT', 'BERT-PKD 6', 'Mobile-BERT 6', 'BERT + SOC', 'TMN BERT', 'Recall F1 BERT', 'random BERT', 'HIN-BERT', 'CSRL-BERT', 'CGT ( BERT )', 'Path-BERT', 'MIMIC-Full-BERT', 'BERT-type models', 'IRNet + BERT', 'MTTSMSA ( BERT )', 'BERT + Aug', 'GMN-BERT', 'BERT-FT RACE', 'BERT-AH', 'KID ( BERT )', 'Oracle BERT', 'BERT pruning approaches', 'Q-BERT 2-8-8', 'BERT-FlowLP', 'Product-Bert', 'BERT study', 'BERT-5000', 'BERT-Pairwise+Dev', 'BERT BASE + TA', 'BERT-FK', 'RAT-SQLv3+BERT', 'BERT+ Argmax', 'BERT + Neutral', 'IRNet V2 + BERT', 'BERT-based grader', 'BERT-FEDA', 'MAPO w/ BERT', 'BERT-kv', 'Kaleido-BERT', 'COPY+BERT', 'TOD-BERT-LA', 'BERT + CDA', 'Col-BERT', 'BERT-rank', 'BERT rank', 'Bert Ranking', 'BERT-based neural network', 'BERT-term MLA', 'BERT-based ’ WEs', 'BERT 6L-768H', 'BERT 8L-256H', 'Bert-Ranker', 'BERT ranker model', 'BSL-BERT', 'HIER-SCI-BERT', 'BERT-Ranker ( BR )', 'BERT-REG', 'PR-Bert', 'ProtoNet-BERT', 'BERT- ( N ) ROP', 'T + BERT', 'BERT-AQuA', 'BERT-AQuA-RAT', 'BERT-ROP', 'BERT-pair-QAB', 'BERT 10−12', 'BERT 11−12', 'BERT QA on', 'BERT-wwm+1T', 'BERT-wwm+2T', 'BERT-wwm+3T', 'Mobile BERT', 'Mobile BERT model', 'BERT-small 66M', 'BERT-wwm+MWA', 'RelUNet BERT', 'wl + BERT', 'Co-BERT', 'T-BERT marked TS', 'BERT + Dep ( IIR )', 'BERT+MTB', 'BERT-bese', 'BERT + PHQA', 'BERT GF', 'S2ORC-BERT', 'Mirror-BERT tuning', 'BERT + Mirror', 'prompt-based BERT models', 'Prompt-BERT', 'human-BERT', 'BERT HUMAN', 'soft-masked BERT model', 'RAT-SQL B + ADV BERT', 'frozen BERT', 'frozen BERT classifier', 'BERT value ( freeze )', 'frozen BERT model', 'BERT-Freeze', 'BERT/CLIP-T', 'BERT boxes', 'BERT + VLM', 'HGCN-BERT + BERT-TFM', 'RATSQLv3+BERT', 'HGCN-BERT', 'DC-BERT', 'LMMS BERT', 'BERT Late+', 'META-BERT-NoMeta', 'META -BERT-NoMeta', 'C2P-BERT Matching', 'BERT ( Margin )', 'BERT margin', 'bert-hsd-layer10-c227', 'BERT-large mode', 'BERT-de 8', 'BERT+AKK', 'BERT/ROB architecture', 'BERT+S1', 'P-BERT †', 'BERT + pooling', 'RAT-SQL BERT', 'RAT-SQL BERT +MAS', 'EN ( BERT-Base ) –IT', 'bert-dbmdz', 'BERT-base + RLMTL', 'Google BERT Base 9', 'BERT MSM', 'BM25 + BERT-base model', 'BERT 15', 'BERT w/Pa', 'BERT ( 104 )', 'BERT 104', 'BERT ’ s [ CLS ]', 'L-Bus BERT', 'BERT-large based detecting model', 'bert w2v', 'BERT ( NoAug )', 'BERT tok', 'BERT-TOK', 'KVL-BERT', 'BERT ( Keep )', 'BERT-based evi', 'BERT ( 101 )', 'BERT ( 27 )', 'BERT ( 45 )', 'BERT ( 29 )', 'BERT+KM', 'BERT-RCMs', 'BERT base +UMAP', 'BERT and ReBERTa )', 'BERT era', 'BERT-era modeling', 'BERT·SC', 'RACL-BERT 1', 'BERT FM', 'BERT-L2X', 'BERT-IBA', 'QASF BERT', 'BERT w/ Mean pooling ’', 'BERT base model 522', 'BERT TN', 'DeFormer-BERTbase', 'DeFormer-BERT-large models', 'DeFormer-BERT model', 'BERT-wwm-est', 'BERT-wwm ( 110M )', 'bert-base-german-cased model 3', 'BERT based generator', 'BERT-based AC', 'BERT ac', 'BERT-345M QA model', 'w/ BERT QA model', 'CDSSM BERT', 'WMSEG ( BERT )', 'BERT ( no aug )', 'BERT + Syn', 'BERT-based TWASP', 'BERT + HN', 'BERT lg', 'BERT-lg', 'BERT+Dep ( SCT )', 'BERT+Dep', 'Freq BERT', 'BERT pool method', 'BERT-to-BERT Books model', 'BERT-to-BERT Books models', 'Irish BERT model', 'BERT ♢', 'BERT rameters', 'BERT-QA 10k', 'BERT-HIGRUsf', 'Bert Match ( BEM )', 'BERT matching ( BEM ) model', 'BERT-HIGRUf model', 'BERT pip.', 'BERT + RR', 'BERT + WR', 'BERT lbl', 'BERT-FTC models', 'BERT-SFT', 'BERT mod', 'KnowBERT BERT', 'BERT-G3', 'BERT-base ( 110M ) namodel', 'VL-BERT_L', 'BERT_few', 'Yelp Bert', 'BERT AOPC', 'BERT transformer reader', 'BERT reader model', 'BERT-base READER model', 'HeadPrune-BERT BASE', 'HeadPrune-BERT', 'v2+BERT', 'BERT base V100x64', 'EE + BERT-large', 'MIMIC-BERT models', 'BERT 3 -PKD', 'BERT+DCMN+ model', 'QANet + BERT', 'BERT 12 BERT 6', 'BERT QA 7', 'R ∗ BERT', 'CEM-BERT ( C )', 'BERT-wwm-ext ∗', 'BERT + BERT-rank', '+BERT-rank', 'Marco-BERT - -', 'BERT BASE 12 -T', 'BERT-based generative model', 'BERT-based generative framework', 'BERT-based generative models', 'BERT-PKD 4', 'BERT-pair-QA-B model', 'News BERT', 'IBM BERT', 'non-BERT parsers', 'Q-BERT 8-8-8', 'BERT 32-32-32', 'BERT-based parsers', 'BERT RanMask', 'Q-BERT 2/4-8-8', 'Q-BERT 2/3-8-8', 'BERT ( LeeBERT )', 'BERT-CALC', 'dbmz BERT 5', 'Y-BERT', 'BERT + sem', 'BERT + dep', 'BERT +char', 'DE BERT +', 'ES BERT +', 'TR BERT +', 'BERT T r', 'KO BERT +', 'ZH BERT +', 'BERT Arg', 'BERT-based FITE model', 'BERT-itpt', 'BERT-B SE', 'BERT + disease )', 'BERT+BART', 'BERT-FR-NS', '• +BERT', 'MMFT-BERT', 'BERT random dev set', 'BERT-base Random', 'BERT-wwmext', 'BERT 1721', 'BERT-based MEL systems', 'BERT UDALM', 'Net + BERT-S', 'BERT-Large OOM', 'BERT P BERT R', 'CEAS ( BERT )', 'Bert met', 'BERT-base-cased ” 6', 'BERT-large-cased ” 6', 'MERET-BERT', 'BERT BASE V & L', '- - CGT ( BERT )', 'M AR C O ( BERT )', 'M ARCO ( BERT )', 'Merge-BERT', 'BERT-merged', 'BERT-Base §', 'BERT Transformer stacks', 'BERT stack', 'BERT 1e-4', 'MIMIC-FULL-BERT', 'BERT ( EngBERT )', 'BERT+QA )', 'BERT 128-128', 'IMOJIE ( w/o BERT ) model', 'IMOJIE ( w/o BERT )', 'BERT-L12', 'BERT-top', 'BERT errors', 'BERT-self', 'BERT-type', 'DFGN + BERT', 'Bert-to-Bert ( B2B )', 'BERT-L †', 'F1 ( F BERT )', 'F BERT ( F 1 )', 'BERT LARGE †', 'UHD-BERT 5', 'BERT mean-pooling', 'mean-pooled BERT base', 'BERT-MAST', 'BERT-MAMT', 'BERT-FlowLP + emb', 'BERT-large OneIE', 'OneIE-bert-base', 'Ent bert', 'BERT base iACE', 'BERT + Aug model', 'BERT/BioBERT', 'BERT ( Ro-BERT )', 'Ro-BERT', 'BERT-M †', 'BERT 12- and 24-layer models', 'BERT B l', 'Bert for QA ”', 'BERT ( a ) -Mean pooling', 'BERT ( a ) -Max pooling', 'MEM-BERT', 'BERT2Span * *', 'BERT + Mean-Pooling', 'BERT + Max-Pooling', 'BERT-l-c-4', 'QACG-BERT', 'BERT + EDF', 'BERT-generated', 'BERT + MDF', 'BERT-l-u-3', 'bert-based CBERT methods', 'BERT ( UM )', 'Conv+BERT', 'BERT + D m', 'KB-BERT', 'BERT-PredOut', 'BERT-base PoE model', 'BERT + BiEDU', 'BERT B cn', 'Mean BERT rank', 'BERT-250 k', 'BERT base - RCMD', 'BERT base -RCMD', 'BERT+mm', 'fine-BERT flow', 'BERT-FT UDA', 'BERT FEQA', 'BERT-fr', 'BERT-based BounTi model', 'GTS +BERT', 'JET t +BERT', 'JET o +BERT', 'BERT + ATD', 'BERT format', 'BERT Latin ( lat )', 'BERT base + PATS', 'Phrase-BERT model', 'CA-SEP-BERT @ N', 'CA-SEP-BERT @ C', 'Mental-BERT based model', 'BERT – 82.4', 'BERT-Boot ( BERT-base )', 'rep BERT i', 'FLOPs UDA ( BERT )', 'bert-cda', 'BERT-EE', 'BERTbase ( BERT-EE )', 'BERT-EE model', 'IRE BERT', 'WWM-BERT-Large 335M', 'BERT Delex', 'BERT Lex ’', 'BERT TS ’', 'BERT+DK', 'BERT+RK', 'BERT+DK+RK', 'BERT 6037', 'BERT states', 'BERT/NumBERT', 'BERT 0', 'BERT + TA', 'BERT GLU+LW )', 'BERT-LC [ WM ]', 'BERT-MASK ( WP )', 'BERT-MASK ( W )', 'BERT oid', 'BERT-Matching ( BERT-M )', 'BERT ClvC', 'BERT MHSA', 'BERT-like encoder ( F-TRM )', 'BERT-large detection model', 'BERT-based detection model', 'BERT-based KBQA method', 'BERT-based KBQA', 'BERT+T', 'TOD-BERT 2', 'transformer encoder BERT 18', 'BERT-produced', 'BERT produces', 'BERT ( MAPO )', 'PT-M 2 BERT', 'BERT 719', 'BERT 352', 'BERT+NoMT', 'BERT-Base -0.6', 'BERT-Large -0.4', 'BERT-base MobileBERT', 'M-JAVE ( BERT ) models', 'BERT-wmm-ext', 'BERT-Sum', 'BERT-SUM', 'BERT + MemNet', 'bem-BERT', 'GEMNET ( BERT )', 'BERT-based why-QA model', 'LexRank BERT', 'BERT+F', 'RATSQL BERT', 'BERT LARGE + ST', 'bert-electra', 'PMEI BERT', 'BERT OrgAug', 'MATCHSUM ( BERT-base )', 'VGAE + BERT', 'BERT-VFT', 'lattice-BERT', 'Lattice-BERT )', 'BERT fea', 'BERT-based Pairwise Model', 'FCA-based BERT', 'BERT- ( P ) HAE', 'BERT model ( r1 )', 'BERT Delex ’', 'BERT Delex ’ models', 'BERT ELECTRA 4', 'BERT BERT-FT WMS', 'BERT-Base 109.5M', 'KAM-BERT-Large 350.4M', '\ue001d\ue002KAM-BERT', '\ue001b\ue002KAM-BERT', 'BERT Matching', 'Match ( BERT )', 'BERT-based Matching', 'BERT embs', 'BERT base 111M', 'BERT-based LaBSE', 'BERT-based EA', 'Mirror-BERT model', 'Mirror-BERT framework', 'BERT base -Mirror', 'BERT-Large + BAM', 'BERT LARGE ++', 'BERT-S++', 'ENC +BERT BASE', 'WCN-BERT', 'BERT/LayoutLM', 'BERT Large based MCQ model', 'BERT based MCQ models', 'BERT Recc', 'BERT + 1', 'BERT + 2', 'BERT + 3', 'HMD-F1 + BERT', 'BERT-R ) 15', 'BERT+BUTD', 'Prop-BERT', 'BERT-KDS *', 'BERT ( 3-way )', 'BERT SP w/', 'VIRT-BERT-Medium', 'BERT MIMA', 'BERT ∗∗', 'BERT-MWA', 'RATSQL + BERT', 'LCF-BERT-CDM *', 'Res-BERT', 'BERT 110M - 340M', 'name-BERT', 'BERT-PAIR ”', 'UniLM-BERT', 'recall BERT base', 'BERT BASE 256', 'Q8-BERT', 'Hindi BERT', 'FiLM-BERT', 'BERT-enhanced parsers', 'BERT ( MelBERT )', 'BERT ♥', 'Inf BERT', '+BERT+SYNDEP', 'ORQA BM25 + BERT', 'BERT_QA Cls', 'visual BERT models', 'Visual BERT', 'BERT Ranking', 'Bert Ranking *', 'BERT based ranking models', 'BERT-based ranking model', 'BERT rank .', 'BERT-based Ranking', 'BERT-based ranking', 'BERT ranking method', 'neural network ( BERT )', 'BERT-based neural models', 'BERT-based Neural Models', 'BERT-based neural model', 'BERT-based neural methods', 'BERT-term MLA )', 'BERT MEAN', 'BERT Mean', 'BERT ( Mean )', 'BERT base -mean', 'BERT boost', 'BERT-REG ) 4', 'BERT-GBT1', 'Mac-BERT-base/large', 'BERT-based Ranker', 'BERT-ranker', 'BERT LARGE WWM model 7', 'BERT-REG WEs', 'BERT-pairNLI-B', 'BERT play', 'BERT-1000', 'BERT-500', '+LN+BERT', 'BERT large level models', 'TFR-BERT ( Ranking )', 'stock BERT models', 'BERT-Rank ( BR ) model', 'BERT-clip 10', 'BERT-PR-large', 'Google BERT 1', 'BERT-based planning model', 'BERT-base ProtoNet', 'LCF-BERT-CDW *', 'LCF-BERT-CDW', 'BERT base †⋄', 'BERT code 15']"
65,111,Method,5.1641,2018,"{'2018': 0.0907, '2019': 1.4211, '2020': 3.7076, '2021': 5.1641, '2022': 5.0252}","['GNN', 'GNNs', 'graph neural networks', 'graph neural network', 'graph neural networks ( GNNs )', 'Graph Neural Networks', 'GNN model', 'graph neural network ( GNN )', 'GNN-based models', 'Graph Neural Networks ( GNNs )', 'Graph Neural Network ( GNN )', 'GNN-based methods', 'GNN models', 'Graph neural networks', 'Graph Neural Network', 'GNN-based EA methods', 'graph neural networks ( GNN )', 'graph-based neural networks', 'GNN ’ s', 'Graph Neural Networks ( GNN )', 'GNN architecture', 'Graph neural networks ( GNNs )', 'graph neural network models', 'graph-based neural network', 'Graph neural networks ( GNN )', 'GNN module', 'graph neural network-based methods', 'graph neural network model', 'Graph neural network ( GNN )', 'GNN methods', 'GNN-based model', 'GNN-based', 'GNN-based method', 'gnn', 'R-GNN', 'SR-GNN', 'graph neural network methods', 'graph neural model', 'GNN GNN', 'GNN-based approach', 'graph-based neural models', 'graph neural', 'graph neural network ( GNN ) model', 'GNN-based approaches', 'GNN )', 'GNN-EK', '+QA-GNN', 'GNN RN', 'Graph neural network', 'graph NNs', 'Graph-NN', 'GNNs model', 'Graph-based Neural Networks', 'neural graph-based model', 'graph neural network-based models', 'graph-based neural network model', 'graph-based neural architectures', 'Graph Neural', 'neural graph-based system', 'graph neural module', 'Graph neural network-based methods', 'GNN based methods', 'Graph Neural Network-based Methods', 'graph-based neural architecture', 'GNN framework', ""GNN ''"", 'GNN architectures', 'graph neural network ( GNN ) techniques', 'graph-structured neural networks', 'graph neural networks ( GNN ’ s )', 'GNN models—a', 'GNN l', 'l -layered GNN', 'GNN # 2', 'graph ) neural networks', 'GNN based model', 'Graph Neural Network-based model', 'Graph Neural Network method', 'GNN Method', 'GNN method', 'Graph-based neural networks', 'graph-based neural network approach', 'graph neural network-based', 'graph neural network based models', 'GNN based models', 'Graph Neural Network-based Models', 'Graph Neural Network ( GNN', 'graph neural network-based approach', 'GNNs models', 'graph based neural network model', 'graph neural network architecture', 'GNN-based )', 'graph neural models', 'Graph neural models', 'graph neural network module', 'GNN-based framework', 'GNN based framework', 'neural network ( GNN )', 'Graph Neural Module', 'graph neural network ( GNN ) models', 'Graph neural network ( GNN ) models', 'graph-based neural networks ( GNN )', 'Graph-based NN ( neural network ) models', 'Graph-structure neural models', 'Graph Neural Network ( GNN ) architecture', 'graph-neural-network approach', 'graph neural network approach', 'GNN approach', 'GNN GNN Dev', 'GNN approaches', 'graph neural network approaches', 'Graph Neural Network approaches', 'GNN ) approaches', 'Graph Neural Networks ( GNN ) model', 'graph-based neural network models', 'GNN-based techniques', 'Graph Neural Network-based ( GNN-based ) methods', 'GNN—', 'GNNs )', 'graph neural network architectures', 'GNN-based modeling', 'neural graph-approach', 'graph neural-based approach', 'GNNs ( Graph Neural Networks )', 'graph-based neural', 'neural graph-based approach', 'neural network Graph neural networks ( GNNs )', 'Graph Neural Network modules', 'graph based neural network method', 'graph-based neural approaches', 'neural graph-based models', 'neural networks ( GNNs )', 'graph-based neural network approaches', 'graph neural network algorithm', 'GNN based architecture', 'graph-based neural model', 'graph neural network ( GNN ) approaches', 'Graph Neural Network ( GNN ) approaches', 'Graph Neural Approach', 'GNN s', 'graph neural networks ( GNN ) -based models', 'Graph Neural Networks )', 'GNN-based systems']"
66,158,Method,5.0863,2017,"{'2017': 0.2554, '2018': 3.2047, '2019': 5.0863, '2020': 4.1469, '2021': 2.9742, '2022': 1.8204}","['fastText', 'FastText', 'FASTTEXT', 'fasttext', 'Fasttext', 'FastText model', 'fastText model', 'fastText models', 'fastText classifier', 'fastText method', 'FastText models', 'fastText 4', 'Fasttext model', 'fasttext model', 'FASTTEXT models', 'FastText )', 'fastText classification model', 'fastText 8', 'fastText 5', 'FastText 5', 'fasttext classifier', 'FastText classifier', 'FASTTEXT model', 'FastText algorithm', 'fasttext models', 'FastText 6', 'fastText 9', 'fastText 14', '+fastText', 'FASTTEXT 5', 'fasttext 5', 'FastText-classifier', 'Fasttext classifier', 'fasttext algorithm', 'fastText algorithm', 'fastText *', 'fastText )', 'FASTTEXT packages', 'fastText classifiers', 'fastText-based architecture', 'Fast-text', 'fast-text', 'fastText-small', 'FastText-based method', '=fastText', 'fasttext fasttext', 'fastText package', 'fastText module', 'fastText classification models', 'fastText 6', 'as FastText', 'Fasttext 2', 'fastText 2', 'FastText 2', 'Fasttext 2 classifiers', 'Fasttext 4', 'FastText 4', 'FASTTEXT model 4', 'FastText 8', 'fastText model 7', '+FastText', ""fastText '' 3"", 'fastText 3', 'fastText 1', 'fastext', 'Fastext']"
67,213,Method,5.0636,2017,"{'2017': -0.2441, '2018': 0.0141, '2019': 0.8373, '2020': 1.4763, '2021': 3.7121, '2022': 5.0636}","['cross-attention', 'cross attention', 'cross-attention mechanism', 'cross attention mechanism', 'cross-attention model', 'Cross Attention', 'cross attentions', 'Cross-attention', 'Cross-Attention', 'cross attention model', 'cross-attentions', 'cross-attention mechanisms', 'cross-attention models', 'cross attention module', 'cross-attention network', 'cross attention ( CA )', 'Cross attention', 'cross attention mechanisms', 'Cross-Attention ( CA )', 'Cross Attention ( CA )', 'Cross Attention Layer ( CA )', 'cross-attention based QA model', 'cross-attention QA models', 'cross-attention QA model', 'cross-attention QA classification model', 'cross- attention', 'cross-attention module', 'Cross-attention models', 'cross-attention networks', 'cross-attention methods', 'cross-attention ( c )', 'Cross-Attn + +', 'cross- 258 attention', 'cross-attentional architectures', 'Cross Attention ( CA ) models', 'Cross Attention models ( CA )', 'crossKG attention', 'cross-attn', 'CROSS-ATT', 'Cross-attention mechanism', 'Cross Attentions', 'Cross-attentions', 'cross ) attentions', 'Cross-Attention model', 'Cross-Attention Model', 'cross attention models', 'Cross Attention models', 'cross-attention classification model', 'cross-attention classifier', 'cross-structure attentions', 'cross-attention-based', '-Cross attention', 'cross-attention method', 'IB cross attention']"
68,151,Method,5.0384,2002,"{'2002': -0.116, '2003': 0.3119, '2004': 1.5411, '2005': 1.648, '2006': 2.1289, '2007': 4.525, '2008': 3.4689, '2009': 4.4653, '2010': 2.6334, '2011': 4.3892, '2012': 3.2198, '2013': 5.0384, '2014': 4.0713, '2015': 4.3848, '2016': 2.9525, '2017': 1.3694, '2018': 0.9931, '2019': 0.1763, '2020': 0.1091, '2021': 0.0952, '2022': -0.1081}","['log-linear model', 'log-linear models', 'log-linear', 'log-linear framework', 'log linear model', 'Log-linear models', 'log-linear classifier', 'log-linear approach', 'Log-linear', 'Log-linear model', 'log-linear classifiers', 'Log-linear Models', 'Log-Linear Model', 'Log-linear Model', 'log linear models', 'Log-Linear', 'LOG-LINEAR', 'log- ) linear model', 'log-linear ) models', 'log–linear models', 'Log-Linear Models', 'log linear framework', 'log-linear modeling', 'log-linear classification model', 'log-linear model ( 3 )', 'M-to-1 log-linear model', 'log_linear', 'log–linear model', 'log ) linear model', 'Log-linear based model', 'log-linear algorithm', 'Log-Linear Framework', 'Log-linear Framework', 'structured log-linear models', 'log-linear model framework', 'log-linear system', 'LOG-LINEAR *', 'Log-linear model architectures', 'log-linear method', 'log-linear models—of', 'log-linear techniques', 'Log-Linear Modeling', 'log-linear modelling', 'log-linear modeling techniques', 'log-linear models—', 'log-linear classification framework', 'Log-linear based', 'log-linear approaches', 'log-liner models', 'Log-linear model 1']"
69,128,Method,4.9988,2014,"{'2014': 1.6668, '2015': 4.7394, '2016': 4.9988, '2017': 4.8577, '2018': 3.9203, '2019': 2.9147, '2020': 0.7754, '2021': 0.9952, '2022': 0.4682}","['skip-gram', 'skip-gram model', 'Skip-gram', 'Skip-gram model', 'Skip-Gram', 'skip-grams', 'Skip-Gram model', 'skip-gram models', 'SKIP-GRAM', 'skip n-grams', 'Skip-gram models', 'skip-gram algorithm', 'skip-gram method', 'skip-gram approach', 'Skip Gram model', 'Skip Gram', 'skip n-gram model', 'skip-gram architecture', 'skip-n-grams', 'skip gram', 'structured skip-gram', 'Skip-Gram models', 'Skip-gram method', 'Skip-gram algorithm', 'skip-n-gram', 'k-skip-n-grams', 'SKIP-GRAM model', 'structured skip-gram approach', 'skip gram model', 'Skip-Gram Model', 'Skip-grams', 'skip grams', 'Skip-Gram method', 'Structured Skip-gram', 'structured skip-gram models', 'Skip-gram architecture', 'Skip-gram based models', 'Structured Skip-gram model', 'skip-gram )', 'skip n-gram models', 'structured skip n-gram', 'structured skip n-gram approach', 'skip-n-grams ( n )', 'k-skip-n-gram', 'skipn-gram model', 'Skip-gram Model', 'Skip -gram', 'SKip-gram', 'skip-gram based model', 'skip-gram-based model', 'SKIP-GRAM models', 'skip-gram technique', 'Skip-Grams', 'Skip-gram-based method', 'skip-gram techniques', 'skip-gram system', 'skip-gram framework', 'Skip-gram framework', 'skip-gram based methods', 'structured skip-gram method', 'skip-gram-based', 'skip-gram architectures', 'Skip-gram Model ( Skip-gram )', 'structured skip-gram model', 'Skip-Gram )', 'skip-grams algorithm', 'skip-gram-based architectures', 'skip-gram based architecture', 'Skip-gram algorithm 2', 'Skip-gram 2', 'skip-gram 2', 'skip-gram 6', 'skip n-grams models', 'Skip n-gram', 'skip-n-gram model', 'Skip-n-gram model', 'Structured skip-n-gram', 'structured skip n gram', 'Structured Skip-n-gram model', 'n-gram skipping models', 'skip 3-grams', 'skip 5-grams']"
70,87,Method,4.9963,2020,"{'2020': 1.2247, '2021': 3.8412, '2022': 4.9963}","['XLM-R', 'XLMR', 'XLM-RoBERTa', 'XLM-R.', 'XLM-R model', 'XLM-R base', 'XLM-Roberta', 'TOD-XLMR', 'XLM-R models', 'XLM-R-large', 'XLM-ROBERTA', 'XLM-R-base', 'XLM-R large', 'XLMR-base', 'XLM-R BASE', 'XLM-R Base', 'XLMR-large', 'I-XLM-R', 'XLMR-Large', 'XLMR model', 'XLM-R Large', 'XLM-R-L', 'PCT-XLM-R base', 'PCT-XLM-R', 'xlmr-10', 'xlm-roberta-base', 'XLM-RoBERTa ( XLM-R )', 'XLM-RoBERTa-large', 'XLM-RoBERTa-base', 'XLM-RoBERTa model', 'XLM-R base model', 'SP-XLMR', 'XLMR base', 'xlm-roberta-large', 'XLMR models', 'XLM-RoBERTa ( base )', 'XLM-R large models', 'XLM-R large model', 'XLM-Roberta-Base', 'XLM-R *', 'XLM_RoBERTa model', 'XLM-R Base model', 'XLMR Method', 'xlmr-1', 'XLM-R+Concat', 'XLM-R L', 'XLMR+SAP', 'XLM-R mean', 'XLM-R B', 'XLMR BASE', 'XLM-RoBERTa ( XLMR )', 'XLM ( -R )', 'XLM-R )', 'XLM-RoBERTa large', 'XLM-RoBERTa models', 'XLM-RoBERTa-base model', 'XLMR-L', 'XLM-R 7', '+XLM-R', 'XLM-RoBERTa-NDPR', 'XCOPA XLM-R', 'XLM-R 2', 'xlmr-base', 'XLM-R – Base', 'XLMR-Base', 'XLM-R (', 'XLM-R-Large', 'XLM-R LARGE', 'XLM-R ( Base )', 'XLM-R-large model', 'XLM-R classification model', 'XLMR-Large 1', 'XLM-R 0', 'SP-XLMR models', 'XLM-on-RoBERTa', 'XLM-onRoBERTa', 'XLM-T/XLM-R', 'XLM-R / mono', 'XLM-R+T', 'XLMR-ft', 'FT_XLM_RoBERTa', 'XLM-R+TREP', 'XLM-R+D', 'XLMR-Large 14', 'XLM-R+R3F/R4F', 'xlm-Roberta-base ’ model', 'XLM-R+R3F', 'XLM-R+R4F', 'XLM-R 11', 'XLMR 3', 'XLM-R 3', 'XLM-RoBERTa 3', 'XLM-RoBERTa 1024', 'XLM-R LaBSE', 'XLM-R ( I-XLMR )', 'I-XLM-R models', 'I-XLM-R model', 'I-XLM-R based model', 'XLM-RoBERTa HS', 'XLM-R +', 'XLM-R BASE model 8', 'XLM-R ( 470M )', 'XLM-RoBERTabase', 'xlm-robertabase', 'XLM-R-Enc-N', 'XLMR-Large 10', 'XLMR B', 'xlm-robertalarge', 'XLM-Robertalarge', 'MARC XLM-R', 'XLM-R\ue021L', 'XLM-R 6', 'XLM-R-based FPI model', 'XLM-R ( es )', 'XLMR-Base 2', 'XLM-R Base 2', 'XLMR-Large 2', 'XLM-RoBERTa base sized model', 'XLM-RoBERTA', 'XLM–R', 'XLM_RoBERTa', 'xlm-Roberta', 'XLM RoBERTa', 'XLM-r', 'XLM-roBERTa', 'xlm-R-base', 'XLM-RoBERTa base', 'XLMR Base', 'XLM RoBERTa ( XLM-R )', 'XLM-RoBERTa )', 'XLM-R - -', 'XLM-Roberta model', 'XLM RoBERTa model', 'XLM-R ) model', 'XLMR Model', 'XLM-R-based', 'XLM-Roberta-Large', 'XLM-RoBERTa-Large', 'XLMR LARGE', 'XLMR Large', 'XLM-Roberta-large', 'xlm-R-large', 'XLM-R ( Large', 'XLMR large', 'xlm-roberta-base models', 'XLM-RoBERTa Base model', 'XLM-R-base model', 'XLMR base model', 'XLM-RoBERTa base model', 'XLM-R BASE model', 'XLM-R ( base )', 'XLM-R base )', 'XLM-RoBERTa ( Base )', 'XLM-RoBERTa-base ( XLM-R )', '/XLM-R', 'XLM-R-large )', 'XLM-R ( Large )', 'XLM-R ( xlm-roberta-large )', 'cross-lingual language model – XLM roberta-base', 'RoBERTa XLM models', 'roberta-xlm', 'XLMR large models', 'XLMR-large model', 'XLM-Roberta large model', 'XLMR architecture', 'XLM-R architecture', 'SOTA XLM-Roberta model', 'XLM-R language model', 'XLM-RoBERTa language model', 'XLM-R LM', 'XLM-R large model based approaches', 'base XLM-RoBERTa model', 'XLM-R ( xlm-roberta-base )', 'XLM-RoBERTa ( XLM-R ) model', 'XLM-Roberta model ( XLM-R )', 'XLM-RoBERTa ( XLMR', 'XLM-RoBERTa ( XLM-R', 'XLM-R-base/large', 'xlm-roberta-base-model models', 'XLM-R-base XLM-R-large', 'Cross-lingual Model-RoBERTA ( XLM-R )', 'XLM-RoBERTa large ( XLMR ) model', 'XLM-R based model', 'XLM-Roberta-large ( XLM-R )', 'MP-XLM-R †', 'PL-XLM-R', 'XLM-R ftall', 'XLM-R fused model', 'SimCSE-XLM-R', 'XLM-R 13', 'EASE-XLM-R', 'XLM-R B XLM-R L', 'XLM-R ft-all', 'XLM-RoBERTa base model ( Con', 'ply XLMR', 'XLM-R ( M2 )', 'XLM-RoBERTa based CFD models', 'PrLM XLM-R', 'XLM_RoBERTa_base 4 model', 'XLM-R ) * 1M', 'SF-XLM-R', 'XLMR-Large 0', 'XLMR-Large 5', 'XLMR 5', 'XLM_RoBERTa 5 model', 'SP-XLMR XLM-R', 'XLM-RoBERTa-base ( XLM-Rbase ) language model']"
71,260,Method,4.7436,2019,"{'2019': 1.6479, '2020': 3.4642, '2021': 3.071, '2022': 4.7436}","['GPT', 'GPT model', 'GPT models', 'GPT-based models', 'GPT ( ours )', 'GPT base', 'GPT-based', 'GPT )', 'GPT-based model', 'gpt', 'Gpt', 'GPT ?', 'GPT-', 'GPT- *', 'GPT *', 'GPT large', 'GPT-large', 'gpt- * models', 'GPT Large model', 'GPT-based approaches', 'GPT-based method', 'GPT-based architecture', 'GPT architecture']"
72,153,Method,4.721,2018,"{'2018': 1.1032, '2019': 3.8399, '2020': 4.3137, '2021': 4.721, '2022': 4.1066}","['multi-head attention', 'MHA', 'multi-head attention mechanism', 'Multi-Head Attention', 'Multi-head attention', 'multi-headed attention', 'Multi-head Attention', 'multi-head attentions', 'multi-head attention ( MHA )', 'multi-head attention network', 'w-MHA', 'multi-head attention module', 'multi-head attention model', 'b-MHA', 'Multi-Head Attention ( MHA )', 'multi-head attention architecture', 'multi-head attention cell', 'multi-headed attention mechanism', 'multi-head attention mechanisms', 'multi-head attention ( MHA ) mechanism', 'Multi-Head attention mechanism', 'multi-head attention networks', '–MHA', 'attention multi-head', 'Multi-head Co-Attention', 'Multi-Head attention', 'multi-headed attentions', 'Multi-Head Attention ( MHA ) mechanism', 'z MHA', 'multi-head attention ( mh )', 'MHA-D', 'MHA-O', 'Multi-Head Attn', 'Multi-Head Attention mechanism', 'Multi-head attention mechanism', 'MHA mechanism', 'Multi-head Attention Module', 'multi-head attention modules', 'Multi-head attention ( MHA )', 'Multi-head Attention ( MHA )', 'multi head attention ( MHA )', 'Multi-head Attention Model', 'Multi-headed Attention', 'Multi-head Attentions', 'multi-headed attention mechanisms ( MHA )', 'Multi-head attention ( MHA ) mechanism', 'multi-head attention module ( MHA )', 'multi-head attention in', 'MHA attention', 'multi-head attention attention', 'multi-head attention framework', 'multi-headed attention architectures', 'multi-headed attention module', 'Multi-Head Attention MHA', 'Multi-head Attention Multi-head attention', 'multi-head attention ( MHA', 'MHA MHA', 'multi-head attention models', 'multi-head attention based model', 'Multi-Head Attention ( MHA ) module', 'multi-headed attention network', '–multi-head attention', 'Multi-Head Attention method', 'multi-head attention method', 'MHA ( . )', 'multi-heads attention', 'multi-head attention networks (', 'Multi-head attention ( ATTN )', 'multi-head attention ( ATTN )', 'multi-head attention architectures', 'Multi-head attention mechanisms', 'multi-heads attentions', 'multi-headed attention architecture', 'multi-head attention approach', 'multi-head attention mechanism ( MHA )', '–MHA ⋄†', 'Multi-head attention ( A )', 'multi-head attetnion', 'Multi-head CoAttention mechanism', 'multi-head dot attention mechanism', 'SUM multi-head attention mechanism', 'multi-head attention ( b )', 'multi-head co-attention', 'Multi-head Co-Attention mechanism', 'multi-head co-attentions', 'multi-head attention sub-layer ( MHA )', 'muli-head attention', 'GED multi-head attention', 'MHA-QK', 'MHA-V.', 'MHA ( X )', 'Multi-Head Attention Mechanism ( MHAM )', 'multi-head attention ( MH A )', 'MHA loss method', 'mha_4', 'mha_8', 'MHA-R2B', 'MHA-B2R']"
73,122,Dataset,4.6995,2008,"{'2008': -0.1813, '2009': -0.0425, '2010': 1.3028, '2011': 1.6044, '2012': 1.8443, '2013': 3.0393, '2014': 2.8102, '2015': 2.3537, '2016': 2.9601, '2017': 2.7331, '2018': 3.2748, '2019': 3.4358, '2020': 4.2991, '2021': 4.6995, '2022': 4.2028}","['Amazon Mechanical Turk', 'AMT', 'MTurk', 'Amazon Mechanical Turk ( AMT )', 'Amazon ’ s Mechanical Turk', 'Amazon Mechanical Turk ( MTurk )', 'Amazon MTurk', 'Mturk', 'Amazon ’ s Mechanical Turk ( MTurk )', 'Amazon Mechanical Turk 1', 'Amazon ’ s Mechanical Turk ( AMT )', 'Amazon Mechanical Turk 4', 'Amazon Mechanical Turk 7', 'mturk', 'Amazon Mechanical Turkers', 'Amazon Mechanical Turk 3', 'Amazon Mechanical Turk 2', 'amazon mechanical turk', 'AMT dataset', 'MTurk dataset', 'Mechanical Turk ( MTurk )', 'Amazon Mechanical Turk 5', 'Amazon mechanical turk', 'Amazon Mechanical Turk ( Mturk )', 'Amazon Mechanical Turks', 'MTurk data', 'Amazon Mechanical Turk 1 ( AMT )', 'Amazon ’ s Mechanical Turk 3', 'AMT + GCS', 'MTurk framework', 'Amazons Mechanical Turk', 'Amazon Mechanical Turk ( AMT ) 1', 'Amazon Mechanical Turk 8', 'Amazon ’ s Mechanical Turk system', 'Amazon Mechanical Turk 12', 'MTurk.com', 'Amazon Mechanical Turk ( MTurk ) 3', 'Amazon Mechanical Turk 6', 'Amazon Mechanical Turk 2 ( AMT )', 'MTurk 200', 'MTurk 800', 'MTurk 4', 'Mturk users', 'Amazon ’ s Mechanical Turk 1', 'Amazon Mechnical Turk', 'MTurk API', 'Amazon Mechanical Turker', 'Amazon Mechanical Turk 10 ( AMT )', 'AMT data', 'AMT corpus', 'MTurk MTurk', 'Amazon Mechanical Truk', 'Amazon Mechanical Turk 7 ( AMT )', 'MTurk Ids', 'AMT 1', 'Amazon Mechanical Turk ( MTurk ) 1', 'AMT 8', 'Amazon Mechanical Turk 15', 'Amazon Mturk ”', 'Amazon Mechanical Turkers 5', 'mturk.com', 'Amazon Mechanical Turk 9', 'MTurk 9', 'Amazon Mechanical Turk ( AMT ) 9', 'Amazon Mechanical Turk 16', 'MTurk MTurk 3', 'Amazon Mechanical Turk 3 ( AMT )', 'Amazon Mechanical Turk 6 ( MTurk )', 'Amazon Mechanical Turk 6 ( AMT )', 'MTurk 2', 'Amazon Mechanical Turk 2 ( Mturk )', 'Amazon Mechanical Turk ( AMT ) 2', 'Amazon Mechanical Turk ( MTurk ) 2', 'Amazon MTurk 2', 'Amazon Mechanical Turk ( AMT ) 4', 'Amazon Mechanical Turk 4 ( MTurk )', 'US-based AMT', 'Amazon ’ s Mechanical Turk ( MTurk ) 1', 'Amazon ’ s Mechanical Turk ( AMT ) 6', 'Amazon ’ s Mechanical Turk 6', 'MTurk blog', 'Amazon.com ’ s Mechanical Turk', 'AMT API', 'Amazon Mechanical Turk ( AMT ) 5', 'Amazon Mechanical Turk 13', 'MTurk 10', 'Amazon Mechanical turk', 'mTurk', 'AMT ( Amazon Mechanical Turk )', 'AMT * ( Amazon Mechanical Turk )', 'Amazon Mturk', 'Mturk-based', 'Amazon Mechanical Turk framework', 'MTurk system', 'Amazon Mechanical Turk system', 'Mechanical Turk ( AMT )', 'AMT –', 'MTurk )', 'Amazon Mechanical Turks ( AMT )', 'corpus MTurk', 'MTurk-MTurk', 'MTurk-gold', ""Amazon 's Mechanical Turk"", 'MTurk systems', 'MTurk MTurk MTurk', 'Amazon ’ s Mechanical Turk 5', 'Amazon Mechanic Turk']"
74,70,Method,4.6622,2019,"{'2019': 0.0308, '2020': 2.5108, '2021': 4.6622, '2022': 3.9582}","['mBERT', 'M-BERT', 'MBERT', 'mBERT model', 'm-BERT', 'multilingual BERT ( mBERT )', 'mBERT models', 'BERT ( mBERT )', 'bert-base-multilingual-cased', 'mBERT-TLADV', 'Multilingual BERT ( mBERT )', 'multilingual BERT', 'mBERT-ft', 'mBERT-RS-RP', 'Multilingual BERT ( M-BERT )', 'mBERT+MGL', 'mBERT )', 'MOVER-2 + CLP ( M-BERT )', 'mBERT+MNRE', 'mBert', 'Multilingual-BERT', 'M-BERT model', 'multilingual BERT ( M-BERT )', 'mBERT-ADV', 'mBERT_linear_unsup', 'mBERT ( X-X )', 'mBERT *', 'MBERT model', 'BERT ( M-BERT )', 'M-BERT models', 'mBERT BASE', 'mBERT+Filters', 'mBERT ( Multi )', 'MOVER-2 + UMD ( M-BERT )', 'MBERT-30', 'multilingual-BERT', 'Multilingual BERT', 'mBERT-based methods', 'mBERT-based', 'MBERT-base', 'mBERT Method', 'mBERT vocabulary', 'MBERT+Akk', 'mBERT ( reproduce', 'w/ M-BERT', 'mBERT_full_sup', 'mBERT_linear_sup', 'mBERT+Att', 'd-mBERT', 'mBERT encoder', 'BERT-Base Multilingual Cased ” model', 'MPoSM + mBERT', 'MBERT0', 'M-BERT )', 'multilingual BERT ( MBERT )', 'multilingual BERT ( mBERT ) model', 'BERT-Base Multilingual Cased model', 'mBERT-based models', 'cased multilingual BERT ( mBERT )', 'multilingual BERT ( base )', 'BERT ( mBERT', 'BERT mBERT', 'Multilingual BERT ( mBERT', 'Multilingual BERT ( MBERT', 'mBERT-base', 'mBERT method', 'multilingual BERT 1', 'mBERT 1', 'mBERT-based WEs', 'ES-MBERT', 'ZH-MBERT', 'Transformer ( mBERT )', 'modified mBERT', 'MBERT+Akk+Eng', 'mBERT Ctxt', 'multi-lingual BERT ( M-BERT )', 'mBERT ( Single )', 'M-BERT+Manifold', 'M-BERT+Manifold Mixup model', 'mBERT-RSDA', 'mBERT 177m / 2h', 'mBERT-TLADVs', 'EASE-mBERT', 'LMMS mBERT', 'mBERT pool', 'Divergent mBERT model', 'Divergent mBERT', 'mBERT 3', 'mBERT+Att model', 'mBERT2mBERT', 'mBERT 0', 'BERT multilingual model 8', 'BERT multilingual cased vocab', 'mBERT ( multilingual )', 'bag-of-words mBERT', 'mBERT2TA model', 'MLQA development set ( mBERT )', 'mBERT 6', 'BERT style multilingual model', 'M-BERT X', 'mBERT - X', 'BERT-base-multilingual-cased 3', 'bert-base-multilingual-cased 3', 'mBERT ( DiGls )', 'ModAvg mBERT', 'ESMBERT/ZH-MBERT', 'Tatoeba mBERT', 'multilingual BERT ( 178M )', 'F1 mBERT base', 'distil-mBERT ( d-mBERT )', 'mBERT ( 4-layers )', 'mBERT 4', 'multilingual BERT ( mBERT ) 4 model', 'multilingual-bert', 'M-Bert', 'mBERT -', 'Multilingual-BERT )', 'm-BERT -', 'mBERT ]', 'mBERT-', 'm-BERT model', 'multilingual BERT model', 'multilingual BERT ( m-BERT )', 'Multilingual-BERT ( M-BERT )', 'multilingual-BERT ( mBERT )', 'multilingual bert ( mBERT )', 'multilingual BERT ( M-BERT ) model', 'Multilingual BERT ( mBERT ) model', 'multilingual-bert-cased model', 'BERT-base-multilingual-cased model', 'bert-base-multilingual-cased model', 'M-BERT-based models', 'mBERT-based ) models', 'mBERT ( base cased )', 'M-BERT base architecture', 'BERT-multilingual-base', 'cased mBERT model', 'mBERT-base model', 'mBERT base model', 'mBERT Base model', 'mBERT classifier', 'mBERT-based classifier', 'multilingual BERT model ( mBERT )', 'BERT based multilingual model', 'mBERT ( bert-base-multilingual-cased )', 'BERT/MBERT', 'BERT/Multilingual-BERT', 'multilingual BERT ( mBERT', 'multilingual-BERT-cased model ( mBERT )', 'm-BERT classification model', 'BERT-base ( mBERT )', 'BERT base multilingual', 'BERT-Base-Multilingual', 'bert-base-multilingual', 'Transformer encoder ( mBERT )', 'BERT-multilingual', 'cased multilingual BERT ( base )', 'BERT base-multilingual-cased ( mBERT )', 'cased multilingual BERT ( mBERT ) base model', 'cased mBERT', 'BERT-base multilingual model', 'BERT multilingual ( MBERT )', 'mBERT-based model', 'cased BERT-base multilingual model', 'mBERT base', 'MBERT ( cased )', 'bert-multilingual-cased model', 'mBERT metrics', 'mBERT-based method', 'mBERT ( and )', 'multilingual BERT ( m-BERT-base )', 'bert-multilingual-cased', 'mBERT Transformer', 'BERT-to-mBERT', 'BERT ( mBERT ) 2', 'mBERT TASK-TUNED', 'mBERT ’ s corpus', 'bert-base-multilingual-cased ’', 'mBERT ( B )', 'M-BERT ( B ) model', 'multilingual BERT 1 ( mBERT )', 'mBERT mean', 'mBERT mean methods', 'f-mBERT', 'f-mBERT ]', 'metric MOVER-2 + CLP ( M-BERT )', 'M-BERT NO-TRAIN ( M NO )', 'BERT Multilingual Beam Search', 'mBERT dual encoder', 'modified mBERT technique', 'mBERT towers', 'BERT-multilingual vocabulary', 'mBERT vocabulary based models', 'mBERT+LRL', 'mBERT+Mix', 'mBERT+Target-Only', 'mBERT en-es-it', 'MOVER-1 + CLP ( M-BERT )', 'MOVER-1 + UMD ( M-BERT )', 'MOVER + M-BERT', 'bert-base-multilingualcased ’ model', 'M-BERT based Mover-2', 'mBERT-based transfer', 'RIKD ( mBERT )', 'mBERT-based MRC model', 'mBERT ( centered )', 'mBERT ( projection )', 'gual BERT ( mBERT )', 'mBERT+S', 'mBERT+S 2 DM_SP MRC model', 'multi-lingual BERT ( mBERT )', 'mBERT training corpus', 'BERT-multilingualcased', 'mBERT NLU model', 'mBERT NLU models', 'GCM mBERT', 'mBERT FG En-Hi', 'mBERT Bangor En-Es', 'mBERT SAIL En-Hi', 'mBERT QA En-Hi', 'En-Hi mBERT model', 'mBERT-single', 'MBERT-Single', 'mBERT-single models', 'UDPipe 2.0 + mBERT', 'UDPipe + mBERT', 'BR ( f-mBERT', 'MBERT ( bert-base-multilinualcased )', 'mBERT mBERT + PCR', 'mBERT_full_unsup', 'mBERT +', 'M-BERT +', 'RAT-SQL + M-BERT', 'mBERT_vecmap_id', 'mBERT_vecmap_sup', 'mBERT-based Simalign 9', 'multi-task mBERT', 'extended mBERT models ( eBERT )', 'mBERT/NoSyn', 'mBERT ( Full )', 'multilingual BERT ( mBERT ) 2', 'mBERT 2', 'cased multilingual BERT model 2', 'mBERT/FullSyn', 'multilingualbert-cased ( mBERT )', 'EASE-mBERT base', 'M-BERT ( MTQA French test set )', 'lingual BERT ( M-BERT )', 'mBERT 5', 'M-BERT 5', 'M-BERT ( bert-base-multilingual-cased ) 5', 'CMTA multilingual BERT framework', 'multilingual BERT 17', 'M-BERT O', 'BERT multilingual tokeniser', 'distil-M-BERT', 'XQUAD ( mBERT )', 'XQUAD MBERT', 'SimCSE-mBERT', 'ML+mBERT', 'MBERT 11', 'mBERT 11', 'BERT ( Multilingual-Base version )', 'M-BERT - 102', 'SF-mBERT', 'mBERT-Only )', 'mBERT 24', 'mBERT-review 6', 'mBERT-review', 'bert-base-multilingual-uncased model', 'BERT-base multilingual uncased model', 'bert-multilingual-uncased', 'mBERT retriever', 'm-BERT / mono', 'm-BERT / cross', 'multilingual BERT ( mBERT ) 8', 'mBERT—it', 'mBERT ( De', 'mBERT-none', 'mBERT-none model', 'HI+EN M-BERT', 'M-BERT zeroshot', 'CS -mBERT BASE', 'mBERT based perturbation method', 'M-BERT 9', 'Flair/M-BERT', 'Multilingual BERT ( MultiBERT )', 'mBERT Average', 'mBERT avg', 'CTP ( MBERT )', 'Multimodal-BERT ( M-BERT )', 'mBERT CLS']"
75,235,Method,4.6618,2018,"{'2018': -0.1338, '2019': 0.4186, '2020': 2.1818, '2021': 3.573, '2022': 4.6618}","['TLM', 'transformer-based language models', 'transformer language models', 'Transformer-based language models', 'transformer language model', 'Transformer language models', 'Transformer language model', 'transformer-based language model', 'transformer-based LMs', 'Transformer-based language model', 'Transformer LMs', 'Transformer LM', 'C-TLM', 'transformer LMs', 'S-TLM', 'MS-TLM', 'transformer LM', 'Transformer-based LMs', 'transformer based language models', 'Transformer-based LM', 'transformer-based LM', 'Transformer-based language models ( LMs )', 'Transformer Language Models', 'language Transformer', 'C-TLM (', 'MS-TLM models', 'tLM', 'Transformer Language Model', 'TLM model', 'transformer LMs ( TLM )', '-TLM', 'large MS-TLM models', 'Transformer-based Language Models', 'transformer based language model', 'TLM-based methods', 'transformer-based language models ( LMs )', 'transformer-based language', 'language transformers', 'C-TLM ( L τ )', 'Transformer + LM', 'language transfer', 'Language transfer', 'transfer languages', 'Transformer-XL language models', 'Transformer-XL language model', 'Transformer-XL LM', 'Transformer-XL LM1B', 'QA transformer-based LMs', 'tree Transformer LMs', 'transformed-based language models', 'MS-TLM model', 'TLM+I', 'Transformer based language models', 'Transformer based Language Models', 'transformer-LM', 'Transformer-LM', 'Transformer Language model', 'transformer based LMs', 'Transformer based Language Model', 'transformer based LM', 'Transformer-based LM approaches', 'language transformer', 'Transformer Language Model ( LM )', 'TLM-based models', 'Transformer based language models ( LMs )', 'Transformer-based Language Models ( LMs )', 'Transformer LM architecture', 'transformer-based Language Model ( LM )', 'transformer based language model ( LM )', 'transformer-based language model ( LM )', 'transformers-based LM', 'language Transformers', 'Transformer-based LM architectures', 'Analysis Transformer based language models', 'Transformer LM )', 'Transformer LM model', 'Transformer Language Models ( TLM )', 'TLM ( -TLM )', 'transformer language', 'structured Transformer language models', 'transformer-based Language Models ( LMs', 'transformer language models ( LMs )', 'Transformer language models ( LM )', 'Transformer language models—', 'transformer-based LM architecture', 'transformer architecture language model', 'language Transformer models', 'Transformer-based large language models', 'transformer_lm_gpt architecture', 'GPT transformer language model', 'C-TLM ( L τ ) model', 'TLM + RS-X', 'transfer language corpus', 'transfer-language corpus', 'transfer-language corpora', 'transfer language', 'language transfer dataset', 'Transfer Languages', 'Transformer or language models', 'Transformer language model 3', 'GPT-2 transformer language model', 'transformer language model ( GPT-2 )', 'Transformer + LM +']"
76,47,Method,4.5819,2017,"{'2017': -0.1056, '2018': 1.0017, '2019': 2.7024, '2020': 4.4678, '2021': 4.5819, '2022': 3.7222}","['GCN', 'GCNs', 'GCN model', 'graph convolution', 'graph convolutional networks', 'graph convolutional network', 'A-GCN', 'Graph Convolutional Network ( GCN )', 'T-GCN', 'C-GCN', 'GCN models', 'graph convolutional network ( GCN )', 'graph convolutional networks ( GCNs )', 'graph convolutional networks ( GCN )', 'Graph Convolutional Networks', 'GCN-based models', 'Graph Convolutional Networks ( GCNs )', 'graph convolutions', 'TPC-GCN', 'S-GCN', 'GCN-based', 'graph convolution network', 'Graph Convolutional Network', 'Graph Convolutional Networks ( GCN )', 'EMC-GCN', 'GCN cat', 'Graph convolutional networks ( GCNs )', 'C-GCN model', 'GCN co', 'GCN-based model', 'AC-GCN', 'OE-GCN', 'ST-GCN', 'f-GCN', 'Graph Convolution Networks', 'Graph Convolution', 'Dep-GCN', 'DC-GCN model', 'Graph Convolution Network', 'R-GCNs', 'GCN-based methods', 'EMC-GCN model', 'I-GCN', 'Q-GCN', 'GCN method', 'graph convolutional', 'GCN-EA', 'graph convolution networks', 'gating GCN', 'D-GCN', 'GCN 2', 'GCN 1', 'A-GCN ( L + G )', 'DC-GCN', 'GCN-sib', 'V-GCN', 'SL-GCN', 'A-GCN models', 'GCN-ED', 'L-GCN', 'Graph convolutional networks', 'GCN architectures', 'GR-GCN', 'GCN-PE', 'graph-convolutional network', 'gcn', 'GCN )', 'GCN-based approaches', 'GCN architecture', 'Graph Convolutional Networks ( GCNs', 'GCN approach', 'GCN ( SE )', 'GCN-JE', 'f-GCN2', 'PC-GCN', 'A-GCN model', 'TP-GCN', 'GCN ”', 'GCN type', 'A-GCN ( L )', '+A-GCN', 'ST-GCN model', 'GCN network', 'graph convolutional layers', 'Graph Convolutions', 'graph convolution layers', 'GCN-JR', 'GCN-JE-r', 'T-GCN model', 'Skip-GCN', 'GCN ” model', 'G-GCN', 'GCN EC', 'GCN-EC', 'GCN1', 'TPC-GCN model', 'Graph convolutional networks ( GCN )', 'Graph convolutional network ( GCN )', 'Graph Convolutional', 'Graph Convolutional Network-based methods', 'GCN methods', 'GCN approaches', 'graph convolutional networks ( GCNs', 'Networks ( GCN )', 'GCN mechanism', 'GCN-based framework', 'graph convolution based model', 'graph convolution technique', 'AVR-GCN', 'gated GCNs', 'graph convolutional networks ( A-GCN )', 'AT-GCN', 'GCN2', 'edge GCNs', 'GCN ( K=1 )', 'GCN ( K=2 )', 'κ -GCN', 'GCN4', 'GCNs 4', 'deep graph convolutional network', 'deep GCNs', 'deep GCN architectures', 'GCN 1 )', 'f-GCN module', 'Graph convolutional network', 'GCN-Based', 'graph-convolutional networks', 'Graph Convolutional networks', 'graph-convolutional networks ( GCNs )', 'GCN Graph convolutional networks (', 'Graph Convolutional Network !', 'GCN-model', 'graph-convolutional approach', 'GCN ( Graph Convolutional Network )', 'graph convolutional network ( GCNs )', 'GCN Method', 'GCN-based system', 'Graph Convolutional Network ( GCN', 'GCN GCNs', 'GCN-models', 'GCN based systems', 'GCN module', 'GCN Module', 'graph convolutional mechanism', 'Graph Convolutional Network ( GCN ) models', 'GCN-based model architecture', 'GCNs architecture', 'Graph Convolutional Networks GCNs', 'GCN algorithm', 'based graph convolutional model ( GCN )', 'graph convolutional structure', 'graph convolutional networks based method', 'graph convolutional layer', 'Convolutional Network ( GCN )', 'convolutional network ( GCN )', 'GCN model System', 'GCN framework', 'Convolutional Networks ( GCN )', 'GCN-based approach', 'graph convolutional network ( GCN ) architectures', 'graph-convolutional-networks/', 'graph convolutional networks based model', 'Our algorithm ( GCN )', 'GCN algorithms', 'graph convolutional network ( GCN ) -based model', 'GCN frameworks', 'GCN networks', 'GCN ( 1L )', 'graph convolutional network ( DC-GCN )', 'GCN ( 2L )', 'GCN based 237 model', 'GR-GCN )', 'f-GCN1', 'GCN/GCN-sib', 'OE-GCN model', 'Graph Gonvolutional Network', 'GCN ( L + G )', 'GCN ( L + G ) models', 'EE-GCN', 'C-GCN *', 'C-GCN /', 'C-GCN models', 'graph convolutional ( C-GCN )', 'path-based graph convolutional network', 'Path-based GCN', 'Gcn5', 'GCNs 5', 'Graph convolution', 'Graph Convolution Layer', 'Graph convolutions', 'G raph convolution networks', 'graph convolution networks model', 'graph convolution network architecture', 'Graph Convolution Network (', 'Graph Convolution Model', 'graph convolution based method', 'graph convolution architecture', 'graph convolution framework', 'graph convolution methods', 'graph-convolution approach', 'C-GCN-CR', 'wMP-GCN', 'FAST ( GCN )', 'dual-Graph convolutional network', 'EMC-GCN framework', 'EMC-GCN architecture', 'gated GCN', 'gated convolutional network ( GCN )', 'Gated Convolutional Network ( GCN )', 'Skip-GCNs', 'GCN X g', 'GCN 3', 'GCN3', 'GCN-based 3', 'DDA-GCN', 'SL-GCN model', 'Q-GCN ( l )', 'I-GCN ( l )', 'I-GCN ( 2 )', 'Q-GCN ( 2 )', 'GCN +', 'I-GCN ( 4 )', 'Q-GCN ( 4 )', '+A-GCN ( L )', 'A-GCN ( G )', 'PC-GCN model', 'PC-GCN /', 'AS-GCN', 'GCN ( A-GCN )', 'LA-GCN', 'k-GCN', 'L layer graph convolutional networks', 'TA- GCN', 'R-GCNs )', 'GCN-ED†', 'AT-GCN )', 'GCN ty pe', 'WR-GCN']"
77,143,Method,4.5786,2014,"{'2014': -0.2005, '2015': -0.1911, '2016': -0.0021, '2017': 0.0798, '2018': 0.856, '2019': 1.3682, '2020': 2.4653, '2021': 2.9147, '2022': 4.5786}","['NLI', 'NLI model', 'NLI models', 'Natural Language Inference ( NLI )', 'NLI dataset', 'NLI-TR', 'NLI data', 'SLR-NLI', 'natural language inference ( NLI )', 'Bias-NLI', 'Natural language inference ( NLI )', 'αNLI', 'α NLI', 'Natural Language Inference', 'natural language inference model', 'δ -NLI', 'FS-NLI', 'NLI corpus', 'NLI classifier', 'NLI systems', 'natural language inference ( NLI ) models', 'NLI system', 'δ-NLI', 'α NLI dataset', 'f NLI', 'NLI Model', 'natural language inference dataset', 'NLI Models', 'natural language inference ( NLI ) model', 'NLI DeBERTa', 'V-NLI', 'natural language inference', 'NLI approaches', 'Natural Language Inference ( NLI ) models', 'natural language inference models', 'NLI )', 'NLI corpora', 'NLI-based system', 'SLR-NLI model', 'δ -NLI dataset', 'NLI-B', 'natural language inference systems', 'NLI-', 'Natural Language Inference ( NLI ) model', 'natural language inference ( NLI', 'NLI data set', 'NLI method', 'NLI approach', 'Natural Language Inference ( NLI ) dataset', 'S-DIORA NLI', 'DIORA NLI', 'NLI2DB systems', 'CTM+FT ( NLI ) model', 'bigram-based NLI classifier', 'BREAK-NLI', 'Non-NLI', 'NLI model M', 'NLI-M', 'natural language inference ( NLI ) model f nli', 'nli', 'NLI Corpus', 'Natural Language Inference data', 'NLI Data', 'NLI-models', 'Natural Language Inference ( NLI ) techniques', 'NLI-based model', 'NLI techniques', 'NLI-based models', 'NLI model Model', 'NLI dev', 'NLI NLI', 'Natural Language Inference ( NLI', 'NLI classifiers', 'NLI architecture', 'natural language inference approach ( NLI )', 'NLI classification model', 'Natural language inference ( NLI ) data', 'NLI framework', 'natural language inference models ( NLI', 'classifier-base NLI models', 'NLI-based metrics', 'NLI-based methods', 'NLI classification approach', 'natural language inference ( NLI ) classifier', 'Natural Language Inference ( NLI ) system', 'NLI-based', 'Natural Language Inference ( NLI ) Approach', 'NLI CAD dataset', 'natural language inference , or NLI )', 'bart-large ( NLI )', 'bart-large-nli', 'tree-based NLI models', 'biased NLI models', 'ford NLI corpus', 'a-NLI', 'WNLI NLI Classification', 'NLI → QA', 'D OC NLI dataset', 'doc-NLI corpora', 'natural language inference 1', 'DOCNLI NLI', 'NLI2DB', 'NLI hsa', 'CTM+FT ( NLI )', 'QA/NLI model', 'GPT NLI', 'δ-NLI dataset', 'δ -NLI data', 'NLI model 11', 'flow ( NLI )', 'Indic NLI', 'NLI/BoolQ', 'α -NLI', 'classification-based α NLI approach', 'CQA NLI', 'GPT2-based NLI models', 'PRPN NLI', 'SOTA NLI model 5', 'NLI model 2']"
78,37,Method,4.5778,2013,"{'2013': -0.0153, '2014': -0.1488, '2015': 0.359, '2016': 0.8358, '2017': 1.6542, '2018': 2.4556, '2019': 3.487, '2020': 3.3047, '2021': 3.1162, '2022': 4.5778}","['MT', 'MT system', 'mT5', 'multi-task model', 'MT model', 'MT systems', 'multi-task', 'MT models', 'multi-task framework', 'multi-task models', 'multitask model', 'MT data', 'MT05', 'MT08', 'multi-tasking', 'Multi-task', 'MT5', 'multitask models', 'MT04', 'MT task', 'multi-task approach', 'Multitask', 'multitask', 'multi-task architecture', 'mt', 'Multi-Task', 'multitask framework', 'multitask approach', 'MULTITASK', 'multitask architecture', 'MT metrics', 'multi-task setting', 'MT tasks', 'CLS+MT', 'multitask setting', 'mT5 model', 'mT5-base', 'MT-I', 'MT-FI', 'multi-task method', 'MT method', 'multi-task methods', 'MT approach', 'MT-based approaches', 'Multi-tasking', 'multi-task approaches', 'mT5-Large', 'mt08', 'MT corpora', 'Multitask model', 'MT System', 'multi-task frameworks', 'MT Systems', 'Multitask method', 'MT-based approach', 'mT5 models', 'MT dataset', 'mT5-Base', 'Mt', 'Multi-task model', 'MT methods', 'multitask approaches', 'MultiTask', 'multi-task strategy', 'MT corpus', 'multi-task )', 'multitask architectures', 'MT approaches', 'multi-task cell', 'MT-based models', 'multi-tasking model', 'MT-XX', 'mT5-large', 'mT5-Small', 'MULTI-TASK', 'multi-classification task', 'multi-task architectures', 'MT-based systems', 'multitask classifier', 'multitask network', 'MT techniques', 'mt5', 'mT5 SMALL', 'MT2', 'MT3', 'Multi-task models', 'Multi-task framework', 'multi-tasking framework', 'MT Task', 'MT-based method', 'MT-based', 'multi-tasks', 'MT-based methods', 'multi-tasking models', 'multi-tasking approach', 'mT5-Large model', 'mT5 BASE', 'MT-F', 'S2S-MT', 'multi task', 'MT Models', 'Multi-Task Models', 'MT-model', 'Multi-task Model', 'multi-task systems', 'Multi-Tasking', 'Multitask models', 'multi-tasked models', 'multi-task modeling', 'multi-task data', 'Multi-task classification', 'multi-task classification', 'MULTI TASK architecture', 'MT-System', 'multi-task system', 'Multitask architecture', 'mT-base', 'multitask strategy', 'MT architectures', 'multi-task modeling approach', 'multitask system', 'multi-task classifier', 'multi-task network', 'Multi-Task Cell', 'multi-task structure', 'multitask layer', 'MT-II', 'MT4', 'mT 5', 'mT5- large', 'mt05', 'multitaslk approach', 'Multi-task ”', 'multi-task ”', 'MT- * ”', '• +MULTI-TASK', 'MT model ( mt2 )', 'multi-task ( 2 )', 'mT5 2', 'Multi Task ’', 'MT-based TS models', 'mt\ue021system', '+multitask', '+Multitask', 'MT†', 'Multi Task', 'MULTI TASK', 'MULTI-TASK MODELS', 'Multi Task Models', 'multi-task Model', 'Multi-Task Model', 'Multi-Task model', 'Multi-Task framework', 'Multi-task Framework', 'Multi-Task Framework', 'Multi-Task Approach', 'multitask Framework', 'Multitask Models', 'multitask systems', 'multitask modeling', 'multi-classifications tasks', 'MT Data', 'MT architecture', 'Multi-task architecture', 'MULTI-TASK )', 'multi-task *', 'MT )', 'MT (', 'MultiTask model', 'Multitask Model', 'MT-system', 'Multi-Task system', 'multitask strategies', 'MT modules', 'multi-task classification methods', 'multi-task classifiers', 'Multitask architectures', 'multitask-setting', 'multi-task mechanism', 'MT based approaches', 'Multitask )', 'MultiTask )', 'multitask-', 'multitask )', 'multitask networks', 'multitask method', 'multi-task classification models', 'M ULTITASK', 'multi-task classification algorithm', 'multi-task based approach', 'MT-based techniques', 'multi-task networks', 'Multitask structure', 'multi-classification tasks', 'Multi-task Network', 'Multi-task Dataset', 'multi-task-cell', 'multi-task ( mt )', 'MT model ( mt )', 'multi-tasked', 'multi-tasked model', 'multi-task algorithms', 'Multi-task algorithms', 'MT-based technique', 'Multi-tasks', 'multi-task setting )', 'Multi-task modelling', 'multi-tasking from', 'multitask classification approach', 'multitask classification', 'Multi-Tasking Models', '- multi-task', 'multi-tasking architecture', 'MT data set', 'multi-tasking method', 'multi-tasking strategy', 'mT-large', '– S MT', 'L2 MT System', 'MT model 4', 'Mutitask', 'Multi-task 5', 'MT5 model', 'mT5- Large', 'mt5-small', 'mT5 small', 'mT5-small', 'MT5-small', 'mT5- Small', 'mT5- base', 'mT5 Base', 'MT5-base', 'mT5 base', 'mT5-base models', 'mT5-Base models', 'mT5 : mT5', 'mT5 mT5', 'MT5 (', 'mT5 )', 'mT5-Large )', 'mT5-based model', 'mT5-small model', 'mT5 ( mT5 )', 'F1 MT F1', 'F1 MT', 'Multi-Task F1 ( % )', 'mult-tasking approach', 'S2S Multi-task']"
79,267,Method,4.5657,2013,"{'2013': -0.0527, '2014': 0.963, '2015': 2.4323, '2016': 3.9082, '2017': 2.7292, '2018': 4.5657, '2019': 3.5887, '2020': 2.4944, '2021': 1.9087, '2022': 1.0915}","['feed-forward neural network', 'feedforward neural network', 'feed-forward neural networks', 'feedforward neural networks', 'feed forward neural network', 'feed-forward neural network ( FFNN )', 'feed-forward neural network model', 'feed forward neural networks', 'feed-forward NN', 'feedforward neural network model', 'feed-forward NNs', 'feed-forward neural network ( FFN )', 'feed-forward neural network ( FNN )', 'feed-forward neural network models', 'feedforward NN', 'feed forward NN', 'Feed-forward neural networks', 'feed-forward neural classifier', 'feedforward Neural Network', 'feed forward NNs', 'Feed-Forward Neural Network', 'Feed Forward Neural Network', 'feed forward neural network ( FFNN )', 'Feed-Forward Neural Network ( FFNN )', 'feed forward neural network classifier', 'feed-forward neural network classifier', 'feed-forward neural network architecture', 'feed-forward neural networks ( FFNs )', 'feed-foward NN', 'feedforward neural architecture', 'feed-forward neural network ( f )', 'feed-forward neural network ( F ̃a )', 'K feed-forward neural networks', 'feed-forward neural network 6', 'Feed-Forward Neural Networks', 'Feed Forward Neural Network ( FFNN )', 'Feed-Forward Neural Network ( FFN )', 'feed-forward NN ( FFNN )', 'Feed Forward Neural Network ( FFN )', 'feed-forward neural network based model', 'feed-forward neural network-based model', 'feed-forward neural network ( NN )', 'Feed-forward neural network ( nn )', 'feed forward neural networks models', 'feed forward NN models', 'feed-forward NN architecture', 'feed forward neural', 'Feed-forward neural', 'feed-forward neural networks ( FNN )', 'Feed Forward Neural Networks ( FFN )', 'feed-forward neural networks ( FFNN )', 'neural feed-forwards', 'feed-forward neural architecture', 'feed-forward neural mapping model', 'feedfoward neural network', 'feed-forward nerual networks', 'feed-foward neural network', 'feed-foward neural networks', 'Feedforward neural network', 'Feedforward neural networks', 'large feedforward neural network', 'feedforward neural model', 'feedforward neural networks ( NNs )', 'feedforward neural models', '( feedforward ) neural network', 'feedforward neural network architecture', 'feedforward neural layer', 'feed-forward neutral network', 'feed-forword neural networks']"
80,167,Method,4.4887,2016,"{'2016': 0.3627, '2017': 1.4474, '2018': 2.4955, '2019': 4.4887, '2020': 4.4295, '2021': 3.094, '2022': 1.8726}","['copy mechanism', 'copying mechanism', 'copy', 'copy mechanisms', 'Copy', 'copying', 'COPY', 'copy model', 'Copy model', 'copying mechanisms', 'copy system', 'Copy mechanism', 'Copy Mechanism', 'Copy models', 'copy network', 'Copying', 'copy-mechanism', 'copy strategies', 'copy-based', 'copying model', 'copy method', 'Copying Mechanism', 'Copy mechanisms', 'copy models', 'copy strategy', 'copy-based methods', 'copy approach', 'copy-based approaches', 'COPY mechanism', 'COpying', 'Copying mechanism', 'copy module', 'Copy Model', 'copy-model', 'copy mechanism models', 'copy mechanism Model', 'copy mechanism model', 'copy-based architectures', 'Copy strategy', 'copied', 'Copied', 'copy technique', 'copy over mechanism', 'copy theory', 'copying method', 'COPY system', 'copy systems', 'copy-based model', 'copy )', 'copy network model', 'copy-based models', 'copying technique', 'copying )', 'structure-copying methods', 'copy mechanism (', 'copying approach', 'copy ( copy )']"
81,145,Dataset,4.4686,2006,"{'2006': 0.268, '2007': 1.1296, '2008': 1.2796, '2009': 1.4159, '2010': 2.4027, '2011': 3.2815, '2012': 2.7176, '2013': 4.4686, '2014': 3.5054, '2015': 2.8575, '2016': 3.4598, '2017': 2.3819, '2018': 2.2818, '2019': 2.1658, '2020': 1.5091, '2021': 0.8116, '2022': 0.5145}","['Gigaword', 'Gigaword corpus', 'Gigaword dataset', 'Gigaword Corpus', 'gigaword', 'Gigaword data', 'GigaWord', 'GIGAWORD corpus', 'Gigaword 5', 'GigaWord corpus', 'Gigaword corpora', 'Gigaword 5 corpora', 'Gigaword corpus 7', 'Gigaword data set', 'Gigaword5', 'GIGAWORD', 'GigaWord Corpus', 'gigaword data', 'gigaword corpora', 'Gigaword model', 'Giga-Word corpus', 'Gigaword )', 'Giga-Word 6', 'gigaword dataset', 'gigaword corpus', 'Gigaword Model', 'Gigaword *', 'Gigaword 5 corpus', 'Gigaword 5 7', 'Gigaword 7 corpus', 'Gigaword Dataset', 'Gigaword ) corpus', 'GigaWord data', 'giga-word', 'Gigaword Corpora', 'GIGAWORD corpora', 'Giga Word corpus', 'giga-word corpora', 'Gigaword/S', 'Gigaword/S-S', 'Gigaword Models', 'Gigaword data-set', 'Giga Word data', 'Gigaword set', '+Gigaword', 'Gigaword corpus 5', 'GigaWord 5', 'Gigaword 5 5', 'Gigaword 5 dataset', 'Gigaword 5 data']"
82,219,Method,4.3575,2019,"{'2019': 0.0064, '2020': 4.3575, '2021': 3.568, '2022': 2.7166}","['XLNet', 'XLNET', 'XLNet model', 'XLNet models', 'XLnet', 'XLNet-base', 'XLNet-Large', 'Xlnet', 'xlnet-large-cased', 'XLNeT', 'XLNet base', 'xlnet', 'XLNEt', 'XLnet-base', 'xlnet-base', 'XLNet-large', 'XLNet ( L )', 'XLNet−base', 'XLNet-Base', 'XLNet-base/large', 'XLNet-based', 'XLNet-based classifier', 'XLNet-base-cased', 'XLNet algorithm', 'XLNet−large', 'XLNet BASE', 'XLNeT base', 'xlnet-large', 'XLNET LARGE', 'XLNet Large', 'XLNet LARGE', 'XLNET-LARGE', 'XLNET-large', 'XLNet-LARGE', 'XlNet-large', 'XLNet large', 'XLnet-large', 'XLNet-base models', 'XLNet-large-cased', 'XLNET-large-cased', 'XLNET architectures', 'XLNet-base model', 'XLNet ( base ) model', '-XLNet', 'XLNet classifier', 'SOTA XLNet model', 'xlnet-base-cased', 'XLNet LARGE model', 'XLNet ( base )', 'XLNet-base-cased model', 'XLNET-based model', 'XLNet )', 'XLNet-B', 'XLNET-L']"
83,344,Method,4.2642,2019,"{'2019': 0.7113, '2020': 2.8891, '2021': 4.2642, '2022': 3.3631}","['BERT encoder', 'BERT-based encoder', 'BERT-base encoder', 'BERT encoders', 'BERT-Te', 'BERT-based encoders', 'BERT-encoder', 'Bert encoder', 'BERT Encoder', 'BERT-large encoder', 'BERT-Te model', 'BERT encodings', 'BERT-like encoder', 'BERT cross-encoder', 'BERT-like encoders', 'BERT-large encoders', 'BERTbase encoder', 'BERT-Base encoder', 'BERT dual encoder', 'BERT-like Encoder', 'BERT-style encoder-only models', 'BERT Siamese/Dual Encoder', 'BERT-base encoders', 'BERT-style encoder', 'BERT dual encoder retrieval model', 'BERTlarge encoder', 'BERT-based Encoder', 'BERT base encoder', 'BERT Base encoder', 'BERT-Large encoder', 'BERT transformer encoder', 'BERT-encoder-layers', 'cross-encoder ( BERT )', 'BERT encoder.297', 'BERT-based decoders', 'BERT Encoder-Decoder model ( BERT', 'BERT base scale cross-encoder', 'BERT-DEDUCTREASONER', 'BERT Θ encoder', 'COVIDTwitter-BERT model 3', 'BERT-based encoder ( A )', 'BERT EncDec', 'BERT-base-uncased encoder', 'BERT relevance encoder', 'BERT-QE encodehsource', 'encoder-only ( BERT )', 'BERT-based joint encoder', 'BERT-based quote encoder', 'BERT-large encoder 7', 'BERT-style text encoder', 'BERT relevance models', 'BERT EDU Encoder', 'BERT EDU encoder', 'BERT Encoder Holmes', 'BERT-related encoders', 'BERT-encoded tweet', 'BERT-based feature encoder', 'PL-BERT W/ NPCRF', 'BERT encoder 6', 'Entity Pair Encoder ( BERT )', 'biencoder BERT', 'BERT based encoders', 'Bert-Large encoders', 'BERT-based bidirectional Transformer encoders', 'BERT-initialized encoder', 'BERT LARGE -based prophecy decoding model', 'SCI-BERT encoder', 'multiset.bert-base-encoder', 'BERT encoders ( SFRN+ )', 'BERT crossencoders', 'BERT text encoder model', 'BERT text encoder', 'findgeneric BERT', 'Topic-BERT encoder', 'Bert Encoder', 'BERTLarge encoder', 'BERT-based encoder module', 'BERT based encoder', 'BERT ( Base ) encoder', 'BERT BASE encoder', 'BERT ( Base ) Encoder', 'BERT ( Large ) Encoder', 'BERT encoder models', 'BERT-encoder-layer', 'BERT-small encoder', 'BERT-Te tuned', 'BERT encoder-decoder architecture', 'BERT-based encoder and decoder model', 'BERT-based encoder-decoder', 'BERT-based model ( Indexer )', 'BERT based dual-encoder model', 'BERT dual-encoder model', 'Bert dual encoder', 'BERT-based dual-encoder', 'BERT-based dual encoder models', 'BERT-based cross-encoder', 'BERT-based cross encoder', 'BERT-based encoder ( BERTSum )', 'K-BERT encdoer', 'K-BERT encoder', 'Ro-BERT encoder']"
84,420,Method,4.2407,2013,"{'2013': 0.2602, '2014': 2.7326, '2015': 4.0805, '2016': 4.2407, '2017': 2.5673, '2018': 2.2538, '2019': 1.5487, '2020': 0.3762, '2021': 0.0615, '2022': -0.0067}","['AdaGrad', 'Adagrad', 'ADAGRAD', 'Adagrad optimizer', 'adagrad', 'AdaGrad algorithm', 'AdaGrad optimizer', 'Adagrad algorithm', 'Ada-grad', 'AdaGrad )', 'Adagrad )', 'AdaGrad method', 'ADAGRAD classifier', 'ADAGRAD ( ∆ )', 'Adadgrad optimizer', 'ADAGRAD +', 'adaGrad', 'adagrad algorithm', 'Ada Grad', 'ADAGRAD (', 'Adagrad technique', 'ADAGRAD Classifier']"
85,105,Method,4.2406,2004,"{'2004': -0.0462, '2005': 0.0451, '2006': 0.8595, '2007': 0.8226, '2008': 1.6718, '2009': 1.6753, '2010': 1.8646, '2011': 2.6094, '2012': 2.6824, '2013': 3.1654, '2014': 3.1923, '2015': 3.337, '2016': 2.124, '2017': 2.4307, '2018': 2.298, '2019': 2.8894, '2020': 3.3549, '2021': 4.2406, '2022': 3.3566}","['graph', 'graph-based models', 'graph-based model', 'graph-based methods', 'graph-based', 'graph model', 'graph-based approach', 'graph-based method', 'graph-based approaches', 'graph-based algorithm', 'graph-based algorithms', 'graph models', 'Graph', 'graph theory', 'graphs', 'Graph-based methods', 'graph algorithms', 'graph-based framework', 's-graph', 'Tgraph', 'Graph-based', 'graph based models', 'graph structure', 'graph algorithm', 'Graphs', 'graph analysis', 'Graph-based models', 'graph modeling', 'graph-based system', 'graph data', 'graph networks', 'graph-theoretic', 'graph module', 'graph-based techniques', 'graph network', 'graph methods', 'graph based approach', 'graph-theoretic model', 'Graph Model', 'Graph Network', 's-graphs', 'graph based algorithm', 'graph structures', 'Graph-based approaches', 'graph based methods', 'graph based approaches', 'graph based method', 'graph based model', 'Graph models', 'graph based algorithms', 'Graph-based algorithms', 'graph-structured data', 'TGRAPH', 'GRAPH', 'Graph-based Models', 'graph-structured networks', 'graph architecture', 'graph-based systems', 'graph method', 'Graph Structures', 'Graph-Based', 'graph based', 'Graph-based model', 'graph approach', 'graph-theoretic framework', 'Graph-Based Algorithm', 'Graph structures', 'Graph-based Method', 'Graph based models', 'Graph Based Approach', 'graph-based modeling', 'graph-based mechanisms', 'graph framework', 'S-S graph', 'graph-based architectures', 'graph )', 'graph-based classification framework', 'XGraph', 'Graph-based algorithm', 'Graph based approaches', 'Graph model', 'Graph-based Model', 'Graph-Based Model', 'graph theoretic', 'Graph Theory', 'Graph based techniques', 'graph-based networks', 'graph-structured', 'graph-theoretic approach', 'graph-based measures', 'SS-Graph', 'Graph-based )', 'graph analysis techniques', 'S–S graph', 'structured graph', 'gold graph', 'graph-based classifiers', 'graph modules', 'graph techniques', 'graph-based technique', 'graph-based data', 'graph-based measure', 'Corpus Graph', 'Graph-structured techniques', '\ue6ae Graph Module', '\ue6ae \ue6ae Graph Module', 'graph-structures', 'Graph Based Methods', 'Graph-Based Methods', 'Graph based methods', 'Graph based method', 'Graph-based method', 'Graph-Based Models', 'Graph-based approach', 'graph Model', 'graph-based Model', 'Graph Models', 'Graph Based Algorithms', 'Graph-based Algorithms', 'Graph based algorithms', 'graph-theory', 'graph structured data', 'Graph algorithm', 'Graph Algorithm', 'graph-based modules', 'Graph Structure', 'Graph modeling', 'Graph Modeling', 'Graph-based techniques', 'Networks Graph networks', 'Graph networks', 'graph-based network models', 'graph-based structure', 'Graph algorithms', 'Graph-based analysis', 'graph-based analysis', 'graph-structured approaches', 'structured graph modeling', 'Graph Structure Analysis', 'Graph Approach', 'Graph approaches', 'SOTA graph-based model', 'graph-theoretic techniques', 'Graph-Theoretic Approaches', 'graph-theoretic measures', 'SS Graph', 'Graph methods', 'Graph-based Classification', 'graph-based classification', 'graph-based ,', 'graph-based (', 'S-S Graph', 'Graph-theoretic algorithms', 'S-graphs', 'graph classification', 'graph in', 'graph-based metric', 'graphs )', 'G raph', 'graph-based module', 'graph-', 'data graph', 'graph module networks', 'graph-based classifier', 'Graph based Graph based method', 'graph mechanism', 'graph-based architecture', 'Big Graphs', 'graph analysis method', 'graph theoretic models', 'graph structure data', 'graph analysis algorithms', 'G raph-based', 'mechanism graph', 'graphs techniques', 'graph network based methods', 'graph-theoretic algorithm', 'graph system', 'graph-based analysis framework', 'graph-based models—for', 'graph-theoretic method', 'graph-structured modeling', 'graph-structured models', 'graph-structured methods', 'graph-structured techniques', 'graph architectures', 'graph-based SOTA model', 'Graph model ( ours )', 'Graph Dataset']"
86,131,Method,4.2398,2003,"{'2003': -0.1522, '2004': 0.3407, '2005': 0.2243, '2006': 0.7796, '2007': 0.4156, '2008': 0.6504, '2009': 2.1278, '2010': 2.1928, '2011': 2.3729, '2012': 1.7867, '2013': 2.264, '2014': 2.1207, '2015': 2.0636, '2016': 1.2044, '2017': 1.8396, '2018': 2.0757, '2019': 2.9252, '2020': 3.1853, '2021': 3.4769, '2022': 4.2398}","['KL divergence', 'KL-divergence', 'Kullback-Leibler divergence', 'KLdivergence', 'KLD', 'KL-Divergence', 'Kullback-Leibler ( KL ) divergence', 'KL Divergence', 'KullbackLeibler divergence', 'Kullback–Leibler divergence', 'KL divergences', 'Kullback–Leibler ( KL ) divergence', 'Kullback-Leibler divergence ( KLD )', 'Kullback-Leibler Divergence', 'KL-divergences', 'Kullback-Liebler divergence', 'KLDivergence', 'K-L divergence', 'kld', 'Kullback Leibler ( KL ) divergence', 'KLD based method', 'KL divergence approach', 'Kullback Leibler divergence', 'Kullback–Leibler divergence ( KL )', 'KLD models', 'KL-divergence scoring method', 'Zero-KL divergence', 'kl-divergence', 'Kullback/Leibler divergence', 'Kullback Leibler Divergence', 'Kullback-Leibler divergence ( KL divergence )', 'Kullback–Leibler divergence ( KLD )', 'KL divergence ( KL )', 'offset KL divergence', 'KL divergence test', 'Kullback–Leiber divergence', 'Kullback-Leiber divergence', 'Kullback-Leibler Divergence ( KLDIV )', 'KLdivergence ( KL-Div . )', 'Kullback-Leibler divergence ( KL-Div )', 'Kullback-Leibler divergence ( KL-div )', 'KL Divergence ( KL Div )', 'KL-divergence ( KL-div . )', 'Kullback-Leibler Divergence ( KL-DIV )', 'Kullback-Leibler ( KL ) divergency', 'KL divergency', 'KL-divergencies', 'Kullback-Leibner divergence', 'Kullback-Leibler divergence ( PKL )', 'KLdivergence scoring method', 'KL divergence score', 'KullbackLiebler Divergence', 'KullbackLiebler divergence', 'Kullback-Leibler Divergence ( m-KL )', 'Kullback-Leibler divergence ( D KL )', 'Kullback–Leibler divergence ( D KL )', 'Kullback-Leibler divergence D KL', 'Kullback-Leibler divergence measure ( KLM )', 'L UKLD ( KLD )', 'IVs KL divergence', 'Topic KLD Model', 'kullback-leibler divergence', 'Kullback–Leibler Divergence', 'KL -divergence', 'Kullback—Leibler divergence', 'KL-DIVERGENCE', 'KLDivergence ( KLD )', 'KL divergence ( KLD )', 'Kullback-Leibler divergence ( KL-divergence )', 'KL -divergence ( KLD )', 'KLD ( KLD )', 'Kullback-Leibler Divergence ( KL-Divergence )', 'KL Divergence ( KL )', 'KL-divergence ( KL )', 'Kullback-Leibler Divergence ( KL )', 'Kullback-Leibler divergence ( KL )', 'KullbackLeibler Divergence', 'KL divergence—', 'KL-divergence )', 'KL divergence )', 'Kullback-Leibler ( KL ) Divergence', 'KL ( Kullback-Leibler ) divergence', 'KLD KLD', 'KLD model', 'KLD Model', 'KL-divergence model', 'KullbackLeibler divergences', 'KL divergence strategy', 'KL divergence measure', 'KL-divergence measure', 'KL Divergence method', 'Kullback-Leibler divergence method', 'KL ( Kullback–Leibler divergence )', 'KL-divergence-based', 'KL divergence classifier', 'KL-divergence-based approach', 'divergence ( KL )', 'KL-diverge', 'Offset KL divergences', 'Kullback-Leibler ( K-L ) divergence', 'KL divergence 3', 'pseudo KL divergence', 'Pseudo KL divergence', 'KL-divergence loss ( KL )', 'KL divergence loss ( KL loss )', 'KL divergence ( drift )', 'KL divergence loss ( D2U-KL )', 'KL- { 1,2 } ( KL divergence )', 'skewed KL divergence']"
87,46,Metric,4.2036,2006,"{'2006': -0.0869, '2007': 0.5048, '2008': 0.5513, '2009': 0.9663, '2010': 0.4572, '2011': 1.0038, '2012': 1.3516, '2013': 0.9141, '2014': 1.1755, '2015': 1.3805, '2016': 1.3526, '2017': 1.2445, '2018': 2.5452, '2019': 3.3017, '2020': 3.8717, '2021': 3.7099, '2022': 4.2036}","['METEOR', 'MT', 'Meteor', 'MT model', 'MT systems', 'METEORR', 'MT metrics', 'METEOR ( M )', 'METEOR↑', 'MT dataset', 'HMETEOR', 'meteor', 'W-METEOR', 'Q-METEOR', 'METEOR )', 'METEOR metrics', 'METEOR metric', 'WMETEOR', '∆METEOR', 'METEOR ( % )', 'Meteor ( % )', 'METEOR model', 'METEOR measure', 'METEOR measures', 'meteors', 'METEOR models', 'Meteor package', 'dev set METEOR', 'Meteor ( M )', 'M ( Meteor )', 'METEOR 8']"
88,244,Method,4.1923,2014,"{'2014': 0.1097, '2015': 1.3846, '2016': 2.4812, '2017': 2.9497, '2018': 4.1923, '2019': 4.1887, '2020': 2.5227, '2021': 1.8269, '2022': 0.9894}","['max-pooling', 'max pooling', 'k-max pooling', 'Max-pooling', 'Max-Pooling', 'max-pool', 'Max Pooling', 'Max pooling', 'max pool', 'Maxpool', 'maxpool', 'max-pooled', '+max-pooling', 'kmax pooling', 'MaxPool', 'Max-pool', 'K-max pooling', 'max-pooling approach', 'max-pooling method', 'max-pooling layers', 'max-pooling architecture', 'k-max-pooling', 'MAX POL', 'MAX pooling', 'MAXPOOL', 'max pooling mechanism', 'max poolings', 'max-pooling strategy', '+MaxPool', 'max-pooling ( C )', 'cs-MAXPOOL', 'max-pooling 3', 'max-k pooling', 'max-k-pooling', 'k-max Pooling', 'K-Max Pooling', 'k-Max Pooling', 'k -max pooling', 'k-max pooling strategy', 'k-max pooling mechanism', 'max-pooling ( · )', 'MAX-POOLING', 'Max pool', 'Max-Pool', 'max pooling approach', 'maxpool (', 'MaxPool )', 'max pooling layer', 'max-pooling layer', 'max-pooling mechanism', 'max-pools', 'max-pool )', 'max pool )', 'max-poolings', 'max-pooling mechanisms', 'max-pooling model', 'max-over pooling', 'max-over-pooling', 'max pooling technique', 'max-pooling technique', 'max pooling based method', 'Max-Pooling models', 'max pooling ]']"
89,441,Method,4.1739,2019,"{'2019': 0.9381, '2020': 3.2437, '2021': 4.1739, '2022': 3.8358}","['bert-base-uncased', 'BERT-base-uncased', 'BERT variants', 'bert-base-uncased model', 'BERT-base-uncased model', 'BERT+ResNet', 'BERT variant', 'bert-large-uncased', 'BERTbase-uncased', 'BERT-large-uncased', 'BERT-Ultra-Pre', 'BERT-VERNet', 'BERT-base uncased model', 'bertbase-uncased', 'BERT-uncased', 'BERT+LASER+GBDT', 'BERT+Patterns', 'BERT+ENG', 'BERT-base ( uncased ) model', 'BERT BASE-UNCASED', 'BERT-based-uncased model', 'BERT ( base uncased )', 'BERT ( bert-base-uncased )', 'BERT+PostKS+CP', 'bert-base-uncased ’ model', 'BERT+AVG', 'BERT+Mirror', 'BERT [ CLS ] -vector', 'BERT CLS-vector', 'BERT+GENIA', 'BERT-base-uncased QA model', 'BERT+Cos+Doc', 'BERT+LogReg *', 'BERT+FUR+ENT', 'BERT+T+cat+R', 'BERT-Base-Uncased model', 'BERT base uncased model', 'BERT-BASE-UNCASED', 'BERT-base ( uncased )', 'BERT ( base-uncased )', 'BERT BASE ( uncased )', 'BERT large-uncased architecture', 'uncased BERT BASE model', 'uncased BERT base model', 'bert-base-uncased models', 'BERT-large-uncased model', 'BERT-based-uncased', 'bert-based-uncased', 'BERT-base-uncased 109M', 'BERT+Linear', 'BERT+RoBERTa', 'bert-base-uncased 2', 'BERT-Base uncased model 2', 'BERT versions', 'BERT-uncased ( BERT-uc )', 'BERT-Large Unsup', 'BERT+AEDA', 'basic- BERT ( ‘ bert-base-uncased ’ ) model', 'BERT-base-uncased version 5', 'bert-base-uncased ” model 4', 'BERT-based model ( UmlsBERT )', 'BERT-URC', 'BERT+ResNet model', 'BERT+ResNet classifier', 'BERT+Glyce', 'BERT VGGNet', 'BERT+SOC', 'BERT+BLEU', 'BERT+Hierarchical', 'BERT+BLEU3', 'BERT VisualBERT', 'BERT+BLEU5', 'BERT-base 2 ( uncased ) model', 'Table-BERT tuned', 'BERT vector', 'BERT+ALBERT', 'BERT+PAWS', 'LEGAL-BERT-base-uncased 5', 'legal-bert-base-uncased', 'BERT-Tuned†', 'BERT+LogReg', 'BERT+UMAP+HDBSCAN', 'bert-base-uncased spacy-models', 'BERT base ( uncased ) 1', 'BERT-based-uncased 1', 'bert-base-uncased 4', 'BERT-base-uncased 4', 'Bert-base-uncased model 4', 'BERTbase uncased model 4', 'BERT variation', 'BERT variations', 'VIRT-BERT-Small', 'VIRT-BERT-Large', 'BERT Variants', 'BERT-variants', 'BERT-variant', 'BERT-small variants', 'BERT-Large variants', 'BERT-large variant', 'BERT-base variant', 'BERT-Base variants', 'vector space - - BERT', 'BERT vector space', 'BERT+Social', 'BERT+BETO', 'ZSBT BERT', 'BERT validation', 'BERT+Sample', 'BERT Base-uncased model', 'BERT base-uncased model', 'BERT BASE-UNCASED model', 'BERT-Base-uncased model', 'BERT Base Uncased model', 'BERT base ( uncased ) model', 'BERT ( Base-uncased ) model', ""bert-base-uncased '' model"", 'BERT-base uncased', 'BERT BASE-uncased', 'BERT Base uncased', 'BERT-Base , Uncased', 'BERT base-uncased', 'Bert-base-uncased', 'BERT base uncased', 'bert-based-uncased model', 'base uncased BERT model', 'Bertbase-uncased', 'bert-uncased', 'BERTlarge uncased', 'BERT-Uncased', 'BERT-large-uncased models', 'bert-base-uncased )', 'BERT-LARGE-UNCASED', 'BERT-Large-Uncased', 'BERT large uncased architecture', 'uncased BERT-base model', 'uncased BERT-Base model', 'BERTbase-uncased model', 'bertbase-uncased model', ""bertbase-uncased '' model"", 'BERT-uncased model', 'BERTlarge-uncased model', 'BERTBASE-UNCASED model', 'BERT-base-uncased models', 'Bert-large-uncased model', 'BERT-Large-Uncased model', ""bert-large-uncased '' model"", 'BERTbase ( bert-base-uncased )', 'Bert-based-uncased', 'bert-large-uncased )', 'uncased-BERT-base', 'uncased BERT Base', 'uncased BERT large model', 'bert-base-uncase', 'BERT-cased/uncased', 'uncased BERTBase model', 'uncased BERT model', 'BERT ( bert-base-uncased ) classifier', 'BERT-uncased-base', 'base BERT uncased', 'BERT-uncased-large model', 'BERT uncased base model', 'bert-uncased-base )', 'BERT base model ( uncased )', 'BERT-base model ( uncased )', 'bert-base-uncased 6', 'BERT-Large , Uncased 6 model', 'BERT-REG vectors', 'BERT model ( UWBERT )', 'BERT ( WBERT ) models', 'BERT-Base-Uncased ∗', 'BERT-base-uncased ’', 'BERT+Labels', 'BERT+MemNet', 'BERT+MemNet model', 'Truncated BERT', 'BERT MINI version', 'BERT+Cos+Ref', 'BERT+Linear architecture', 'BERT-base vocab', 'BERT+CrossAtt', 'BERT+LXMERT', 'BERT+Adapter', 'bert − base − uncased models', 'BERT\ue003Encoder\ue003\ue003', 'TSDAE-BERT-Base', 'BERT based verification model', 'tBERT ( tuned BERT )', 'BERT BASE uncased model 5', 'BERT-Base ( cased version ) 5', 'BERT+WordNet', 'BERT-base-uncased model 8', 'BERT-base ( “ uncased ” ) model', 'BERT mdoel ( bert-base-uncased )', 'bert-base-uncased variant', 'bert-large-uncased variant', 'BERT-large-uncased 13', 'BERT + UNANSWERABLE', 'bert-based-uncased ”', 'bert-base-uncased ” models', 'bert-base-uncased model 2', 'BERT+Local', 'BERT-based version', 'BERT ( base version )', 'BERT-Ultra-Pre also', 'BERT-Ultra', 'BERT+CNNgram', 'BERT+RDT', 'BERT-based-uncased model 3', 'BERT-base-uncased 3', 'bert-base-uncased 3', 'BERTbase ( uncased ) 3', 'BERT+SST+DAT']"
90,96,Method,4.1317,2007,"{'2007': -0.0022, '2008': 0.1333, '2009': 0.5657, '2010': 1.5778, '2011': 1.9093, '2012': 3.2134, '2013': 3.3803, '2014': 4.1317, '2015': 2.9884, '2016': 2.0442, '2017': 1.5898, '2018': 1.3547, '2019': 1.7349, '2020': 1.1044, '2021': 1.1056, '2022': 0.9257}","['TM', 'topic models', 'topic model', 'topic modeling', 'Topic models', 'topic', 'Topic Models', 'Topic', 'topic modelling', 'topic modeling approach', 'Topic modeling', 'Topic Model', 'Topics', 'topic modeling techniques', 'Topic modelling', 'TOPIC', 'topic-based methods', 'topic modeling algorithm', 'topic-based models', 'in-topic model', 'Topic Modeling', 'topics', 'topic modeling approaches', 'topic classifier', 'Topic Model ( TM )', 'Topic model', 'topic-based', 'topic-based classification', 'topic-based model', 'topic-modeling approach', 'topic modelling techniques', 'tm', 'topic-model', 'topics models', 'topic modeling algorithms', 'topic modelling approaches', 'k-topic model', 'topic-modeling', 'topic )', 'topic model approaches', 'topic based classification', 'topic modeling method', 'structured topic models', 'TM )', 'topic model based approaches', 'topic based approaches', 'Topic modeling algorithms', 'topic model-based models', 'topic modeling methods', 'topic modelling approach', 'topic model approach', 'TM model', 'topic-based system', 'in-topic models', 'Topic-5', 'TOPIC model', 'topic modeling ( TM )', 'Topic Modeling ( TM )', 'TM strategies', 'topic classification', 'topic-classification', 'SOTA topic models', 'topic-', 'Topic-based', 'topic-modeling method', 'topic model-based techniques', 'Topic Model based Techniques', 'Structured Topic Models', 'topic-model based approach', 'topic model based', 'topic model-based approaches', 'topic-model-based approaches', 'TM-based approaches', 'topic based classifier system', 'topic modeling based classification systems', 'topic models ( TM )', 'topic model ( TM )', 'topic-based approaches', 'Topic-based approaches', 'topic modelling-based approach', 'topic model-based methods', 'topic model–based methods', 'topic model based methods', 'topic model algorithms', 'topic model-based model', 'topic-based method', 'large topic models', 'topic modeling-based approach', 'topic modelling-based approaches', 'topic modelling based approaches', 'topic modeling based approaches', 'topic structures', 'topic modeling framework', 'topic model framework', 'topic-based analysis', 'topic-based approach', '( topic ) model', 'topic model—for', 'Topic Modeling Based Methods', 'topic modeling based methods', 'Structured Topic', 'topic classification model', 'Topic classification model', 'topic modeling corpus', 'modeling ( TM )', 'in-topic', 'topics model', 'TM method', 'structured topic model', 'structured topic modelling', 'topic model architecture', 'In-topic models', 'topics data', 'topic modelling-based models', 'Topic-Modeling ( TM ) Based', 'TM algorithm', 'Topic Model Networks', 'Topic 7']"
91,48,Method,4.0903,2006,"{'2006': -0.0773, '2007': 0.2656, '2008': 0.5039, '2009': 1.0182, '2010': 2.246, '2011': 2.3045, '2012': 3.0023, '2013': 4.0903, '2014': 3.6174, '2015': 3.9616, '2016': 2.3406, '2017': 1.7225, '2018': 1.7087, '2019': 1.8203, '2020': 1.305, '2021': 1.2368, '2022': 0.9887}","['LDA', 'LDA model', 'LDA models', 'lda', 'LDA-based models', 'Rel-LDA', 'LDA algorithm', 'Rel-LDA1', 'LDA approach', 'linear discriminant analysis', 'LDA-based', 'LDA-based approach', 'LDA-based methods', 'rel-LDA', 'LDA-based model', 'linear discriminant analysis ( LDA )', 'LDA )', 'w-LDA', 'Linear Discriminant Analysis ( LDA )', 'LDA-based method', 'rel-LDA1', 'Linear Discriminant Analysis', 'LDA method', 'LDA analysis', 'T C lda', 'LDA classifier', 'LDA modeling', 'linear discriminants', 'rel-LDA models', 'BHMM LDA', 'LDA 16', 'LDA lmao', 'LDA 2', 'MVC-LDA', 'C lda', 'Linear discriminant analysis ( LDA )', 'linear discriminant', 'LDA LDA', 'linear discriminant analysis LDA', 'Base LDA', 'LDA methods', 'LDA-based techniques', 'linear discriminant analysis algorithm', 'LDA technique', 'linear discriminant models', 'linear discriminant modeling', 'LDA-based analysis', 'LDA system', '( LDA ) model', 'LDA framework—by', 'rel-LDA1 models']"
92,250,Method,4.0892,2003,"{'2003': 0.3716, '2004': -0.1355, '2005': 0.2007, '2006': 0.3703, '2007': 0.9833, '2008': 0.2065, '2009': 0.9894, '2010': 1.158, '2011': 0.534, '2012': 1.2013, '2013': 1.2388, '2014': 2.0173, '2015': 1.4651, '2016': 1.5806, '2017': 1.2543, '2018': 1.9958, '2019': 2.9407, '2020': 2.9912, '2021': 3.4073, '2022': 4.0892}","['train', 'training', 'Training', 'train set', 'Train', 'training algorithm', 'train data', 'TRAIN', 'Trains corpus', 'trained', 'train dataset', '∆-training', 'training data', 'Train set', 'trains', 'train+dev', 'training set', 'Train Dev', 'training methods', 'train-set', 'training dataset', 'train_set', 'training method', 'Training Algorithm', 'training mechanism', 'structured training', 'training strategies', 'in-training', 'train sets', 'training model', 'model-train', 'training approaches', 'TRAINS', 'Train data', 'train-dev', 'train/dev', 'train corpus', 'training corpus', 'training approach', 'training strategy', 'training-based approaches', 'Train+Dev dataset', 'train+dev set', 'Trained', 'training-set', 'Training algorithm', 'Train Dataset', 'training )', 'Training approach', 'Training Dataset', 'trained model', 'TRAINS corpus', 'In-Train', 'training-based methods', '∆-training framework', 'Train+Dev', 'train+dev data', 'train+dev data set', 'α train', 'TRAIN sets', 'Train sets', 'TrainS', 'Trains', 'Training Set', 'in-training approach', 'base model training', 'TRAIN data', 'base training data set', 'train *', 'train )', 'train–', 'Training Method', 'train dev', 'TRAIN/DEV', 'Training methods', 'training-based', 'TRAIN corpus', 'train-corpus', 'Train dataset', 'Train Train * Dev', 'Training algorithms', 'Training Models', 'Training models', 'training architectures', 'Trained classifiers', 'The ( train ) dataset', '# train', 'model-training', 'Training data', 'Training Corpus', 'Training corpus', 'training system', 'Dev-Train', 'dev-train', 'big training data', 'Big Training Data', 'The training ( train', 'Corpus Train set Dev set', 'structured training approaches', 'system train dev', 'GOLD training', 'Training Data Model', 'train_model', 'train fold', 'training network', 'structure training strategy', 'train/dev set', 'train-dev set', 'training techniques', 'Training Techniques', 'Trained model', 'gold training data', 'training-based approach', 'Trained models', 's-training set', 'large training dataset', 'Structured Training Approach', 'train models', 'Corpus Train', 'train corpora', 'training framework', 'Train Dataset model', 'gold training dataset', 'train/dev sets', 'Training-based Approaches', 'classification training algorithm', 'training classifiers', 'Train classifier', 'ss-train', 'Training Corpora', 'training corpora', 'network training', 'train method', 'small-set Train', 'trained base module']"
93,100,Method,4.0821,2002,"{'2002': 0.2989, '2003': -0.1655, '2004': 0.0381, '2005': -0.0586, '2006': 1.0354, '2007': 0.3513, '2008': 0.5987, '2009': 0.7242, '2010': 0.7736, '2011': 0.394, '2012': 1.0701, '2013': 1.484, '2014': 0.9026, '2015': 1.578, '2016': 1.0752, '2017': 1.3557, '2018': 1.9192, '2019': 3.083, '2020': 3.3476, '2021': 4.0821, '2022': 3.8365}","['NER', 'NER model', 'NER models', 'Named Entity Recognition ( NER )', 'named entity recognition ( NER )', 'NER system', 'nested NER', 'NER dataset', 'Named entity recognition ( NER )', 'named entity recognition', 'NER++', 'NER data', 'NER+L model', 'ner', 'NER systems', 'NER )', 'ORCAS-NER', 'CORD-NER dataset', 'Nested NER', 'Named Entity Recognition', 'CORD-NER', 'nested NER models', 'FOFE-NER', 'FS-NER', 'OA-NER', 'NER-SL', 'NER corpus', 'named-entity recognition', 'named entity recognition ( NER ) models', 'named entity recognition ( NER ) model', 'named entity recognition system', 'Bio-NER', 'NER Model', 'NER classifier', 'IC-NER models', 'CAN-NER', 'Named Entity Recognition ( NER', 'Named entity recognition', 'Named Entity Recognition system', 'NE recognition model', 'NER approach', 'Named Entity Recognition ( NER ) system', 'rule-based NER system', 'CAN-NER Model', 'T-NER', 'LX-NER', 'CORD-NER corpus', 'nested NER model', 'nested named entity recognition ( NER )', 'Named-Entity Recognition ( NER )', 'named-entity recognition ( NER )', 'NER corpora', 'NE recognition', 'NER architecture', 'NER approaches', 'NER algorithms', 'named entity recognition model', 'NER techniques', 'NER data sets', 'named entity recognition ( NER ) system', 'mixed case NER', 'IC-NER model', 'deep NER models', 'span-based NER', 'CO-NER', 'L NER', 'FOFE-NER model', 'URES-NER', 'NER4', 'NER++ system', '• Named Entity Recognition ( NER )', 'NER+L', 'NER3', 'NER 3-fold', 'Nested Named Entity Recognition', 'nested NER dataset', 'Named-entity recognition ( NER )', 'named entity recognition ( NER ) )', 'NER Models', 'Named Entity Recognition ( NER ) models', 'NER Module', 'Named Entity Recognition ( NER ) model', 'named entity recognition data', 'NER package', 'NER methods', 'NER method', 'named entity recognition )', 'name entity recognition ( NER )', 'f i ner -139', 'rule-based NER', 'CM-NER', 'span-based NER models', 'span based NER model', 'named entity recognition ( NER ) model 2', 'NW-NER', 'NKJP-NER', 'DXY-NER', 'CI NER', 'NITE-NER', 'L S NER model', 'OA-NER+SS', 'es ) NER', 'SA-NER', 'CLS NER', 'NER-to-EAT', 'NER+EL model', 'NER+EL system', 'ner -139', 'RGX + NER', 'CORD-NER System', 'Named Entity Recognition ( NER ) algorithm 4', 'NER-Wh', 'RND NER', 'nese NER', 'word-based NER model', 'NER cus', 'NER++ Method', 'NER++ model', 'NER++ approach', 'NER++ classifier', 'NER sys', 'NER+NED', 'in-house NER data', 'F1 NER', 'NER F 1', 'NER ( F1 )', 'En-Hi NER corpus', 'NER+L systems', 'En-Es NER', 'NER-SL models', 'IBM named entity recognition system', 'NER/EL models', 'NER 5', 'Nested Name Entity Recognition', 'Nested NER models', 'nested named entity recognition', 'nested NER algorithm', 'name entity recognition model', 'named entity recognition ( ner )', 'Named-entity Recognition ( NER )', 'named-entity recognition ( NER ) systems', 'named entity recognition ( NER ) systems', 'Named Entity Recognition ( NER ) systems', 'Named-Entity Recognition ( NER', 'named entity recognition ( NER ) techniques', 'NER *', 'named entity recognition ( NER ) corpus', 'NER System', 'Named Entity recognition', 'NER Dataset', 'named entity recognition ( NER ) modeling technique', 'Named-Entity Recognition ( NER ) model', 'named-entity recognition ( NER ) model', 'base NER system', 'base NER systems', 'NER Data', 'named-entity recognition system', 'NE recognition system', 'Named entity recognition systems', 'Named-entity recognition systems', 'NE recognition systems', 'named entity recognition data set', '( -ner )', 'NER classifiers', 'Named Entity Recognition (', 'Named Entity Recognition )', 'named entity recognition ( NER ) approaches', 'NER ( ner )', 'Name Entity Recognition ( NER )', 'Named Entity Recognition Architecture', 'NER dev set', 'gold NER data', 'NER gold data', 'NER data set', 'NER small', 'NER-based', 'Named Entity Recognition module', 'NER dev sets', 'base NER algorithm', 'named-entity recognition approach', 'NER network architectures', 'NER ( Named Entity Recognition )', 'Named Entity Recognition ( NER ) methods', 'base NER models', 'Base NER Models', 'Named entity recognition dataset', 'NER-based systems', 'base NER', 'For Named Entity Recognition ( NER )', 'entity recognition ( NER )', 'NER framework', 'news-based NER dataset', 'NER IID models', 'UR NER dataset', 'IC-NER', 'IC NER', 'EN-JA NER', 'deep NER model', 'NER com', 'rule-based named entities recognition module', 'rule-based NER systems', 'NER -8 -6', 'CM-NER corpus', '( CM-NER ) corpus', 'NER stu', 'span based NER', 'span based named entity recognition ( NER ) model', 'NER 1K', 'NER 2 system', 'NER2', 'NER 2', 'named entity recognition system 2', 'Named entity recognition and classification ( NERC )', 'Named-entity recognition and classification ( NERC )', 'named entity recognition and classification ( NERC ) systems', 'Named Entity Recognition 7', 'CAN-NER model', 'DL-based NER', 'NER/D', 'CR-NER', 'NER-CR-NEL', 'F1 NER1', 'OA-NER approach', 'BERN2 NER model', 'TW-NER', 'KMOU NER', 'NER dataset 1', 'NER system 1', 'NER1', 'NER 1', 'L ner', 'FOFE NER', 'T-NER system', 'T-NER )', 'UDP NER', 'k NER models', 'DAPC-NER', 'NER Dutch']"
94,311,Method,4.066,2014,"{'2014': -0.0655, '2015': 1.3181, '2016': 2.7188, '2017': 3.1389, '2018': 4.066, '2019': 3.0431, '2020': 2.2365, '2021': 2.0127, '2022': 2.0101}","['dropout', 'Dropout', 'dropout strategy', 'dropout technique', 'dropout method', 'dropout mechanism', 'dropout model', 'Dropout strategy', 'dropouts', 'dropout methods', 'dropout techniques', 'structured dropout', 'Dropouts', 'dropout-based', 'structure dropout', 'G-Dropout', 'F-Dropout', 'Dropout method', 'dropout models', 'layer dropout', 'drop-out', 'Drop-out', 'dropout strategies', 'dropout Q-Network', 'Droupout', 'Dropout ( D )', 'dropout ”', 'Dropout ’', 'dropout ( DO )', 'Dropout+Metric', 'DropOut', 'DROPOUT', 'structured dropout methods', 'Structured dropout', 'Dropout models', 'dropout (', 'dropout )', 'Drop-Out', 'drop out', 'Dropout dropout', 'drop-outs', 'Dropout mechanism', 'network-dropout', 'DROPOU T', 'dropout-based approach', 'Structure dropout', 'data dropout', 'data dropout strategy', 'value dropout', 'Dropout-based methods', 'Dropout layer', 'Dropout layers', 'dropout-based method']"
95,218,Method,4.0154,2005,"{'2005': 0.2573, '2006': 0.1305, '2007': 0.3584, '2008': 0.06, '2009': 0.2169, '2010': 0.744, '2011': 0.4935, '2012': 1.3037, '2013': 0.8207, '2014': 0.8136, '2015': 1.2858, '2016': 1.1569, '2017': 0.4524, '2018': 0.8549, '2019': 1.6292, '2020': 2.6075, '2021': 3.0402, '2022': 4.0154}","['NLP', 'NLP models', 'NLP techniques', 'NLP model', 'natural language processing', 'NLP systems', 'NLP4J', 'NLP methods', 'NLP system', 'UTD_NLP', 'NLP algorithms', 'Natural Language Processing', 'natural language processing ( NLP )', 'NL+NLP', 'Natural language processing', 'Natural Language Processing ( NLP )', 'natural language processing techniques', 'Natural Language Process', 'UNC NLP', 'UCF_NLP2', 'NLP+C', 'Natural language processing ( NLP ) techniques', 'Natural language processing models', 'Natural language processing ( NLP )', 'Natural language processing techniques', 'Natural Language Processing ( NLP ) techniques', 'Natural Language Processing ( NLP ) models', 'GS-NLP', 'Malay NLP', 'nlp', 'NLP )', 'NLP approaches', 'natural language processing ( NLP ) techniques', 'NLP4PLP dataset', 'NLP-cont', 'UCF_NLP1', 'NLP Models', 'natural language processing ( NLP ) models', 'NLP-based system', 'NLP4PLP', 'UNAL-NLP', 'NLP 1', 'UW NLP', 'natural language processing models', 'Natural language processing systems', 'NLP-based', 'Natural language processing ( NLP ) models', 'NLP algorithm', 'Natural Language Processing ( NLP ) methods', 'NLP-based systems', 'corpus-based NLP techniques', 'NLP approach', 'NLP-based method', 'NLP DAtaset', 'MSR-NLP', 'BLCU_NLP', 'X NLP', 'NLP2RDF', 'natual language processing', 'NLP & CC', 'UTD_NLP 2', 'N -Dataset NLP', 'i2b2 NLP dataset 2', 'NLP commu', 'NLP–be', 'nlp4j 7', 'language-neutral processing', 'NLP2CT', 'HCI-NLP', 'CU-NLP', 'NLP4NLP Corpus', 'bio-NLP', 'NLP 1 methods', 'NLP—it', '• NLP4J', 'NLP dataset 6', 'Unified natural language processing', 'No NLP', 'NLP 4', 'Natural language processing algorithms', 'natural language processing algorithms', 'nlp-models', 'Natural Language Processing techniques', 'NLP-methods', 'Natural language processing methods', 'NLP Systems', 'natural language processing systems', 'NLP Model', 'natural language processing model', 'NLP ( )', 'Natural Language Processing ( NLP ) systems', 'natural language processing system', 'Natural Language Processing ( NLP ) algorithms', 'Natural language processing ( NLP ) algorithms', 'SotA NLP models', 'SotA NLP', 'SOTA NLP techniques', 'NLP corpora', 'classification based NLP algorithms', 'natural language processing based system', 'modeling NLP structured data', 'NLP based models', 'NLP-based approach', 'NLP analysis', 'Natural Language Processing techniques ( NLP )', 'NLP based approaches', 'NLP-based approaches', 'NLP-based corpus', 'big-data NLP', 'NLP data', 'NLP-based methods', 'classification-based NLP', 'NLP data sets', 'NL P', 'natural language process', 'NLP-based method.', 'Corpus-based NLP', 'Natural Language Processing ( NLP ) model', 'big NLP models', 'natural language processors', 'CLTK ’ s NLP ( )', 'NLP model - M', 'FL-based NLP', 'NLP/HTML-based methods', 'NLP4J package 5', 'NLP4J 2', 'NLP-HUJI', 'NLP4PLP dataset 5', 'Non-NLP', 'CSK-NLP', 'D-NLP1', 'D-NLP2', 'NLP-Cube', 'NL+NLP approach', 'DL4J NLP package 7', 'NL+NLP ( PB )', 'NLP—one', 'voice NLP', 'UNC-NLP']"
96,180,Dataset,3.9806,2008,"{'2008': -0.0693, '2009': 0.4936, '2010': 1.0752, '2011': 0.786, '2012': 0.9006, '2013': 1.4046, '2014': 1.8564, '2015': 2.9058, '2016': 2.7549, '2017': 1.5801, '2018': 2.2009, '2019': 2.5902, '2020': 2.9274, '2021': 3.4963, '2022': 3.9806}","['English Wikipedia', 'English Wikipedia corpus', 'English Wikipedia data', 'English Wikipedia 2', 'English Wikipedia 5', 'English Wikipedia dataset', 'English Wikipedia 3', 'English Wikipedia 1', 'English Wikipedia 6', 'English Wikipedia ( EW )', 'English wikipedia', 'Wikipedia ( English )', 'English Wikipedia corpora', 'English Wikipedia 4', 'English-Wikipedia', 'English Wiki', 'English Wikipedia Corpus', 'Wikipedia English corpus', 'Wikipedia English', 'English Wikipedia data set', 'Large English Wikipedia', 'English Wikipedia ( SEW )', 'English Wikipedias', 'English Wikpedia', 'English Wikipedia corpus 7', 'English Wikipedia 7', 'English Wikipedia corpus 3', 'English Wikipedia Corpus 3', 'English WIKIPEDIA 3', 'English WIKIPEDIA', 'english Wikipedia', 'English WikiPedia', 'ENGLISH WIKIPEDIA', 'English Wiki dataset', 'Wikipedia ( English ) corpus', 'English Wikipedia Model', 'Wikipedia English dataset', 'English Wikipedia ( Wiki )', 'English Wikipedia-based', 'English Wikipeida 4', 'English-Wiki dataset v1.2', 'English Wikipedia 9', 'English Wikipedia ( Ewk ) corpus', 'English Wikipedia Corpus 2', 'English Wikipedia ( en', 'English WIKI-40B', 'English Wikipedia ( W eng )', 'WCL ( English Wikipedia ) corpus', 'English Wikipedia using', 'Wikipedia 1 English corpus', 'English Wikipedia 8', 'English Wikipedia 11', 'English Wikipedia based dataset 11']"
97,117,Method,3.9749,2017,"{'2017': 0.511, '2018': 2.2171, '2019': 3.9749, '2020': 2.8399, '2021': 3.5895, '2022': 2.59}","['adversarial training', 'Adversarial training', 'Adversarial Training', 'adversarial training method', 'adversarial training methods', 'adversarial training strategy', 'adversarial training ( AT )', 'adversarial training approach', 'adversarial training set', 'adversarial training framework', 'adversarial training algorithm', 'adversarial training model', 'adversarial training algorithms', 'adversarial trained model', 'Adversarial Training ( AT )', 'adversarial training data', 'adversarial training mechanism', 'Adversarial training ( AT )', 'adversarial training technique', 'Adversarial training methods', 'adversarial training approaches', 'adversarial training strategies', 'adversarial training techniques', 'adversarial training models', 'adversarially trained', 'Adversarial training algorithm', 'adversarial training architecture', 'adversarial training based approaches', 'adversarial-based training', 'Adversarial Training ( AdvT )', 'PGD-based adversarial training', 'adversatial training strategy', 'dversarial training', 'adversarial training ( AT ) model', 'task adversarial training', 'adversarial-training', 'Adversarial training strategies', 'Adversarial Training Set', 'Adversarial Training Data', 'Adversarial training technique', 'adversarial training network', 'Adversarial Training Methods', 'adversarial-training approach', 'Adversarial training techniques', 'adversarial model training', 'adversarial training Model', 'adversarial training based classifier', 'Adversarial training based approaches', 'adversial training', 'Adversarial training ( AT ) 1', 'adversarialy-trained', 'adversarially trained models', 'adversarially trained system', 'adversarially trained model', 'adversarial training ( A2T )', 'adverarial training', 'FGM-based adversarial training', 'adversarial training ( TAT )', 'adversarial traing', 'PGD adversarial training', 'PGD-based adversarial training method']"
98,950,Method,3.9193,2015,"{'2015': 0.5806, '2016': 2.6598, '2017': 3.9193, '2018': 2.366, '2019': 0.7983, '2020': 0.0289, '2021': -0.1464, '2022': -0.2203}","['Adadelta', 'AdaDelta', 'ADADELTA', 'Adadelta optimizer', 'AdaDelta optimizer', 'ADADELTA optimizer', 'Adadelta algorithm', 'adadelta optimizer', 'AdaDelta algorithm', 'adadelta', 'Ada Delta', 'ADADELTA method', 'Ada-Delta', 'adadelta algorithm', 'AdaDelta )']"
99,84,Dataset,3.8797,2009,"{'2009': -0.3105, '2010': 0.0995, '2011': 0.5184, '2012': 1.2717, '2013': 1.2645, '2014': 3.0572, '2015': 3.7541, '2016': 3.8797, '2017': 3.3204, '2018': 2.7459, '2019': 2.5268, '2020': 0.8738, '2021': 1.6376, '2022': 1.6851}","['Freebase', 'FreeBase', 'FREEBASE', 'freebase', 'Freebase data', 'Freebase 1', 'Freebase dataset', 'Freebase )', 'Freebase 4', 'In-Freebase', 'Freebase 3', 'freebase/', '/freebase', 'Freebase-based', 'Freebase-1', 'Freebase 1B', 'Freebase 13', 'freebase dataset', 'FREEBASE )', '/Freebase', 'Freebase FREEBASE', 'Freebase-Freebase', 'Freebase Freebase', 'F reebase', 'FREEBASE set', 'gold Freebase', 'FREEBASE 1', 'Freebase 1 )', 'Freebase 10', 'Freebase 8', 'Freebase 9', 'Freebase 11']"
100,204,Method,3.8146,2004,"{'2004': 0.3209, '2005': 0.3233, '2006': 0.0154, '2007': 0.8404, '2008': 1.0254, '2009': 1.5485, '2010': 1.4625, '2011': 1.8626, '2012': 1.6811, '2013': 2.5699, '2014': 2.6665, '2015': 3.8146, '2016': 2.5401, '2017': 1.5528, '2018': 2.2354, '2019': 2.2032, '2020': 1.9448, '2021': 1.8327, '2022': 2.1064}","['linear model', 'linear models', 'linear classifier', 'linear classifiers', 'linear', 'Linear', 'Linear model', 'linear methods', 'linear algorithm', 'Linear models', 'linear approach', 'linear method', 'Linear Model', 'Linear Models', 'linear network', 'structured linear model', 'linear system', 'linear classifier model', 'Linear-T', 'LinearT2', 'linear layer', 'LINEAR', 'linear model-based', 'Linear Measure', 'linear TS', 'LINEAR model', 'Linear Classifier', 'Linear Classifiers', 'linear layers', 'linear classification', 'structured linear models', 'linear algorithms', 'linear classification model', 'Linear-Large', 'linear measures', 'Linear-2', 'linear-classifier', 'Linear techniques', 'linear model technique', 'structured linear classifiers', 'Structured Linear Models', 'structured model linear models', 'linear model-based structured model', 'linear classifier-based model', 'linear classification models', 'based linear model', 'Linear system', 'Linear methods', 'linear model approach', 'linear )', 'linear layer classifier', 'linear architectures', 'linear modeling framework', 'linear networks', 'linear classifier-based methods', 'linear setting', 'linear modeling', 'Linear Linear', 'linear classification module', 'linear classifier modeling', 'Linear-Base model', 'Linear-Base', 'Linear-Large model', 'linear structure', 'linear measure', 'linear classification algorithm', 'linear analysis methods', 'base linear models', 'linear architecture', 'linear strategy']"
101,200,Method,3.7794,2003,"{'2003': 0.0069, '2004': 0.3804, '2005': 1.0116, '2006': 1.1921, '2007': 2.5006, '2008': 2.8138, '2009': 3.7794, '2010': 2.9615, '2011': 2.5969, '2012': 2.017, '2013': 3.5298, '2014': 2.0805, '2015': 1.6525, '2016': 1.5806, '2017': 0.5324, '2018': 0.8797, '2019': 0.1056, '2020': 0.0344, '2021': 0.0551, '2022': -0.0921}","['phrase-based system', 'phrase-based', 'phrase-based model', 'phrase-based models', 'phrase-based systems', 'phrase-based approach', 'C-PHRASE', 'phrase', 'phrase model', 'phrase models', 'Phrase-based models', 'Phrases', 'phrase-based approaches', 'Phrase-based', 'phrase-structure', 'phrase structure', 'phrase-based methods', 'Phrase', 'Phrase-based systems', 'CPHRASE', 'phrases', 's-phrases', 'phrase based models', 'phrase based system', 'Phrase-based approaches', 'C-PHRASE model', 'PHRASE', 'phrase-based modeling', 'phrase-based method', 'phrase-based framework', 'phrase-based architecture', 's-phrase', 'phrase based', 'Phrase-Based', 'phrase based model', 'Phrase-based model', 'phrase dataset', 'phrase structures', 'phrase method', 'phrase-', 'phrase-based ,', 'phrase-base systems', 'phrase-structure-based model', 'Phrase-based Classifier ( PC )', 'phrase-based classifiers ( PC )', 'phrase based approach', 'Phrase-based Model', 'Phrase models', 'phrase structure based analyses', 'phrase-structures', 'phrase structure analysis', 'phrase-structure analysis', 'phrase-based-models', 'Phrase-based Models', 'phrase–based models', 'Phrase structure', 'Phrase-structure', 'phrase–based system', 'phrase-based ) system', 'Phrase based systems', 'phrase based systems', 'phrase based approaches', 'phrase-base framework', 'Phrase-based modeling method', 'Phrase-based method', 'phrase based method', 'phrase Model', 'phrase systems', 'phrase structure corpora', 'phrase set', 'phrase-based )', 'phrasing', 'phrase based structures', 'base-phrase', 'phrase–phrase', 'phrased-based corpora', 'phrased-based model', 'phrase-based networks', 'phrase-based classifiers', 'phrase-value', 'Phrase-value', 'phrase-based technique', 'phrase framework', 'phrased-based )', 'phrase structure modeling', 'phrase structure ( PS )', 'Phrase structure ( PS )', 'C-Phrase', 'c-phrase model', 'C-PHRASE )']"
102,241,Method,3.7475,2005,"{'2005': -0.0303, '2006': 0.025, '2007': -0.1378, '2008': -0.0994, '2009': -0.25, '2010': 0.1497, '2011': 0.0082, '2012': -0.2126, '2013': -0.1064, '2014': -0.2982, '2015': 0.1716, '2016': 0.6933, '2017': 0.5539, '2018': 0.6833, '2019': 1.5081, '2020': 2.3796, '2021': 2.7548, '2022': 3.7475}","['QA', 'QA model', 'QA models', 'QA system', 'QA dataset', 'QA Model', 'QA systems', 'QA Models', 'QA data', 'QA-based method', 'QA-based approach', 'QA )', 'QA-S', 'QA corpus', 'qa', 'QA-based', 'QA-based model', 'QA-base', 'QA-based approaches', 'QA corpora', 'Qa', 'QA framework', 'QA-based metric', 'QA Systems', 'QA-based models', 'QA set', 'base QA system', 'QA-corpora', 'Qa dataset', 'QA network', 'QA method', 'value QA model', 'QA base', 'QA-base system', 'QA based system', 'Corpus QA', 'QA classifier', 'gold QA', 'QA-S dataset', 'QA-Based Metrics', 'QA approach', 'QA methods', 'base QA models', 'corpus-based QA system', 'QA-based methods', 'QA-based technique', 'QA strategy', 'QA metrics']"
103,101,Method,3.7423,2005,"{'2005': 0.1489, '2006': 0.6069, '2007': -0.1985, '2008': -0.177, '2009': -0.0021, '2010': 0.3656, '2011': 0.2913, '2012': 0.0945, '2013': 0.4471, '2014': 0.2591, '2015': 0.1615, '2016': 0.4273, '2017': 1.5625, '2018': 2.1965, '2019': 3.7423, '2020': 2.9174, '2021': 3.1356, '2022': 3.3971}","['RL', 'relevance', 'Relevance', 'relevance model', 'relevance models', 'relevance classifier', 'S-relevance', 'Relevance @ K', 'Srelevance', 'relevance modeling', 'Relevance Models', 'Relevance Model', 'S-relevance classification', 'S-relevance corpus', 'relvance', 'relevance 4', 'Relevance @ K.', 'relevance framework', 'Relevance model', 'relevance method', 'relevance mechanisms', 'Relevance @', 'S-relevance data', 'Relevance classifier', 'Relevance Classifier', 'relevance network', 'relevance theory', 'Relevance Theory', 'Relevance Set', 'relevance set', 'relevance based methods', 'relevance metrics', 'Relevance classification', 'relevance classification approach', 'relevance-based', 'Relevance-based', 'Relevance based', 'Relevance - measures', 'relevance measure', 'relevance module', 'Relevance Relevance', 'Relevance κ']"
104,196,Method,3.7324,2003,"{'2003': 0.743, '2004': 0.9607, '2005': 1.1813, '2006': 1.7836, '2007': 3.7324, '2008': 2.4217, '2009': 3.3672, '2010': 1.7818, '2011': 2.9299, '2012': 3.4245, '2013': 2.9786, '2014': 2.9998, '2015': 2.1865, '2016': 2.0708, '2017': 0.8602, '2018': 0.7481, '2019': 0.3845, '2020': 0.5749, '2021': 0.6431, '2022': 0.3617}","['n-gram language model', 'n-gram language models', 'n-gram LM', 'n-gram LMs', 'N-gram language models', 'N-gram language model', 'N-gram LM', 'n-gram language model ( LM )', 'n -gram LMs', 'n-gram language modeling', '-gram language models', 'g-gram LM', 'n -gram language models', 'N-gram LMs', 'n -gram language model', 'N -gram language model', 'N -gram language models', 'N -gram LM', 'n-gram based language model', '-gram language model', 'N-gram Language Model', 'n -gram LM', 'n-gram language models ( LMs )', 'n–gram language models', 'N-GRAM LMs', 'N–gram language model', 'N-gram language model ( LM )', 'n-gram language models ( LM )', 'n-gram based language models', 'N-gram based language models', 'n-gram language modelling', 'ggram LM', 'm-gram language modeling', 'm-gram language model', 'n gram language models', 'n-gram Language Model', 'N-gram language modeling', 'n-gram Language Modeling', 'n-gram based language model approach', 'n-grams 3-LM', 'gram language model', 'gram LM', 'uni-gram language model', 'n-gram language model 3', 'n− gram language models', '∞-gram language model', 'n-gram language model—we', 'n615 gram language model', 'n -gram language models—has', 'GT N-gram LM', 'm-gram LMs', 'm-gram LM', 'm-gram language models', 'n-gram Language models', 'n– gram language models', 'N -gram Language Models', 'n-gram Language Models', 'N-Gram Language Models', 'N-gram Language Models', 'n gram language model', 'n–gram language model', 'N gram language model', 'N –gram language model', 'N-Gram Language Model', 'n-gram Language Model ( LM )', 'n-grams language models', 'N-gram-based language model', 'n-gram-based language model', 'N-gram-based LMs', 'n-gram-based LMs', 'N-gram based LMs', 'n -gram language modeling', 'n-gram Language modeling', 'n-gram Language Models ( LM )', 'N-gram language models ( LMs )', 'n-gram-based language models', 'n-gram language', 'n-gram language model LM', 'n-gram LM based approaches', 'n-gram LM approach', 'n-gram language model model', 'n-gram based language modelling', 'N-gram based language models ( LM )', 'n-gram-based language classifier', 'N-grams LMs', 'n-grams language model', 'n-gram language model ( LM ) approach', 'n-gram language modeling techniques', 'n-gram language model (', 'large N-gram language model', 'LM 3gram Model', 'n-gram LM 5', 'n-gram LM 6', 'n-gram language model 8', 'Ney 5-gram LM', '-gram Language', 'gram language models', '-gram LMs', '-gram language modeling', 'The -gram language model', 'the -gram language model', 'uni-gram LM']"
105,288,Dataset,3.6939,2011,"{'2011': 0.1139, '2012': -0.1646, '2013': -0.1952, '2015': 0.095, '2016': -0.0838, '2017': -0.1446, '2018': 0.3466, '2019': 1.0967, '2020': 2.2091, '2021': 2.5548, '2022': 3.6939}","['GLUE', 'GLEU', 'GLUE tasks', 'GLUE dataset', 'GLUE dev set', 'glue rules', 'GLUE score', 'GLUE-SciTail', 'P-GLEU', 'GLUE dev', 'Glue', 'FS-GLUE', 'GLUE task', 'X-glue', 'GLUE dataset suite', 'GLUE test set', 'R-Glue', 'ABC glue rules', 'glue-rules', 'graph glue rules', 'HD-HR+Glue', 'GLUE-dev', 'General Language Understanding Evaluation ( GLUE )', 'GLUE suite', 'GLEU score', 'R-glue', 'glue grammars', 'X-Glue', 'GLEU 0', 'Generalized Language Evaluation Understanding ( GLEU )', 'GLUE 2', 'GLUE dev sets', 'glue sticks', 'inverted glue rules', 'inverted glue rule', 'GLEU-3', 'GLEU 3', 'GLEU scores', 'R-glue rule', 'GLEU 9', 'GLUE training tasks', 'X-glue rules', 'GLUE taskset', 'GLUE Tasks', 'GLUE task corpora', 'GLUE language understanding tasks', 'general language understanding ( GLUE ) tasks', 'GLUE 4 dev sets', 'SAE ( GLUE )', 'GLUE 10', 'Glue Rules', 'glue rule', 'GLUE training sets', 'GLUE spilt', '“ glue ” algorithm', 'glue grammar', 'GLEU +', 'GLEU+ metric', 'GLUE web site', 'Gleu', 'GLEU model', 'Generalized Language Evaluation Understanding Metric ( GLEU )', 'GLEU metric', 'GLEU metrics', 'GLUE ( General Language Understanding Evaluation ) 2', 'General Language Understanding Evaluation', 'GLUE ( General Language Understanding Evaluation )', 'Dataset GLUE', 'General Language Understanding Evaluation ( GLUE', 'GLUE paper', 'GLEU-4', 'GLEU + score', 'glue ” rules', 'GLEU M 2 Model']"
106,602,Method,3.6838,2020,"{'2020': -0.1033, '2021': 0.3742, '2022': 3.6838}","['XPROMPT', 'prompt', 'Prompts', 'prompts', 'prompt-based methods', 'prompt-based models', 'prompt-based model', 'Prompting', 'prompt-based', 'prompting', 'prompt model', 'Prompt', 'prompt-based approaches', 'prompt-based approach', 'prompt-based method', 'Prompt-based methods', 'Prompt-based approaches', 'PROMPTING', 'prompt-based techniques', 'prompt models', 'STRUCTURED-PROMPT', 'prompt methods', 'prompting-based methods', 'Prompt-T', 'Prompt-based models', 'prompted-based models', 'prompts )', 'Prompt-based techniques', 'prompt-based classifier', 'prompt-based framework', 'Prompt 8', 'PROMPT + +', 'prompts 2', 'XPROMPT approach', 'PROMPT', 'Prompt -based', 'Prompt-based', 'Prompt-based method', 'prompt-based architecture', 'prompt based techniques', 'Prompt-based Mechanism', 'Prompt-based mechanism', 'prompt-based technique', 'Prompt-based Approach', 'prompt approach', 'prompting mechanism', 'prompt mechanism', 'Prompting-Based Approaches', 'prompting-based approach', 'prompt techniques', 'prompting )', 'Prompting )']"
107,259,Method,3.6758,2019,"{'2019': -0.0042, '2020': 1.0033, '2021': 2.7541, '2022': 3.6758}","['SBERT', 'Sentence-BERT', 'sentence-BERT', 'S-BERT', 'Sentence-BERT ( SBERT )', 'SBERT model', 'SBERT-WK', 'SBERT models', 'Sentence-BERT model', 'Sentence-Bert', 'SBERT-base', 'sBERT', 'SBERT base', 'Person/SBERT-WK', 'SBERT embeddings', 'BERT-based sequence labeling model', 'BSL-SBERT', 'SBERT/SRoBERTa', 'SBert', 'sentence-BERT model', 'SBERT BASE', 'SBERT framework', 'SBERT *', 'SBERT mpnet', 'SBERT para-mpnet', 'SBERT base -whitening', 'SBERT base -flow', 'SBERT-para', 'BERT-based sequence tagging model', 'BERT sentiment classifier', 'sentence-BERT embeddings', 'BERT sentence classification model', 'Sentence-bert', 'sentence-BERT ( SBERT )', 'sentence-based BERT model', 'Sentence-BERT models', 'BERT ( SBERT )', 'SBERT architecture', 'Sentence-BERT encoder', 'POWER-BERT Sequence', 'CT-SBERT base', 'SBERT-AFS', 'SBERT-AFS-base', 'SBERT-AFS-large', 'DSSCC SBERT', 'BERT-based sequence classifier', 'BERT-based sequence classification model', 'BERT sequence classification model', 'BERT sequence classifier models', 'SentenceBERT ( SBERT )', 'SBERT ( SentenceBERT )', 'Sentence-BERT embeddings', 'SBERT-only', 'BERT-based sequence tagger', 'Our-S-BERT ( all )', 'sbert', 'SENTENCE-BERT', 'Sentence BERT', 'sentence-bert', 'BERT sentence classifier', 'Sentence BERT ( SBERT )', 'SENTENCE BERT ( SBERT )', 'sentence BERT ( SBERT )', 'S-BERT model', 'sBERT model', 'Sentence-BERT-base', 'Sentence-BERT base', 'S BERT-large', 'SBERT-large', 'sentence transformers ( SBERT )', 'sentence-transformers ( sbert )', 'S-BERT models', 'Sentence-BERT-based methods', 'SBERT methods', 'BERT/SBERT-based models', 'SBERT method', 'SBERT )', 'S-BERT ( base )', 'S-Bert transformer', 'Sentence-BERT ( SBERT', 'Sentence-BERT Sentence-BERT', 'BERT sentence model', 'BERT SBERT', 'Sentence-BERT ( SBERT ) model', 'BERT/SBERT models', 'SBERT base model', 'bert sequential model', 'BERT-based sequential classifier', 'SBERT too', 'KS-SBERT', 'SBERT-base - smart batching', 'sentence-BERT-based encoder model', 'BERT Sentence Encoder', 'SBERT ( node )', 'SimCSE-SBERT base', 'SimCSE-SBERT', 'ArcCSE-SBERT base', 'ArcCSE-SBERT', 'Info-Sentence BERT ( IS-BERT )', 'BERT based sequence labeling model', 'BERT-based sequence labeling models', 'SBERT +', 'SBERT-First ( SB-F )', 'SBERT-MaxP ( SB-M )', 'SBERT-MaxP', 'Cosine ( Sentence BERT )', 'SBERT cosine', 'K-means SBERT', 'Kmeans SBERT', 'Info-Sentence BERT', 'Sentence-BERT 6', 'SBERT mpnet )', 'SBERT para-mpnet )', 'SBERT sel', 'Sentence-Bert ( SENTBERT )', 'sBERT-based retrieval', 'BERT-induced sentence', 'S-BERT Retriever', 'BERT-based Sentence Compression', 'sequence-based BERT', 'BERT sequence', 'BERT-based sequence', 'sequence bert large cased model', 'SBERT ( SRoBERTa ) model', 'SBERT base +UMAP', 'BERT-based evidence sentence classifier', 'SBERT training', 'SBERT training method', 'SBERT 1', 'SBERT-WikiSec-base', 'SBERT-WikiSec-large', 'BERT-based sentence embedding', 'BERT sentence embedding ( BERT )', 'BERT sentence embedding model', 'CELMO ( C-SBERT )', 'SBERT-flow', 'SBERT ( MiniLM-L6 ) - FT ( C )', 'BERT-based sequence tagging models', 'BERT sequence tagging model', 'BERT sentiment classifier ( BERT-SA )', 'BERT-based sentence-level classifier', 'BERT transformer-based sentence level classifier', 'Sentence-BERT 3', 'BERT-like sentence', 'S-BERT score', 'BERT-like sequence models', 'English SBERT model', 'SBERT 10', 'SBERT FE ST+C', 'SBERT Embeddings ( SEmb )', 'Sentence- ( Ro ) BERT ( a ) -base models', 'M SBERT', 'BERT sentiment analysis model', 'BERT sentiment model', 'BERT sentiment classifiers', 'SBERT-based LexRank ( SLR )', 'SBERT-based clustering ( SC ) method', 'SBERT ( Embeddings )', 'SBERT-WK TextText', 'BSL-SBERT ( BSL-SRoBERTa )', 'KAT SBERT', 'SBERT 7', 'BERT-based binary sequence classifier', 'BERT binary sequence classifier', 'SentenceSentence-BERT', 'SBERT+LGBM', 'BERT-based Sequence Tagger', 'BERT-based sequence taggers', 'SBERT 4', 'Sentence-BERT ( SBERT ) 4']"
108,226,Method,3.672,2008,"{'2008': -0.0822, '2009': -0.2846, '2010': 0.0403, '2011': -0.1599, '2012': 0.0785, '2013': 0.7226, '2014': 0.9314, '2015': 2.2429, '2016': 3.3154, '2017': 3.2618, '2018': 3.672, '2019': 2.6366, '2020': 1.5319, '2021': 1.4024, '2022': 1.3123}","['SGD', 'SGD optimizer', 'SGD dataset', 'SGD algorithm', 'π -SGD', 'SGD method', 'SGD-X', 'SGD )', 'Schema-Guided Dialogue ( SGD )', 'schema-guided dialog', 'Merge SGD', 'sgd', 'schema-guided dialogue dataset', 'SGD corpus', 'SGD data', 'Schema Guided Dialogue ( SGD ) dataset', 'SGD 2', 'Sgd-x', 'Schema-Guided Dialogue', 'schema-guided dialogue', 'Schema-guided Dialogue', 'sgd )', 'Schema-Guided dialogue dataset', 'Schema Guided Dialogue dataset', 'Schema-Guided Dialogue dataset', 'Schema-Guided Dialogue ( SGD ) corpus', 'Schema-Guided-Dialogue ( SGD )', 'Schema-guided Dialogue ( SGD )', 'Schema Guided Dialogue ( SGD )', 'data–SGD', 'Schema-guided dialogues ( SGD )', 'Schema-Guided Dataset ( SGD )', 'Meta-SGD', 'Meta-sgd', 'Sgd-qa', 'SGD-like', 'Schema-Guided Dialog dataset', 'schema guided dialogs', 'Schema-Guided Dialog', 'schema guided dialog', 'schema-guided dialog modeling', 'Adam-SGD', 'Adam SGD optimizer', 'sync-sgd']"
109,262,Method,3.644,2014,"{'2014': 0.3884, '2015': 0.5484, '2016': 0.5337, '2017': 1.0709, '2018': 1.3234, '2019': 2.1601, '2020': 2.3404, '2021': 2.8179, '2022': 3.644}","['t-SNE', 'T-SNE', 'tSNE', 't-SNE plots', 't-SNE plot', 'TSNE', 't-SNE algorithm', 't-sne', 't-Distributed Stochastic Neighbor Embedding ( t-SNE )', 't-SNE method', 'tSNE plot', 't -SNE', 't-distributed Stochastic Neighbor Embedding ( t-SNE )', 't-distributed stochastic neighbor embedding ( tSNE )', 't-SNE model', 't-SNE )', 'tSNE plots', 't -SNE plots', 't-sne plots', 'T-SNE algorithm', 't-Distributed Stochastic Neighbor Embedding', 't-SNE technique', 'ConAE-128 ( t-SNE )', 'ConAE-64 ( t-SNE )', 't-sne plot', 'TSNE plot', 'TSNE plots', 'T-SNE plots', 't-SNE Plots', 't-SNE plots-', 't-SNE graph', 't-SNE ( Maaten', 't-SNE 8', 't-SNE toolkit', 'T-SNE toolkit', 'Local t-SNE', 't-SNE tool', 't-SNE 4', 'T-SNE clustering', 't-SNE clustering', 't-SNE Cluster', 't-SNE clustering algorithm', 't-SNE toolkit 4', 'tsne', 'T-Sne', 't-distributed stochastic neighbor embedding ( T-SNE )', 'T-Distributed Stochastic Neighbor Embedding ( t-SNE )', 't-Distributed Stochastic Neighbor Embedding ( tSNE )', 't-distributed stochastic neighbor embedding ( t-SNE )', 't-distributed stochastic neighbor embedding ( t-SNE ) method', 'tSNE algorithm', 't-distributed Stochastic Neighbor Embedding', 't-SNE analysis', 't-Distributed Stochastic Neighbor Embedding ( t-SNE ) technique', 't-Distributed Stochastic Neighbor Embedding ( tSNE ) technique', 't-SNE-based', 't-SNE embedding', 'tSNE methods', 't-SNE models', 't-SNE (', 't-SNE embedding method', 't-SNE method ( t-distributed Stochastic Neighbor Embedding )', 'TSNE 5', 't-Distributed Stochastic Neighbor Embedding ( tSNE ) 5 technique']"
110,355,Method,3.6354,2018,"{'2018': 0.5549, '2019': 1.8821, '2020': 2.9584, '2021': 3.6354, '2022': 3.1705}","['attention heads', 'attention head', 'attention head masking', 'Attention Heads', 'head attention', 'attention-heads', 'Attention Head', 'attention-head', 'attention head architecture', 'K-head attention', 'Attention Heads ( MoA )', 'attention head masking technique', 'p -head attention mechanism', 'attention head 9', 'head-attention', 'heads attention', 'attention-over-heads', 'attentionhead', 'attention head analysis', 'attention head head', 'attention heads analysis', 'attention heads-based approach', 'attention head approach', 'attention heads (', 'attention heads module', 'h -head attention', 'attentional heads', 'N -headed attention', 'N -head attention', 'K-head attention networks', 'M -head attention', 'M -head attention model', 'attention head ’ s']"
111,394,Dataset,3.5803,2005,"{'2005': 0.1064, '2006': 0.0538, '2007': 0.412, '2008': 0.0686, '2009': 0.6147, '2010': -0.1341, '2011': 0.2228, '2012': 0.6415, '2013': 1.3812, '2014': 0.9486, '2015': 1.0481, '2016': 0.8719, '2017': 0.431, '2018': 1.522, '2019': 1.2967, '2020': 2.5983, '2021': 2.9706, '2022': 3.5803}","['task', 'tasks', 'Task', 'task model', 'Tasks', 'task data', 'Task Dataset', 'task dataset', 'TASK', 'task models', 'Task Model Method', 'SS task', 'task corpus', 'task-model', 'Task-based systems', 'task-based systems', 'task-data', 'task classifier', 'task method', 'Classification-for-Modeling task', 'base task', 'in-task', 'task set', 'metrics task data', 'Task Models', 'Dataset Metric Task Method', 'Task Data Model', 'base task model', 'task framework', 'Task Model', 'task measure', 'task-based approaches', 'Task Data', 'metric task', 'task-dataset', 'task algorithms', 'task-based methods', 'big task', 'task Corpus', 's metric task', 'Task Method', 'task methods', 'Classification-to-Modeling task', 'Dev set task', 'task architectures', 'In-task', 'task-based system', 'Large Task', ') task', 'Classification Tasks', 'classification task', 'Classification task', 'task dev sets', 'task-based', 'tasks models', 'Data Task Data', 'Tasks Methods', 'task corpora']"
112,539,Tool,3.563,2016,"{'2016': 0.3, '2017': 3.416, '2018': 3.563, '2019': 2.1902, '2020': 1.222, '2021': 1.335, '2022': 0.4045}","['TensorFlow', 'Tensorflow', 'tensorflow', 'TensorFlow framework', 'TENSORFLOW', 'TensorFlow 1', 'Tensorflow framework', 'TensorFlow 2', 'Tensorflow 1', 'Tensorflow 3', 'Tensorflow 2', 'TENSORFLOW 2', 'TensorFlow models', 'TensorFlow model', 'Tensorflow 2.1', 'TensorFlow 7', 'TensorFlow 8', 'TensorflowJS', 'TensorFlow 6', 'Tensorflow 6', 'TensorFlow 5', 'Tensorflow 5', 'tensorflow 5', 'TensorFlow 2.0', 'Tensorflow 2.0', 'TensforFlow', 'TensorFlow 2.3', 'Tensorflow 16', 'Tensorflow-1.7', 'TensorFlow 9', 'Tensorflow 10', 'TensorFlow 10', 'TensorFlow 1.0', 'TensorFlow2', 'TensorFlow 2 framework', 'Tensorflow framework 2', 'TensorFlow frameworks', 'TensorFlow-based framework', 'TensorFlow package', 'Tensorflow approaches', 'tensorflow package 3', 'TensorFlow 3']"
113,120,Metric,3.5419,2003,"{'2003': -0.4175, '2004': -0.2496, '2005': -0.299, '2006': -0.3203, '2007': -0.1378, '2008': -0.2977, '2009': 0.1132, '2010': 0.0906, '2011': -0.2034, '2012': -0.0239, '2013': 0.3373, '2014': 0.1299, '2015': 0.8929, '2016': 0.4501, '2017': 0.1949, '2018': 0.448, '2019': 0.6186, '2020': 1.3122, '2021': 2.5218, '2022': 3.5419}","['BM25', 'BM25 model', 'bm25', 'BM-25', 'BM25 algorithm', 'BM25 system', 'BM25 )', 'BM 25', 'BM25-based', 'BM25 method', 'BM25 approach', 'bm-25', 'BM25-based methods', 'BM25-', 'BM25 (', 'BM25 -', 'BM-25 method', 'BM25 Model', 'BM25-based algorithms', 'BM25 models', 'BM 25 models', 'BM25 methods', 'BM25 Methods']"
114,170,Metric,3.492,2005,"{'2005': -0.2707, '2006': 0.2648, '2007': 0.0764, '2008': 0.2841, '2009': 0.2054, '2010': 0.6258, '2011': 0.366, '2012': -0.0686, '2013': 0.384, '2014': 0.7159, '2015': 1.306, '2016': 0.9137, '2017': 0.9772, '2018': 2.3951, '2019': 2.2602, '2020': 2.1699, '2021': 2.795, '2022': 3.492}","['ROUGE-1', 'Rouge-1', 'ROUGE1', 'ROUGE 1', 'ROUGE-1 ( R1 )', 'ROUGE-1 ( R-1 )', 'Rouge1', 'ROUGE-1 models', 'ROUGE -1', 'ROUGE–1', 'ROUGE-1 System', 'ROUGE1-R', 'Rouge-1 ( R-1 )', 'ROUGE-1 ( R1', 'R-1 ( ROUGE-1 )', 'Rouge 1', 'ROUGE- 1', 'ROUGE_1', 'ROUGE - 1', 'ROUGE=1', 'ROUGE-1 method', 'ROUGE-1 (', 'ROUGE-1 1', 'ROUGE 1 measure', 'ROUGE-1 measure', 'Rouge-1R', 'ROUGE 1-R', 'ROUGE1- R']"
115,75,Method,3.4854,2002,"{'2002': -0.3695, '2003': -0.2981, '2004': 0.0877, '2005': 0.4223, '2006': 0.5174, '2007': 1.1903, '2008': 0.8185, '2009': 1.7618, '2010': 1.8617, '2011': 2.46, '2012': 1.8027, '2013': 3.4854, '2014': 2.3333, '2015': 2.2792, '2016': 2.0575, '2017': 1.4708, '2018': 1.2381, '2019': 0.8511, '2020': 0.7298, '2021': 1.1988, '2022': 1.2695}","['PMI', 'pointwise mutual information ( PMI )', 'pointwise mutual information', 'Pointwise Mutual Information ( PMI )', 'pmi', 'Pointwise Mutual Information', 'SO-PMI', 'PMI DC', 'Max-PMI', 'PMI model', 'PMI-IR', 'PMI++', 'PMI sig', 'PMI-SO', 'PMI 2', 'Pointwise mutual information', 'PMI-based', 'PMI-Masking', 'PMI method', 'PMI measure', 'PMI-WLDA', 'PMI-based method', 'SO-PMI method', 'Pointwise mutual information ( PMI )', 'PMI-cos', '∆PMI', 'PMI matrix', 'SO-PMI algorithm', 'DR-PMI', 'pointwise mutual information ( pmi )', 'pointwise mutual information ( PMI', 'PMI-based measure', 'PMI + T', 'pmi En', 'PMI ( pointwise mutual information )', 'Pointwise Mutual Information ( PMI', 'PMI-based approach', 'Ours-PMI', 'PMI )', 'SOC-PMI', 'Neg-PMI', 'P-PMI', 'pointwise-mutual information', 'PMI models', 'PMI corpus', 'PMI analysis', '∆PMI measure', 'PMI + L', 'PMI-IR algorithm', 'Google PMI', 'GM ( PMI )', 'Pointwise Mutual Information ( I )', 'PMI EX', 'PMI Masking', 's PMI', 'PMI measures', 'PMI-based methods', 'PMI (', 'PMI system', 'pmi methods', 'PMI methods', 'PMI ( S )', 'PMI-based models', 'PMI - rel', 'PMI - abs', 'PMI GloVe', 'Glove PMI', 'MICR : pmi', 'W-PMI2', 'W-PMI', 'N-gram PMI', 'top-PMI', 'PMI 1', 'Web-PMI', 'pmi T', 'PMI rules', 'h PMI', 'PMI ( SLP )', 'PMI-based SLP', 'pmi+wadd', 'pmi + wadd', 'SO-PMI )', 'SO-PMI Algorithm', 'discr PMI', 'pmi3', 'mation ( PMI )', 'PMI 5', 'PMI n', 'PMI Mask', 'PMI 14', 'PMI/Local', 'local PMI model', 'PMI test', 'Pointwise-Mutual Information', 'pointwise mutual-information', 'Pointwise Mutual information', 'Pointwise_mutual_information', 'pointwise mutual information , PMI )', 'PMI ( Pointwise Mutual Information )', 'Pointwise Mutual information ( PMI )', 'pointwise mutual information ( PMI ) )', 'Pointwise-Mutual Information ( PMI )', 'Pointwise Mutual Information [ PMI ]', 'PMI ( Pointwise mutual information )', 'PMI based approaches', 'PMI-based approaches', 'Pointwise mutual information ( PMI', 'PMI-Pointwise Mutual Information', 'pmi pmi', 'PMI dataset', 'PMI Analysis', 'pointwise mutual information measure', 'Pointwise Mutual Information measure ( PMI )', 'PMI ( pointwise mutual information ) approach', 'pointwise mutual information ( PMI ) measure', 'pointwise mutual informations', 'PMI-model', 'PMI data', 'pmi-based classifier', 'pointwise mutual information ( PMI ) methods', 'PMI based method', 'pmi )', '* PMI approaches', 'PMI-based analysis', 'mutual information ( PMI )', 'The -pmi methods', '-pmi', '-pmi methods', 'PMI approach', 'pmi approach', 'Approach - PMI', 'PMI-based technique', 'pointwise mutual infor mation', 'PMI Point mutual information ( PMI )', 'PMI corpus-based measure', 'pointwise mutual information metric', 'PMI ( Ori . )', 'PMI ( OP )', 'PMI ( -LRE )', 'LRE ( -PMI )', 'PMI-age', 'PMI LLR', 'L1 pmi', 'PMI - 23.3', 'All-PMI-X', 'PMI task', 'CU-2B PMI', 'All-PMI-MAX', 'All-PMI3MAX', 'mwetk : pmi', 'pointwise mutual information ( CPMI )', 'PMI Following', 'pmi graph', 'DR-PMI64', 'GLSA-cos-pmi', 'PMI ( G )', 'max pmi', 'pointwise mutual information ( 4 )', 'PMI-based learning model', 'PMI + T model', 'PMI W', '∆PMI )', 'PMI + L model', 'PMI > 0', 'PMI ♥ - - 82', 'pointwise mutual information ( PMI-IR )', 'PMI-IR approach', 'poitwise mutual information', 'd PMI', 'pointwise mutual information ( PMI ) 2', 'Pointwise Mutual Information ( PMI ) 1', 'L2 PMI ( pmi En )', 'half-pointwise mutual information', 'pmi-like', 'Pointwise Mutual Information ( Web-PMI )', 'Web-PMI )']"
116,374,Method,3.3624,2018,"{'2018': -0.1424, '2019': 0.6007, '2020': 2.0214, '2021': 2.7326, '2022': 3.3624}","['CLS', 'cls', 'CLS ]', '[ CLS ]', 'CLS model', 'CLS dataset', 'CLSS models', 'CLs', 'Cls', '-CLS', 'CLS methods', 'CLS models', 'CLS systems', 'L & C', 'CLS theory', 'C & L', 'BASE_CLS', 'cls (', 'CLS ] )', 'CLS )', 'Cls.', 'L cs', 'classification ( CLS )', 'CLS-IN', 'CS & L', 'CLSS model', 'classification model ( CLS )', 'CLS corpora', 'CLS corpus', 'CLS data', '[ CLS ] -based methods', '[ CLS ] techniques', '[ CLS ] Model']"
117,55,Method,3.3381,2001,"{'2001': 0.7403, '2002': 1.7855, '2003': 1.4591, '2004': 2.186, '2005': 1.2426, '2006': 2.4614, '2007': 2.6256, '2008': 2.3786, '2009': 2.1855, '2010': 3.0, '2011': 2.9361, '2012': 3.3381, '2013': 1.5751, '2014': 1.8851, '2015': 1.9407, '2016': 0.8928, '2017': 0.2144, '2018': 0.5074, '2019': 0.2194, '2020': 0.2914, '2021': 0.205, '2022': 0.487}","['PCFG', 'PCFGs', 'PCFG model', 'L-PCFG', 'PCFG-LA', 'L-PCFGs', 'C-PCFG', 'probabilistic context-free grammar', 'PCFG-LA parser', 'probabilistic context-free grammars', 'probabilistic context-free grammar ( PCFG )', 'PCFG models', 'PCFG-LA model', 'TN-PCFG', 'VC-PCFG', 'MMC-PCFG', 'probabilistic context-free grammars ( PCFGs )', 'PCFG-LA models', 'TN-PCFGs', 'pcfg', 'PCFG-based', 'TD-PCFGs', 'N-PCFG', 'probabilistic context free grammar ( PCFG )', 'NL-PCFG', 'PTC-PCFG', 'C-PCFGs', 'Probabilistic Context-Free Grammars ( PCFGs )', 'PCFG parser', 'PCFG-p', 'PCFG-I', 'NoWo-PCFG', 'PCFG-s', 'probabilistic contextfree grammars', 'TD-PCFG', 'probabilistic context free grammar', 'PCFG dataset', 'NL-PCFGs', 'PCFG parsers', 'Gibbs PCFG', 'NBL-PCFGs', 'PCFG approach', 'PA-PCFG', 'NBL-PCFG', 'Probabilistic Context Free Grammar ( PCFG )', 'N-PCFGs', 'Gibbs-PCFG', 'Probabilistic context-free grammars ( PCFGs )', 'probabilistic context free grammars ( PCFGs )', 'Probabilistic Context-Free Grammar ( PCFG )', 'probabilistic context free grammars', 'PCFG-based approach', 'C-PCFG ( W )', 'HL-PCFGs', 'VC-PCFGs', 'L-PCFG model', 'SC-PCFGs', 'PCFG-I model', 'Probabilistic Context Free Grammars ( PCFGs )', 'probabilistic context-free grammars ( PCFG )', 'PCFG )', 'PCFG-based model', 'PCFG models ( PCFG', 'MCLE PCFG', 'PCFG-LA parsing', 'PCFG-LA parsers', 'DEP-PCFG', 'tb-pcfg', 'probabilistic context free grammars ( PCFG )', 'probabilistic CFG', 'Probabilistic Context-Free Grammar', 'probabilistic CFG ( PCFG )', 'Probabilistic context-free grammars', 'probabilistic contextfree grammar', 'PCFG-based approaches', 'PCFG parsing model', 'PCFG parsing algorithm', 'PCFG-LA parsing models', 'C-PCFG (', 'PCFG-based parser', 'P-PCFG', 'MMC-PCFG model', 'C-PCFG ( WP )', 'DB-PCFG', 'PTC-PCFG model', 'Probabilistic Context Free Grammars ( PCFG )', 'Probabilistic context-free grammars ( PCFG )', 'Probabilistic Context Free Grammar', 'probabilistic CFGs ( PCFGs )', 'Probabilistic Context-free Grammar ( PCFG )', 'probabilistic context-free grammar model', 'PCFG-based models', 'PCFG-based architecture', 'PCFG grammar', 'context-free grammar ( PCFG )', 'PCFG algorithms', 'MCLE PCFGs', 'PCFG parser 2', 'PCFG-like models', 'PCFG tagger', 'SHRG–PCFG', 'PTSG/PCFG', 'PFSA/PCFG', 'PCFG-LA parsing model', 'PCFG-LA Parsing', 'PCFG-LA parsing method', 'PCFG-LA parsing algorithm', 'SUP-PCFG', 'PCFG ( C-PCFG ) model', 'C-PCFG model', 'PCFG 2', 'probabilistic context-free-grammar parser', 'probabilistic context-free grammar parser', 'PCFG-based parsers', 'Probabilistic Context-Free Grammar ( PCFG ) parser', 'PCFG-rules', 'PCFG P-PCFG', 'RR ) PCFG models', 'history-based PCFG approach', 'MMC-PCFG approach', 'SP-PCFG', 'PCFG-only', 'infinite PCFG model', 'probabilistic contex t-free grammar', 'PCFG-PA', 'PTC-PCFG approach', 'NoWo-PCFG model', 'Tied Probabilistic Context Free Grammars', 'PCFGS/PPDAs', 'LA-PCFG', 'PCFG-LA )', 'PCFG-LA method', 'probabilistic context-free-grammars ( PCFGs )', 'Probabilistic Context-Free Grammars ( PCFG )', 'Probabilistic context-free grammar', 'PCFGs—', 'PCFG (', 'PCFG-based methods', 'PCFG ( Probabilistic Context Free Grammar )', 'PCFG ( Probabilistic Context-Free Grammar )', 'PCFG Model', 'PCFG ) Models', 'Probabilistic Context Free Grammars', 'PCFG ( s )', 'Probabilistic ContextFree Grammar', 'PCFG corpus', 'PCFGs—the', 'probabilistic context-free grammar ( PCFG ) approach', 'probabilistic context-free grammar ( PCFG', 'PCFG grammars', 'PCFG method', 'PCFG approaches', 'PCFG-S model', 'PCFG-TRANS', 'PCFG +', 'TD-PCFGs ( TN-PCFGs )', 'PCFG-type', 'A-PCFG', 'PCFG 1', 'Probabilistic context-free grammar ( 1 )', 'PCFG ( G )', 'PCFG parsing methods', 'PCFG-based parsing', 'PCFG parsing approaches', 'PCFG parsing algorithms', 'PCFGs ( ca', 'HL-PCFG', 'HL-PCFG-based', 'probabilistic phrase structure grammars ( PCFGs )', 'PCFG-preferred', 'PCFG-like model', 'PCFG-like', 'M2C SHRG–PCFG', 'VC-PCFGs )', 'VC-PCFG model', 'PCFG†', 'C-PCFG 3', 'PCFG + M5', 'PCFGs ( L-PCFGs )', 'L-PCFG algorithm', 'L-PCFG models', 'SHRG– PCFG', 'PCFG-scores', 'PCFG-score', 'grounded PCFG model', 'factored PCFG', 'Factored PCFGs', 'PCFG ( TN-PCFG )', 'TN-PCFG model', 'PCFG 4']"
118,132,Method,3.2987,2016,"{'2016': -0.213, '2017': 0.1695, '2018': -0.0377, '2019': 0.4609, '2020': 1.3797, '2021': 2.7849, '2022': 3.2987}","['KD', 'knowledge distillation', 'Seq-KD', 'knowledge distillation ( KD )', 'Knowledge distillation', 'Knowledge Distillation ( KD )', 'Knowledge Distillation', 'KD methods', 'Knowledge distillation ( KD )', 'Meta-KD', 'KD method', 'MATE-KD', 'KD approaches', 'FiD-KD', 'knowledge distillation method', 'KD techniques', 'KD+STR', 'KD+R', 'KD M', 'KD approach', 'KD framework', 'IaM-KD', 'Meta-KD framework', 'mutual knowledge distillation', 'knowledge distillation approach', 'KD data', 'IrM-KD', 'knowledge distillation methods', 'knowledge distillation framework', 'KD-based methods', 'KD+R+K', 'PS-KD', 'knowledge distillation technique', 'KD algorithm', 'TAMT-KD', 'kd', 'knowledge distillation approaches', 'KD models', 'KD B', 'KD technique', 'KD )', 'knowledge distillation models', 'Seq-KD model', 'kd-tree', 'KD-tree', 'KD+', 'No-KD', 'UDA+KD', 'knowledge distillation techniques', 'knowledge distillation strategies', 'KD algorithms', 'SEQ-KD', 'Seq-KD data', '−→ KD B', 'kd-trees', 'KD-Trees', 'KD-Pre', 'KD-4', 'MTN-KD', 'kdθk 2', 'Meta-Knowledge Distillation ( Meta-KD ) framework', 'Meta-KD method', 'knowledge distillation ( KD ) technique', 'knowledge-distillation', 'knowledge distillation model', 'knowledge distillation based methods', 'KD-based model', 'KD strategies', 'knowledge distillation based strategy', 'KD-based method', 'KD strategy', 'G -KD', 'G-KD', 'KD-based DG approaches', 'KD 17', 'gold-only KD', 'kd sa', 'KD P C', 'KD studies', 'KD 6 9M', 'Rep-KD', 'SEED-KD', 'Mate-KD', 'MATE-KD )', 'MATE-KD technique', '+ Seq-KD', 'KD-QAT', 'KD-un', 'KD-up', 'KD †', 'KD-tree data structure', 'kd tree method', 'KD+STR models', 'PTQ → KD', 'Fid-KD model', 'FiD-KD model', 'L KD', 'Knowledge Distillation ( L KD )', 'KD 4', 'KD setup', 'KD ( 3 )', 'KD & LRS', 'KD-sim )', 'KD-sim', '−→ KD data', '−→ KD', '←− KD', 'MaskT ( KD )', 'KD→', 'Meta-KD methods', 'Knowledge distillation methods', 'KD-Methods', 'Knowledge distillation approach', 'SMALL ( KD )', 'KD ( SMALL )', 'KD- *', 'KD-data', 'KD S', 'knowledge distillation framework ( -kd )', 'knowledge distillation method ( KD )', 'KD model', 'KD-based models', 'Knowledge distillation based models', 'Knowledge distillation techniques', 'KD Techniques', 'SOTA knowledge distillation methods', 'KD Strategies', 'Knowledge distillation algorithms', 'KD frameworks', 'knowledge distillation-based', 'knowledge distillation based', 'knowledge distillation ( KD ) models', 'knowledge distillation ( KD ) model', 'knowledge distillation ( KD ) methods', 'knowledge-distillation-based algorithms', 'Knowledge Distillation ( KD ) techniques', 'Knowledge Distillation ( KD ) method', 'knowledge distillation-based method', 'Knowledge distillation mechanism', 'knowledge distillation mechanism', 'S-KD', 'knowledge distillation ( KD ) mechanism', 'KD analysis', 'KD systems', '←− KD ”', 'KD ( ←− KD M )']"
119,366,Tool,3.2967,2006,"{'2006': 0.0218, '2007': 0.2121, '2008': 0.7668, '2009': 1.8367, '2010': 2.6452, '2011': 2.5845, '2012': 2.3657, '2013': 3.1841, '2014': 2.7355, '2015': 3.2967, '2016': 1.7972, '2017': 0.907, '2018': 0.7653, '2019': 0.404, '2020': 0.3643, '2021': 0.0436, '2022': -0.09}","['Stanford parser', 'Stanford Parser', 'Stanford parser 1', 'Stanford Parser 1', 'Stanford Parser 4', 'Stanford parsers', 'Stanford parser 5', 'Stanford parser 2', 'Stanford Parser 2', 'Stanford ’ s parser', 'Stanford parser 3', 'Stanford Parser 3', 'Stanford parser 4', 'Stanford parser 6', 'Stanford Parsers', 'Stanford parsing', 'Stanford Parser 5', 'CVG Stanford parser', 'Stanford parser v3.6', 'Stanford Parser 6', 'stanford parser', 'Stanford parser 9', 'Stanford parser 8', 'Stanford SG Parser', 'Standford Parser 4', 'Stanford parser ( prs )', 'Standford Parser 5', 'Standford Parser', 'Stanford Zh parser', 'Stanford parser 7', 'Stanford Parser 7', 'Stanford Parser 12', 'S tanford parser systems']"
120,147,Dataset,3.2733,2013,"{'2013': -0.3119, '2014': -0.1862, '2015': 0.0426, '2016': 0.2848, '2017': 0.191, '2018': 0.9747, '2019': 2.0171, '2020': 2.2848, '2021': 2.7907, '2022': 3.2733}","['Reddit', 'Reddit dataset', 'Reddit data', 'reddit', 'Reddit corpus', 'REDDIT', 'Reddit Corpus', 'Reddit 1', 'Reddit model', 'Reddit Dataset', 'Reddit 3', 'Reddit )', 'Reddit 13', 'Reddit corpora', 'Reddit Model', 'reddit data', 'Reddit data set', 'Reddit models', 'Reddit set', 'Reddit-based model', 'Reddit Data', 'REDDIT data', 'reddit dataset', ""Reddit '' dataset"", 'Reddit -', 'reddit )', 'Reddit-based dataset', 'Reddit-based models', 'Reddit Reddit data', 'reddit_network', 'Reddit Network Corpus', 'Reddit Method', 'REDDIT 1', 'Reddit dataset 1', 'Reddit Small 3', 'Reddit - Reddit 3', 'Reddit data 3']"
121,523,Method,3.2366,2016,"{'2016': 0.0663, '2017': 1.4357, '2018': 2.6704, '2019': 3.2366, '2020': 2.2574, '2021': 1.4433, '2022': 0.9966}","['GloVe embeddings', 'Glove embeddings', 'GLOVE embeddings', 'GloVE embeddings', 'glove embeddings', 'Glove Embeddings', 'GloVe embeddings 2', 'GLoVe embeddings', 'fixed GloVe embeddings', 'GloVe 300-d embeddings', 'GNGloVe embeddings', 'GloVe2 embeddings', 'GloVe embeddings 12', 'GloVe embeddings 4', 'GloVe embeddings 1', '-D GloVe embeddings', 'GLoVE embeddings', 'GloVe Embeddings', 'GloVe-based embeddings', 'GloVe embeddings 3']"
122,228,Dataset,3.227,2015,"{'2015': 0.1615, '2016': -0.0572, '2017': -0.0139, '2018': 0.3638, '2019': 0.8276, '2020': 1.1956, '2021': 2.1482, '2022': 3.227}","['MRPC', 'MSRP', 'MRPC dataset', 'Microsoft Research Paraphrase Corpus', 'Microsoft Research Paraphrase Corpus ( MRPC )', 'MSRPC', 'MSRP corpus', 'MRPC-R1', 'MRPC task', 'MSRP dataset', 'MSRP task', 'Microsoft Research Paraphrase Corpus ( MSRP )', 'MRPC )', 'MRPC ( F1 )', 'Microsoft Research Paraphrase Matching ( MRPC )', 'Microsoft Research Paraphrase corpus', 'Microsoft Research Paraphrase Corpus ( MRPC', 'Microsoft Research Paraphrase Corpus ( MSRPC )', 'MRPC F1', 'MRPC 6', 'MRPC tasks', 'Microsoft Research paraphrase corpus', 'MSRPC dataset', 'Microsoft Research Paraphrase ( MSRP ) corpus', 'MRPC corpus', 'Microsoft Paraphrase Corpus ( MRPC )', 'MRPC ASHA', 'MRPC—but', 'pus ( MSRP )', 'MRPC train set', 'MRPC training set', 'MRPC ( 5,801', 'MRPC-R1 training set', 'crosoft Research Paraphrase Corpus', 'Microsoft Research ( MSR ) Paraphrasing Corpus', 'MSR Paraphrase Corpus ( MRPC', 'MSR Paraphrase Corpus [ MSRP ]', 'MSRP—which', 'MRPC 1k', 'MSRP raters', 'MSRPC test dataset', 'MRPC std', 'MSRP 4', 'Microsoft Research Paraphrase coupus', 'MSRP 3', 'MSRP + PPNMT', 'mrpc', 'MRPC Microsoft Research Paraphrase Corpus', 'dataset ( MSRP )', 'Microsoft Research paraphrase corpus ( MSRP )', 'MSRP )', 'MRPC ( dev set )', 'Microsoft Research Paraphrasing Corpus', 'Microsoft Research Paraphrasing corpus', 'Microsoft Research Paraphrase ( MSRP ) Corpus', 'Microsoft Research Paraphrase Corpus paraphrase corpus', 'MSRP Data', 'MSRP data', 'Microsoft Paraphrase corpus ( MRPC )', 'Microsoft Research Paraphrase dataset ( MRPC )', 'Microsoft Research Paraphrase Corpus ( MRPC ) Dataset', 'MRPC ( RoBERTa )']"
123,210,Method,3.1678,2014,"{'2014': 1.5662, '2015': 3.1678, '2016': 2.5097, '2017': 1.6269, '2018': 1.7659, '2019': 1.1934, '2020': 0.2686, '2021': 0.2845, '2022': 0.2445}","['CBOW', 'CBOW model', 'cbow', 'CBOW models', 'CBoW', 'CBOW-char', 'W2V-CBOW', 'bag-of-words ( CBOW )', 'continuous bag-of-words model', 'CBOW algorithm', 'Continuous Bag of Words ( CBOW )', 'cbow model', 'CBoW model', 'continuous bag of words ( CBOW )', 'C-CBOW', 'continuous bag-of-words ( CBOW )', 'Continuous Bag-of-Words ( CBOW ) model', 'bag-of-words model ( CBOW )', 'w2v-cbow', 'w2v cbow', 'X-CBOW', 'continuous-bag-of-words ( CBOW )', 'Continuous Bag-of-Words ( CBOW )', 'Continuous Bag of Words ( CBOW ) model', 'continuous bag of words ( CBOW ) model', 'CBOW method', 'Bag-of-Words ( CBOW )', 'bag-of-words ( CBOW ) model', 'CBOW architecture', 'FW-CBOW', 'CBOW 2', 'CBOW-2', 'CBOW/SG', 'CBOW+CWE', 'CBOW-5', 'CBOW 1B', 'CBOW model ( CBOW )', 'CBOW Model', 'bag of words ( CBOW )', 'CBOW )', 'cbow )', 'continuous bag of words model', 'continuous bag-of-words ( CBOW ) models', 'continuous bag-of-words architecture', 'continuous bag-of-words model ( CBOW )', 'CBOW approach', 'CBOW base model', 'CBOW MSSG', 'C-CBOW model', 'C-CBOW technique', 'SW-CBOW', 'R cbow', 'CBOW1', 'CBOW-1', 'cbow 1', 'continuous bag-of-words ” ( CBOW ) models', 'CBOW 300', 'CBOW - 750', 'continuous bagof-words', 'CBOW CLMs', 'w2v CBOW', 'CBOW ( W2V-CBOW )', 'CBOW ( w2v-cbow )', 'cbow ( W2V CBOW )', 'W2V-CBOW model', 'OOV ( CBOW )', 'CBOW 10', 'continuous-bag-of-words model 3', 'CBOW 3', 'SG CBOW Corpus', 'continuous-bag-ofwords model', 'continuous bag-ofwords model', 'continuous-bag-ofwords architecture', 'Continuous Bag-ofWords modeling', 'CBOW WP+D', 'CBOW-char methods', 'CBOW-char model', 'Continuous Bag-of-Words 16', 'continuous bag-of-word model', 'continuous bag of word model', 'continuous bag of words techniques', 'Continuous Bag Of Words ( CBOW )', 'continuous bag-of-words ( CBoW )', 'Continuous bag of words ( CBOW )', 'Continuous Bag of words ( CBOW )', 'Continuous Bag of Words ( CBoW )', 'CBoW models', 'cbow models', 'CBOW-based', 'Continuous Bag-Of-Words ( CBOW ) model', 'continuous bag-of-words ( CBOW ) model', 'Continuous-bag-of-words ( CBOW ) model', 'continuous bag of words ( CBoW ) model', 'Bag-of-words ( CBOW )', 'Bag-Of-Words ( CBOW )', 'CBOW approaches', 'continuous-bag-of-words model', 'Continuous Bag of Words model', 'Continuous Bag of Words ( CBOW ) architecture', 'Continuous Bag of Words', 'Continuous Bag-Of-Words', 'continuous-bag-of-words', 'continuous bag of words', 'Continuous Bag-of-Words', 'Bag-of-Words ( CBOW ) model', 'Bag of Words model ( CBOW )', 'Bag-of-Words Model ( CBOW )', 'continuous-bag-of-words models', 'Continuous Bag-of-Words models', 'CBOW architectures', 'CBOW—the', '-cbow', 'CBOW based models', 'continuous bag of words methods', 'continuous bag of words ( CBOW ) models', 'continuous bag-of-words models ( CBOW )', 'continuous bags of words ( CBOW )', 'continuous bag-of-words ( cbow ) approach', 'CBOW ( continuous bag of words ) model', 'Continuous Bag-of-Words model ( CBOW )', 'CBOW-based methods', 'CBOW framework', 'continuous bag of words ( CBOW', 'CBOW ( Continuous Bags of Words )', 'continuous bag-of-words approach', 'CBOW measure', 'Continuous Bag-of-Word ( CBOW ) models', 'bag-of-words ( CBOW ) algorithm', 'Continuous Bag-of-Words method', 'continuous bag-of-words ( CBOW ) algorithm', 'continuous bag-of-words ( CBOW † ) models', 'CBOW ∗']"
124,337,Method,3.1472,2016,"{'2016': 0.7674, '2017': 1.3655, '2018': 1.8501, '2019': 3.1472, '2020': 2.8362, '2021': 2.103, '2022': 1.4194}","['attention weights', 'attention weight', 'attention-weighted', 'Attention weights', 'Attention Weights', 'attention-weighted sum', 'weighted attention', 'Attention weight', 'Attention Weight', 'attention weighted sum', 'attention-weight', 'attention weight analysis', 'un-weighted ” attention', 'attention-weighed', 'Attention-weight', 'Attention weight analysis', 'attention weight analyses', 'attention weight approach', 'attention-weighting mechanism', 'attention weighting mechanism', 'attention weighted', 'weight attention', 'weighted attention mechanism', 'weighted attentions', 'attention weights based methods', 'attention-based weights', 'attention weights to [ SEP ]', 'attention-based weight', 'weighted attention model', 'attention ( weights )', 'weighting attention', 'AttentionWeight', 'un-weighted Attn', 'Attention Weighted Sum', 'Attention weighted sum sum', 'weighted-sum attention mechanism', 'weighted sum of attentions', 'attentionweighted']"
125,176,Method,3.1351,2003,"{'2003': 0.8557, '2004': -0.0264, '2005': 1.1672, '2006': 0.5046, '2007': 1.4545, '2008': 1.0081, '2009': 0.7645, '2010': 1.5483, '2011': 1.567, '2012': 2.3465, '2013': 1.7152, '2014': 1.9598, '2015': 1.9125, '2016': 1.9891, '2017': 1.982, '2018': 2.1728, '2019': 2.2488, '2020': 2.3787, '2021': 2.9757, '2022': 3.1351}","['t-test', 't-tests', 'T-test', 'ttest', 't -test', 'T test', 't test', 'T-tests', 'T-Test', 't -tests', 'ttests', 't.test', 'TTEST', 't-Test', 'T-Tests', 'T -tests', 'Ttest', 't−test', 'T Test', 't tests', 'tTest', 's t-test', 'ttest )', 't−tests']"
126,52,Method,3.109,2004,"{'2004': 0.058, '2005': 0.031, '2006': 0.2744, '2007': 0.5977, '2008': 0.8056, '2009': 1.7416, '2010': 1.3768, '2011': 1.8315, '2012': 1.2877, '2013': 1.6918, '2014': 1.5174, '2015': 1.0682, '2016': 0.9574, '2017': 0.4056, '2018': 0.6617, '2019': 1.0284, '2020': 1.1518, '2021': 2.2271, '2022': 3.109}","['MT', 'MT system', 'mT5', 'MT model', 'MT systems', 'MT models', 'MT06', 'MT data', 'MT05', 'MT08', 'MT5', 'MT04', 'mt', 'machine translation', 'MT03', 'MT02', 'MT metrics', 'MT6', 'machine translation ( MT )', 'MT-CHAT', 'MT09', 'MT method', 'Machine Translation ( MT )', 'MT approach', 'MT-based approaches', 'MT1', 'machine translation model', 'base MT system', 'MT System', 'machine translation system', 'Machine Translation', 'machine translation systems', 'MT corpora', 'mt08', 'MT Systems', 'MT-based approach', 'MT-05', 'machine translation models', 'Machine translation ( MT )', 'MT dataset', 'MT-hier', 'MT-03', 'Machine translation', 'MT approaches', 'MT methods', 'MT-based models', 'MT08 set', 'MT-IR', 'mt5', 'base MT systems', 'MT techniques', 'MT-based systems', 'machine translation ( MT ) systems', 'MT-D', 'MT3', 'MT-04', 'MT2', 'MT AD', 'machine-translation', 'Machine translation systems', 'MT-based', 'MT-based methods', 'MT-based method', 'MT algorithms', 'machine translation ( MT ) model', 'MT10', 'MT-08', 'MT ’ 08', 'MT-IR system', 'MT02 dataset', 'MT-02', 'MT ( better )', 'MT03 data set', 'MT ’ 04', 'MT ’ 05', 'MT4', 'MT APIs', 'Hiero MT system', 'MT Models', 'MT architectures', 'base MT model', 'machine translation techniques', 'machine translation dataset', 'MT-S', 'Machine Translation ( MT ) models', 'AK-MT', 'MT-based SA', 'MT ’ 03', 'MT06 data', 'mt06', 'MT05 data sets', 'MT12', 'MT-LF', 'MT08 sets', 'q MT', 'COTS MT systems', 'machine translation §', 'machine translation system 3', 'machine translation 3', 'MT04 set', 'MT04 data', '( MT04 )', 'MT04 dataset', 'MT 2', 'MT systems 2', 'big data ” MT', 'LRL MT model', 'MT ( IR )', 'MT com', 'MT02 data', 'heard machine translation', 'MT03 dev set', 'MT03 set', 'A machine translation model', 'MT NAV', 'mT5- small', 'MT loop', 'MT-1', 'machine translation system 1', 'MT system 1', 'Machine Translation 4', 'MT-4', 'MT HT', 'MT/HT', 'Machine Translation System', 'Machine Translation system', 'Machine Translation models', 'machine-translation model', 'machine translation corpora', 'machine translation approach', 'MT-approach', 'corpus-based machine translation', 'Machine Translation ( MT ) based methods', 'machine translation based', 'machine-translation-based methods', 'machine translation ( MT ) system', 'Machine translation ( MT ) system', 'machine translation ( MT ) data', 'machine translation-based technique', 'machine translation approaches', 'MT Metrics', 'machine translation methods', 'machine-translation-based approaches', 'Machine translation based approaches', 'System-Based Machine Translation', 'Machine translation techniques', 'machine translation based systems', 'MT framework', 'MT-based )', 'MT-based system', 'based Machine Translation', 'machine translation-based approach', 'MT network architecture', 'based machine translation system', 'MT ( machine translation ) based approach', 'for Machine Translation', 'machinetranslation techniques', 'MT based models', 'small machine translation dataset', 'Machine Translation Dataset', 'MT ]', 'MT—', 'corpus-based MT approaches', 'MT data ( MT )', 'MT-metric based approach', 'MT metrics based approach', 'MT metric', 'MT-in', 'machine translation based model', 'Machine Translation Module', 'machine translation API 1', 'MT7', 'MT-7', 'Machine Translation 7 method', 'MT-based SA approaches', 'MT-based AMU system', 'IBM MT Models', 'IBM machine translation models', 'IBM MT models', 'mt09', 'MT09 data sets', 'MT09 data', 'MT09 set', 'MT-06', 'MT 06.', 'MT ’ 06', 'MT05 data', 'MT05 data set', 'MT05 (', 'MT05 sets', 'IR+MT', 'MT9', 'MT-08 data set', 'MT-08 corpus', 'MT08 data', 'MT08 dataset', 'ASL MT Systems', 'MT06G', 'MT06N', 'Machine Translation for All', 'MT-CHAT system', 'TCH MT system', 'MT-log', 'cal machine translation']"
127,381,Dataset,3.0424,2018,"{'2018': -0.0981, '2019': 0.6698, '2020': 1.4198, '2021': 1.7782, '2022': 3.0424}","['QQP', 'QQP dataset', 'Quora Question Pairs ( QQP )', 'Quora Question Pairs dataset', 'Quora question pair dataset', 'Quora Question Pairs ( QQP ) dataset', 'PAWS_QQP', 'QQP models', 'QQP 3', 'QQP 1', 'quora-question-pairs', 'Quora Question Pairs', 'QQP 2', 'Quora question pairs dataset', 'QQP 4', 'Quora Question Pairs dataset 1', 'The Quora question pair dataset', 'Quora Question Pair dataset', 'Quora question pairs', 'Quora ( QQP )', 'QQP classifier', 'Quora question pairs ( QQP ) corpus', 'QQP model )', 'Quora Question Pairs ( QQP ) Quora', 'Quora Question Pairs corpus', 'QQP ( 364k data', 'PAWS_QQP : QQP', 'Quora Question Pair Dataset 3', 'Quora question pair dataset 4', 'Quora Question Pairs dataset 4', 'Quora Question Pair ( QQP ) 7', 'Quora Question Pairs 3 task', 'Quora Question Pairs 2', 'Quora question pairs 2', 'Quora Question Pairs dataset 2', 'Quora question pair dataset 2', 'Quora question pair 2 dataset', 'Quora Question Pairs 2 dataset', 'QQP 8', 'Quora question pair dataset 8']"
128,207,Method,3.0348,2001,"{'2001': 0.1913, '2002': 0.1375, '2003': 0.2257, '2004': 0.8665, '2005': 0.493, '2006': 1.1793, '2007': 1.7616, '2008': 1.2451, '2009': 2.4449, '2010': 2.2076, '2011': 1.8626, '2012': 1.6587, '2013': 2.5956, '2014': 2.3563, '2015': 3.0348, '2016': 1.5122, '2017': 2.378, '2018': 1.9494, '2019': 2.6707, '2020': 2.0505, '2021': 2.0198, '2022': 1.5454}","['supervised learning', 'supervised learning approach', 'supervised learning methods', 'supervised learning approaches', 'supervised learning method', 'supervised learning techniques', 'supervised learning algorithm', 'Supervised Learning', 'Supervised learning', 'supervised learning model', 'supervised learning models', 'supervised learning framework', 'supervised learning task', 'Supervised learning approaches', 'supervised learning technique', 'supervised learning system', 'Supervised learning methods', 'supervised-learning', 'supervised learning systems', 'supervised learning based methods', 'supervised learning classifier', 'supervised learning classifiers', 'supervised learner', 'supervised-learning methods', 'supervised-learning approaches', 'supervised-learning method', 'supervised-learning systems', 'Supervised learning models', 'supervised metric learning classifier', 'Corpus-based supervised learning', 'supervised-learning framework', 'supervised learning mechanism', 'supervised structure learning', 'supervised learn', 'supervised learning )', 'supervised learning frameworks', 'supervised learning algorithm )', 'supervised learning tasks', 'supervised leaning algorithm', 'supervised leaning', 'supervised learners', 'supervised learning ( IPS )', 'supervised learning TSD algorithm', 'SUPERVISED learning', 'Supervised Learning methods', 'Supervised-learning approaches', 'supervised–learning techniques', 'Supervised Learning model', 'Supervised learning approach', 'supervised-learning approach', 'supervised-learning-based', 'supervised learning based approaches', 'supervised learning-based approaches', 'Supervised-learning-based approaches', 'supervised learning setting', 'Supervised learning algorithm', 'supervised learning strategies', 'supervised model learning', 'supervised learning-based methods', 'supervised corpus-based learning method', 'supervised-learning based approach', 'supervised learning dataset', 'supervised learning ( TSL )', 'supervised learning-based IE system', 'supervised learning WSD system', 'supervised ” learning', 'supervised learning ”']"
129,507,Method,3.0266,2013,"{'2013': 0.1248, '2014': 1.2359, '2015': 2.2127, '2016': 3.0266, '2017': 1.9839, '2018': 2.3757, '2019': 1.7447, '2020': 0.7672, '2021': 0.8374, '2022': 0.3031}","['tanh', 'Tanh', 'TanH', 'tanh ( )', 'tanh tanh', 'tanh-layer', 'tanh )', 'tanh (', 'tanh-layer )', 'tanh-layer model']"
130,60,Dataset,3.0222,2003,"{'2003': 0.0268, '2004': -0.076, '2005': 0.7193, '2006': 1.093, '2007': 2.0543, '2008': 1.0254, '2009': 1.439, '2010': 1.2319, '2011': 1.5888, '2012': 2.7016, '2013': 2.3854, '2014': 2.5976, '2015': 2.93, '2016': 2.8309, '2017': 2.5263, '2018': 2.2225, '2019': 3.0187, '2020': 3.0222, '2021': 2.9448, '2022': 2.9411}","['CoNLL', 'CoNLL-2014', 'CoNLL03', 'CoNLL2003', 'CoNLL-2003', 'CoNLL dataset', 'CoNLL 2003', 'CoNLL data', 'CoNLL-2009', 'CoNLL 2003 dataset', 'CoNLL04', 'CoNLL-2012', 'CoNLL-2013', 'CoNLL-2005', 'CoNLL-X', 'CoNLL-14', 'CoNLL 2012', 'CoNLL-03', 'CoNLL03 dataset', 'CoNLL2014', 'CoNLL-2003 dataset', 'CoNLL ’ 03', 'CoNLL 2005', 'CoNLL2000', 'CoNLL-2000', 'CoNLL2003 dataset', 'CoNLL score', 'CoNLL 2009', 'CONLL', 'CoNLL 2000', 'CoNLL-13', 'CoNLL scorer', 'CoNLL14', 'CoNLL-2002', 'CoNLL 2007', 'CoNLL-2008', 'CoNLL-2012 dataset', 'CoNLL-09', 'CoNLL09', 'CoNLL 2003 data', 'CoNLL2009', 'CoNLL-2009 dataset', 'CoNLL-12', 'CoNLL 2006', 'CoNLL 2014', 'CoNLL-05', 'CoNLL-train', 'Conll2003', 'CoNLL 2009 dataset', 'CoNLL2000 dataset', 'CoNLL ’ 09', 'CoNLL-2014 dataset', 'CoNLL ’ 07', 'CoNLL 2008', 'CoNLL ’ X', 'CoNLL-2000 dataset', 'CoNLL task', 'CoNLL F1', 'CoNLL bart', 'CoNLL corpus', 'CoNLL data sets', 'CoNLL00', 'CoNLL 2005 data', 'CoNLL ’ 00', 'CoNLL-2011', 'CoNLL-03 dataset', 'CoNLL 2002', 'CoNLL2013', 'CoNLL18', 'CoNLL 2011', 'CoNLL 2012 dataset', 'CoNLL-2000 task', 'CoNLL2014 dataset', 'CONLL-2003', 'CONLL 2003', 'CoNLL-2016', 'CoNLL 2009 data sets', 'CoNLL dec', 'TR-CONLL', 'CONLL 2002', 'CoNLL ’ 12', 'ConLL', 'CoNLL corpora', 'Computational Natural Language Learning ( CoNLL )', 'CoNLL-U', 'CONLL 2003 dataset', 'CoNLL2005', 'CoNLL-2005 dataset', 'CoNLL ’ 05', 'conll07', 'CoNLL2002', 'CoNLL training set', 'CoNLL-2013 dataset', 'CoNLL 2000 dataset', 'CoNLL-2012 data', 'CoNLL 2012 data', 'CoNLL-2012 data set', 'CoNLL 2002/2003', 'CoNLL ’ 08', 'Conll', 'CONLL-03', 'CONLL2003 dataset', 'CoNLL-2005 data set', 'CoNLL ( en )', 'CoNLL04 dataset', 'CoNLL scores', 'CoNLL12', 'CoNLL testa', 'CoNLL-2007', 'CONLL-2007', 'CoNLL 2000 data set', 'CoNLL2012', 'CoNLL 2012 test set', 'CoNLL 2003 training set', 'CoNLL 2010', 'CoNLL ’ 02', 'CoNLL data set', 'CoNLL dev', 'CoNLL-Test dataset', 'CoNLL F 1 scores', 'CoNLL-2014 test set', 'CONLL03', 'CoNLL03 dev', 'CoNLL-2003 data', 'CoNLL 2003 corpus', 'CoNLL-2003 corpus', 'CoNLL-05 dataset', 'CoNLL2005 dataset', 'CONLL04', 'CoNLL test-b', 'CoNLL-2009 data', 'CoNLL-2004', 'CoNLL-2008 data', 'CoNLL 2008 dataset', 'CoNLL-style', 'CoNLL2003 ( Dict )', 'CoNLL-2003 task', 'CoNLL-2006', 'CoNLL07', 'CoNLL train', 'CoNLL training data', 'CoNLL-X data', 'CoNLL2012 dataset', 'CONLL-2012 dataset', 'CoNLL tasks', 'CoNLL-2013 test data', 'CoNLL bart data', 'CoNLL testb', 'CONLL data', 'CoNLL )', 'CoNLL metric', 'CoNLL I2B2', 'CoNLL F1 score', 'CONLL03 dataset', 'CONLL2003', 'Conll-2003', 'Conll 2003', 'CoNLL 2003 data set', 'CoNLL 05', 'CoNLL-05 data', 'CoNLL-2005 data', 'CoNLL 2016', 'CONLL04 dataset', 'CoNLL 2009 data', 'CoNLL-12 data', 'CoNLL2004', 'CoNLL 2018', 'CoNLL16', 'CoNLL-12 EMB MODEL', 'CoNLL2003 ( KB )', 'CoNLL-2008 dataset', 'CoNLL-2008 systems', 'CoNLL18 UDPipe 1.2', 'CoNLL-10', 'CoNLL 2003 task', 'CoNLL + L8', 'CoNLL 2012 task', 'CoNLL-14-test', 'CoNLL test-a', 'CoNLL-2014 task', 'CoNLL 2002 Spanish dataset', 'CoNLL-02', 'CONLL-14', 'CoNLL-2003 2', 'Train CoNLL07', 'CoNLL Train', 'CoNLL-2005 task', 'CoNLL 2013', 'CoNLL-2013 data set', 'CoNLL-2013 set', 'CoNLL X', 'CoNLL-X data sets', 'CoNLL-2009 ST best', 'CoNLL2019', 'CoNLL-2014 top system', 'CONLL 2000 dataset', 'CoNLL-2000 corpus', 'CoNLL 2000 data', 'CoNLL-2000 data sets', 'CoNLL-2002 test set', 'CoNLL 2012 corpus', 'CoNLL-2012 dev set', 'CoNLL-to-QA', 'CoNLL-to-QA data', 'CONLL-2012 test set', 'CoNLL-2002 3', 'CoNLL-2013 test set', 'CoNLL F 1', 'FCE+CoNLL', 'CoNLL file', 'CONLL dataset', 'CoNLL Dataset', 'conll/', 'CoNLL Corpus', 'CoNLL systems', 'CoNLL system', 'CoNLL System', 'CoNLL sets', 'CoNLL dev set', 'CoNLL02/03', 'CoNLL-2011/2012', 'CoNLL17-1', 'CoNLL17-10', 'CoNLL 2000- style', 'CoNLL 2012 training set', 'CoNLL2014 test', 'CoNLL-X task', 'CoNLL 03', 'CoNLL03 data', 'CoNLL 2009 task', 'CoNLL-2014 data', 'CoNLL-2014 dev set', 'conll-2003', 'CoNLL 2003 dev set', 'CoNLL2003 models', 'CoNLL05', 'CoNLL 2008 training dataset', 'CoNLL-06 data', 'CoNLL 2005 dataset', 'CoNLL2005 data set', 'CoNLL-2005 data-set', 'CoNLL-2005 models', 'p-CoNLL09', 'CoNLL2000POS dataset', 'CoNLL2000POS', 'CONLL score', 'CoNLL-2009 data sets', 'CoNLL 2009 data set', 'CoNLL-2009 data set', 'CoNLL12 dataset', 'CoNLL 2004 data', 'CoNLL-2012 scorer', 'CoNLL-2004 1', 'CoNLL2012 global score', 'CoNLL2008', 'CoNLL 2008 data', 'CoNLL 2008 data set', 'CoNLL-2008 data set', 'CoNLL 2008 systems', 'CONLL style', 'CoNLL 2003 LC', 'CoNLL-Val dataset', 'CoNLL JFLEG System', 'AIDA CoNLL training', 'AIDA CoNLL training set', 'CoNLL M 2', 'FCE CoNLL-2014', 'CoNLL dataset 10', 'CoNLL07 schemes', 'CoNLL ’ 00 corpus', 'CONLL 2003 task', 'CoNLL 2012 2', 'CoNLL-2008 task', 'CoNLL 2000 script', 'CoNLL 2006 corpora', 'CoNLL-14-Test', 'CoNLL14-test', 'CoNLL test-a dataset', 'CoNLL2003g', 'CoNLL2003g dataset', 'CoNLL-2000 data 2', 'CONLL 07', 'CoNLL-ST 1', 'CoNLL-2005 5', 'CoNLL2003etest', 'μF1 CoNLL04', 'CoNLL scorer 2', 'CoNLL-2002 Spanish dataset', 'ConLL02', 'CoNLL02', 'conll14', 'CoNLL-14 dataset', 'CoNLL-14 data', 'CONLL-14 (', 'CoNLL-14 Model', 'CoNLL 2003 2', 'CoNLL Span', 'Conll2002', 'CONLL 2002 dataset', 'CoNLL2002 dataset', 'CoNLL2007', 'CoNLL 2007 dataset', 'CONLL12 CONLL05', 'CoNLL train data', 'CoNLL-train )', 'CoNLL data ( train set )', 'CoNLL training sets', 'CoNLL training corpus', 'CoNLL training', 'Conll-2005 task', 'Conll 2005 task', 'CoNLL 2009 best systems', 'CoNLL-scorer', 'CONLL scorer', 'CoNLL-2013 corpus', 'CoNLL 2013 dev set', 'CoNLL-2013 dev set', 'CoNLL-2003 standard', 'CoNLL-2014 teams', 'BLANC CoNLL System', 'CoNLL 2004-2005', 'CONLL-X', 'CONLL-X data sets', 'CONLL-X data set', 'CoNLL-X corpora', 'CoNLL-2014 setup', 'CoNLL18 model', 'conll18/', 'CoNLL-ST', 'CoNLL-ST dataset', 'CoNLL 2003 script', 'CoNLL shared', 'CoNLL-2009 ST best system', 'span-based CoNLL 2012 data', 'CoNLL-2019', 'CoNLL-2013 set 9', 'CoNLL-2009 website 5', 'CoNLL 09', 'CoNLL-09 dataset', 'CoNLL09 dataset', 'CoNLL-09 corpus', 'CoNLL-09 data', 'CoNLL2011', 'CoNLL 2011 dataset', 'CoNLL 2011 data', 'CoNLL2011 data', 'CoNLL-2000 dataset 4', 'CONLL-2000', 'CONLL 2000', 'CoNLL 2000 corpus', 'CoNLL 2000 data-set', 'CoNLL-2000 data set', 'CoNLL2000 data', 'CoNLL-2000 data', 'conll17/', 'CoNLL ’ 09 dev set', 'CONLL 2012 dataset', 'CONLL2012', 'CONLL 2012', 'CoNLL–2012', 'CONLL-2012', 'CONLL-2012 data', 'CoNLL2012 data', 'CoNLL 2012 system', 'FIR CoNLL-03', 'CoNLL ’ 12 3', 'CoNLL 3', 'CoNLL years', 'CoNLL2009 data set 6', 'CoNLL-2009 6', 'CoNLL dataset 4', 'CoNLL ’ 2010', 'CoNLL 2006/2007 data sets', 'CoNLL 2006/2007 dataset', 'CoNLL-2012 TEST', 'CoNLL2012 TEST', 'CoNLL-2012 test', 'CoNLL-2012 test set', 'CONLL-2012 test dataset', 'CoNLL-2003 3', 'CoNLL-2002/2003 data', 'CoNLL-2002 and -2003 sets', 'CoNLL13', 'CoNLL-13 data', 'CoNLL2003 - It', 'CoNLL- src', 'CoNLL task systems', 'SemEval/CoNLL', 'CoNLL2003 training set', 'CONLL 2003 training set', 'CoNLL2003 training dataset', 'CoNLL-2003 training dataset', 'Speech-CoNLL04', 'CoNLL-2003 ( EN ) dataset', 'CoNLL-2003 ( EN )', 'CoNLL14 shared', 'CoNLL-X shared', 'CoNLL-2010 dataset', 'CoNLL 2010 dataset', 'CoNLL-2010', 'CoNLL2013 test data', 'CoNLL2013-test', 'CoNLL2013 test set', 'CoNLL-2013 test data set', 'CoNLL ’ 02 data', 'CoNLL ’ 08 data', 'CoNLL dataset–a', 'CoNLL scorer ( v5 )', 'CoNLL ’ 19', 'CoNLL-X test data', 'CoNLL polyglot model', 'CoNLL F1 )', 'CoNLL 2011 task', 'CoNLL02-Spanish', 'CoNLL-02 Spanish', 'CONLL ’ 03', 'CoNLL ’ 03 data', 'CoNLL ’ 03 data set', 'CoNLL Basic', 'CoNLL bart approaches', 'CoNLL04 statistics', 'conll', 'CoNll', 'CONLL corpora', 'CoNLL Data', 'CoNLL model', 'CoNLL Model', 'Conll )', 'CONLL corpus', 'CoNLL set', '–CoNLL', 'CoNLL models', 'CoNLL measures', 'CoNLL 2012 script', 'Conll2003-OOV', 'Conll2003-Typos', 'Conll03-OOV', 'auto_conll', 'skyline CoNLL17-1', 'ConLL02/03', 'CoNLL-02/03', 'CoNLL02/03 data', 'CoNLL-2011/2012 system', 'CoNLL test data', 'CoNLL tests', 'CoNLL Test', 'CoNLL-2012 scorer 1', 'CoNLL 2009 training set', 'CoNLL-2009 ( wordbased )', 'CoNLL ’ 11', 'CoNLL ’ 11 data', 'CoNLL workshop', 'Labeled CoNLL 2009 F1 Data Model', 'CoNLL 1', 'CoNLL-2014 gold standard', 'CoNLL seq', 'CoNLL-2014 training data', 'CoNLL2014 training data', 'CoNLL-2014 training set', 'CoNLL2003 , 1BWB', 'CoNLL F 1 score', 'MIMIC-CoNLL2003', 'CoNLL-2012 training set', 'CoNLL-2012 training data', 'PubMed-CoNLL2003', 'CoNLL scorer 1', 'CoNLL essay data', 'CONLL-2014-test', 'CoNLL-2014 test', 'CoNLL2014 test set', 'CoNLL-2014 test data', 'CoNLL-2014 test set ( CoNLL )', 'CoNLL-2014 test set ( CoNLL2014 )', 'CoNLL-2014 test dataset', 'CONLL-de', 'CoNLL-de', 'CoNLL scorer v8.01', 'type CoNLL2003-test', 'CoNLL2000 tasks', 'CoNLL-2000 tasks', 'CoNLL2000 task', 'CONLL 2000 task', 'CoNLL 2000 tagged corpus', 'ConLL-03', 'CoNLL 03 data set', 'CONLL-03 data', 'CoNLL03 corpus', 'CoNLL03 model', 'CoNLL03 dev set', 'CoNLL03 )', 'CoNLL 2017 data', 'CoNLL 2017 data set', 'CoNLL-X webpage', 'CoNLL 312K', 'CoNLL 2014 dataset', 'CoNLL-2014 systems', 'CoNLL2014 data', 'CoNLL-2014 data sets', 'CoNLL-2014 data set', 'CoNLL-2014 set', 'CoNLL-X free', 'ConLL 2003', 'ConLL-2003', 'ConLL 2003.', 'CONLL-2003 dataset', 'Conll 2003 dataset', 'ConLL-2003 data', 'CONLL 2003 corpus', 'CoNLL-2003 data set', 'conll 2003 data set', 'CoNLL-2003 ( CoNLL )', 'CoNLL2003 metrics', 'CoNLL 2003 corpora', 'CoNLL 05 dev set', 'CoNLL 2003 Reuters corpus', 'CoNLL-X training set', 'XQUAD CoNLL', 'CoNLL 2008 training data', 'CoNLL2003 test dataset', 'CoNLL-2003 test set', 'CONLL 2003 test set', 'CoNLL2003-test', 'CoNLL-2003 DEV TEST', 'CoNLL 2003 Test data', 'CoNLL scheme', 'CoNLL schemes', 'CoNLL LFT', 'CoNLL ONB', 'CoNLL 2006 training data', 'CoNLL 2000 CoNLL 2012 Model', 'ConLL-2005', 'CONLL2005', 'Conll-2005', 'CoNLL-2005 corpus', 'CoNLL-2005 systems', 'CoNLL 2005 systems', 'CoNLL 2005 data-set', 'CoNLL 2005 data set', 'CoNLL 2005.', 'CoNLL-2005 F1 score', 'CoNLL09-test', 'CoNLL TFM', 'p-CoNLL09 dataset', 'CoNLL2001', 'CoNLL-2005 3', 'CoNLL ’ 17 data', 'CoNLL-2012 gle phrase', 'CONLL-en', 'CoNLL-en', 'EN-CoNLL', 'CoNLL2016 corpora', 'ERE-ES CoNLL', 'CoNLL-00 CoNLL-03', 'CoNLL 2002 script', 'CoNLL04 corpora', 'CoNLL04 Model', 'SNIPS CoNLL-03', 'CoNLL-2003 11 Named', 'CoNLL text', 'CoNLL test-b dataset', 'CoNLL-03 test dataset', 'CoNLL-2015', 'CoNLL-2011/2012 tasks', 'CoNLL–2009 dataset', 'CoNLL2009 dataset', 'CoNLL-2009 systems', 'CoNLL2009 data', 'CoNLL2009 data sets', 'CoNLL 2009.', 'CoNLL 2009 corpus', 'CoNLL-2009 dev data', 'CoNLL-2009 system', 'CoNLL 2009 dev set', 'CoNLL03 training set', 'CoNLL2000POS dataset 2', '• CoNLL-2003', 'CoNLL-13/-14', 'CoNLL-14 BEA', 'CoNLL12 data', 'CoNLL-12 models', 'CoNLL-12 dataset', 'CoNLL03/WNUT17', 'CoNLL-U file', 'CoNLL-2003 dataset ( CONLL03 )', 'CoNLL 2004 dataset', 'CoNLL 2004', 'CoNLL-2004 corpus', 'CoNLL 2004 model', 'CoNLL-08 corpus', 'CoNLL-08', 'CoNLL-13 OOV', 'CoNLL ’ 16', 'CoNLL-2000 1', 'CoNLL 2012 scorer', 'TR-CONLL corpus', 'CoNLL testa dataset', 'CoNLL-2000 shal', 'CoNLL 2002 Spanish training data', 'CoNLL 2018 data', 'CoNLL-YAGO ( train )', 'BEA-test CoNLL-2014', 'CoNLL-style trees', 'CoNLL Spanish corpus', 'Spanish CoNLL corpus', 'CoNLL ’ 06', 'CoNLL ’ 06 data', 'CoNLL task 1', 'CONLL DQM', 'CoNLL-2008 Shared']"
131,89,Method,3.0167,2016,"{'2016': 0.129, '2017': 0.6631, '2018': 1.0708, '2019': 2.9309, '2020': 3.0167, '2021': 2.3719, '2022': 2.5523}","['VAE', 'VAEs', 'VQ-VAE', 'variational autoencoders', 'variational autoencoder', 'variational autoencoder ( VAE )', 'VAE model', 'RRT-VAE', 'DSS-VAE', 'Variational Autoencoder ( VAE )', 'VAE models', 'S-VAE', 'variational auto-encoder', 'variational auto-encoders', 'HRQ-VAE', 'SA-VAE', 'Variational Autoencoder', 'CUC-VAE', 'β -VAE', 'VAE-based models', 'VAE-based', 'SS-VQ-VAE', 'VQ-VAEs', 'Variational Autoencoders', 'δ -VAE', 'VAE framework', 'G-VAE', 'Variational AutoEncoder ( VAE )', 'VAE∗', 'R-VAE', 'variational autoencoder model', 'VAE-based methods', 'VAE-based model', 'variational auto-encoder framework', 'VQ-VAE model', 'EA-VQ-VAE', 'v-VAE', 'vae', 'VAE methods', 'variational auto-encoding framework', 'T-VAE', 'VAE-NF', 'δ-VAE', 'Cross-VAEs', 'Variational AutoEncoder', 'variational autoencoder framework', 'β-VAE', 'Lag-VAE', 'Lag VAE', 'Variational autoencoders', 'DSS-VAE model', 'GCC-VAE', 'Variational autoencoder ( VAE )', 'Variational Auto-Encoder', 'Cyc-VAE', 'VQ-VAE models', 'β− VAE', '\ue021-VAE', 'Base + VAE + LA model', 'VAE+HF', 'VAE-0.5', 'δ -VAEs', 'Dual-VAEs', 'VAE-only', 'VAE method', 'variational auto-encoder model', 'VAE )', 'variational autoencoder approach ( VAE )', 'big VAE model', 'VAE system', '-VAE', 'variational auto-encoding model', 'L vae', 'Sent-VAE', 'VAE/WAE frameworks', 'B-VAE', 'Vatiational Auto-Encoder', 'VAE∗ model', 'β C -VAE', 'neural VAE', 'neural variational autoencoder ( VAE )', 'Dis VAE', 'Base + VAE + LA architecture', 'Base + VAE + LA', 'Variational AutoEncoders', 'variational auto encoders', 'Variational auto encoders', 'Variational Auto Encoders', 'Variational Auto-Encoders', 'variational autoencoders )', 'J vae', 'Base + VAE', 'Disen VAE', 'VAE-Sen', 'VAE DAE VAE', 'VAE-based ones', 'C-VAE', 'VAE M', 'variational autoencoders ( DVAE )', 'P-VAE', 'D-VAE', 'SA-VAE M', 'Dual-VAEs models', 'dual variational autoencoder ( Dual-VAEs )', 'dual variational autoencoder model', 'cross variational autoencoder ( Cross-VAEs )', 'Variational Autoencoder 1', 'vae +C', 'VAE-only model', 'Yelp VAE', 'R-VAE models', 'variational autoencoder ( R-VAE )', 'VAE+FB', 'variational auto-ecoder framework', 'VQ-VAE 2', 'variational-autoencoder ( VAE )', 'VAE ( Variational Autoencoder )', 'variational AE ( VAE )', 'variational Autoencoder ( VAE )', 'VAES', 'variational AEs', 'Variational autoencoder', 'Variational Autoencoder-based', 'VAE Model', 'VAEs model', 'Variational auto-encoder', 'VAE Models', 'VA Es', 'variational autoencoder based models', 'variational autoencoder based model', 'layered VAE', 'Variational AutoEncoder ( VAE ) model', 'Variational Autoencoder ( VAE ) model', 'variational autoencoder based framework', 'VAE-based framework', 'VAEs )', 'variational autoencoder-based ( VAE-based )', 'VAE techniques', 'variational autoencoder techniques', 'VAE systems', 'variational autoencoder architecture', 'structured variational autoencoder', 'VAE approach', 'variational autoencoder approach', 'VAE-based architecture', 'SoTA VAE methods', 'VAE-based approaches', 'variational autoencoder ( VAE ) -based approaches', '( VAEs )', 'variational autoencoder based approach', 'variational autoencoder ( VAE ) network', 'variational autoencoder framework ( VAE )', 'variational autoencoder-based ( VAE )', 'VAE-S', '- VAE', 'VAE approaches', 'variational autoencoder framework ( VAE', 'Variational autoencoder ( VAE ) models', 'variational auto-encoder based algorithm', 'VAE frameworks', 'CUC-VAE system', 'structured variational autoencoder ( SVAE )', 'HVQ-VAE model', 'HVQ-VAE', 'VQ-VAE )', 'VQ-VAE method', 'SS-VQ-VAE model', 'VAE-like', 'Variational Autoencoders ( CUVA ) 1', 'Variational Autoencoders ( CUVA )', 'β VAE', 'β -VAEs', 'Lagging VAE', 'lagging-VAE', 'lagging VAE ( LAG-VAE )', 'lagging-VAE * *', 'variational auto-encoding', 'Variational Auto Encoding architectures', 'variational auto-encoder learning', 'VAE Learning', 'variation autoencoder', 'T-VAE-4']"
132,49,Method,3.0058,2014,"{'2014': -0.1, '2015': 0.3731, '2016': 0.0986, '2017': 0.0583, '2018': 1.1183, '2019': 1.8609, '2020': 1.9348, '2021': 3.0058, '2022': 2.9497}","['KG', 'KGs', 'knowledge graph', 'knowledge graph ( KG )', 'Knowledge graphs ( KGs )', 'Knowledge Graph ( KG )', 'knowledge graphs ( KGs )', 'Knowledge Graphs ( KGs )', 'KG-A2C', 'KG G', 'Knowledge Graph', 'KG models', 'KG model', 'KGs )', 'Knowledge graph ( KG )', 'K & G', 'KG2E', 'A knowledge graph ( KG )', 'knowledge graphs ( KG )', 'Knowledge graphs ( KG )', 'KG dataset', 'K & G architecture', 'Knowledge graph', 'KG )', 'KG-based models', 'Knowledge Graphs ( KG )', 'knowledge graphs', 'KG-Classifier', 'KG ( G )', 'KG16', 'KG 2', 'KG0', 'Knowledge graphs', 'K & G model', 'KG-KR model', 'KG-50', 'KG RGs', 'KG ( I )', 'knowledge-graph', 'Knowledge Graph )', 'knowledge graph data', 'graph-structured knowledge', 'KG-based', 'KG structures', 'KG-based methods', 'KG-73K', 'knowledge graph ” ( KG )', 'KG QA', 'KG-QA', 'KG-QA approach', 'KS/KG models', 'KG k', 'KG 10', 'KG-1', 'KG 1', 'KG1', 'Knowledge Graph 1', 'knowledge graphs 1 ( KGs )', 'Knowledge Graph ( KG ) 1', 'A Knowledge Graph ( KG )', 'KG-KR', 'knowlege graph', 'KG G 0', 'KG16a', 'KG-2', 'KGs 2', 'KG-based RG', 'Knowledge-Graph RGs', 'Knowledge Graph-based RGs ( KG RGs )', 'Kg', 'kgs', 'knowledge-graph ( KG )', 'knowledge Graphs ( KGs )', 'knowledge-graphs ( KGs )', 'KG methods', 'knowledge graph methods', 'Knowledge Graphs', ""KG ''"", 'knowledge graph )', 'KG data', 'knowledge graph techniques', 'KG techniques', 'KG architectures', 'knowledge graph models', 'KG-based approaches', 'knowledge graph-based approaches', 'graph-based knowledge base', 'graph knowledge base', 'graph knowledge-base', 'knowledge graph ( KG ) corpus', 'KG model )', 'KG approaches', 'knowledge graph approaches', 'KG modeling approaches', 'KG & corpus', 'KG modeling', 'graph based knowledge', 'structured knowledge graphs', 'corpus-KG', 'KG structure', 'KG-based method', 'KG-based dataset', 'structured knowledge ( KGs )', 'KG classification dataset', 'knowledge-graph based approach approaches', 'structured knowledge graph ( KG )', 'M KGs', 'KG ) 5']"
133,446,Method,3.0017,2014,"{'2014': -0.0282, '2015': 0.5725, '2016': 1.9055, '2017': 1.7752, '2018': 3.0017, '2019': 2.7659, '2020': 2.0132, '2021': 1.6821, '2022': 1.0111}","['ReLU', 'Relu', 'relu', 'rectified linear unit ( ReLU )', 'ReLu', 'rectified linear units', 'Rectified Linear Unit ( ReLU )', 'RELU', 'Rectified Linear Units ( ReLU )', 'rectified linear units ( ReLU )', 'rectified linear ( ReLU )', 'ReLU gate', 'res4_relu', 'rectified linear unit ( ReLu )', 'Rectified Linear Units', 'ReLU ( · )', 'ReLu Optimizer Adam', 'relu ( 2 )', 'GC-based ReLU', 'MGN ReLU', 'ReLU gates', 'relu ( 4 )', 'ReLU FC', 'rectified linear unit ( relu )', 'ReLU ( Rectified Linear Unit )', 'Rectified Linear Units ( Relu )', 'rectified linear ( ReLu )', 'rectified linear ( Relu )', 'Rectified Linear ( ReLU )', 'ReLU systems', 'ReLU—in', 'ReLU network', 'rectified linear unit', '- ReLU', 'relu6 1']"
134,163,Dataset,2.9939,2005,"{'2005': -0.2142, '2006': 0.1145, '2008': 0.2712, '2009': 0.1362, '2010': 0.3331, '2011': -0.051, '2012': 0.3472, '2013': 0.2696, '2014': -0.1258, '2015': 0.5181, '2016': 0.5318, '2017': 0.0876, '2018': 0.1771, '2019': 0.4715, '2020': 1.191, '2021': 1.9754, '2022': 2.9939}","['RTE', 'RTE task', 'RTE-3', 'RTE system', 'RTE dataset', 'RTE data', 'RTE tasks', 'RTE-1', 'RTE methods', 'RTE-3 dataset', 'RTE-2', 'RTE model', 'RTE+SWD', 'RTE 3', 'Relational Triple Extraction ( RTE )', 'RTE method', 'Relational Triple Extraction ( RTE ) methods', 'RTE-style', 'Pascal RTE3', 'RTE systems 2', 'Relational Triple Extraction', 'Relational triple extraction ( RTE )']"
135,155,Dataset,2.9864,2010,"{'2010': -0.072, '2011': 0.0797, '2012': 0.1553, '2013': 0.029, '2014': 0.601, '2015': 0.9151, '2016': 0.5755, '2017': 1.2777, '2018': 1.5479, '2019': 1.7398, '2020': 2.8417, '2021': 2.9864, '2022': 2.3692}","['WMT', 'WMT14', 'WMT ’ 14', 'WMT data', 'WMT ’ 16', 'WMT ’ 18', 'WMT ’ 15', 'WMT-14', 'WMT ’ 19', 'WMT dataset', 'WMT14 dataset', 'WMT 14', 'WMT ’ 17', 'WMT corpus', 'wmt', 'WMT ’ 13', 'WMT corpora', 'WMT14 corpus', 'WMT ’ 18 dataset', 'wmt14', 'WMT ’ 14 dataset', 'WMT data sets', 'WMT model', 'WMT ’ 12', 'WMT ’ 11', 'WMT system', 'WMT models', 'WMT ’ 17 data', 'WMT-14 dataset', 'WMT14 data', 'WMT DARR corpus', 'WMT ’ 19 data', 'WMT Corpus', 'WMT data set', 'WMT )', 'Machine Translation Workshop ( WMT ) data', 'WMT-NT sets', 'WMT ’ 12 data', 'WMT ’ 09', 'WMT ’ 17 dataset', 'WMT ’ 14 WMT ’ 16', 'WMT14 )', 'WMT14 system', 'WMT ’ 14 data', 'WMT ’ 14 corpus', 'WMT ’ 13 data set', 'WMT ’ 13 data', 'WMT ’ 15 Cs→En', 'WMT ’ 16 dataset', 'NIST WMT', 'WMT ’ 16 ROEN', 'WMT ’ 16 En', 'WMT ’ 14 ( fr )', 'WMT ’ 21', 'WMT ’ 19 dev set', 'WMT ’ 19 model', 'WMT ’ 19 dataset', 'WMT ’ 06 dataset', 'WMT ’ 06', 'NIST WMT ’ 14', 'OPUS WMT ’ 19', 'WMT ’ 19 , 8', 'WMT ’ 08 data', 'WMT ’ 08', 'WMT ’ 14 5', '-wmt', 'WMT data )', 'WMT.', 'WMT Translation', 'Machine Translation ( WMT )', 'WMT Corpora', 'WMT systems', 'WMT Metrics corpus', 'WMT sets', 'WMT Metrics', 'WMT-NT', 'WMT ’ 19 En → Fr', 'NT ’ 18 WMT', 'NT ’ 19 WMT', 'NT ’ 20 WMT', 'WMT ’ 15 corpora', 'WMT ’ 15 SOTA system', 'WMT ’ 12-13', 'WMT-high', 'WMT ’ 14 , 16', 'WMT ’ 08 1', 'WMT 14 dataset', 'WMT 14 corpus', 'WMT14 set', 'WMT14 corpora', 'WMT14 Model', 'WMT ’ 13 5', 'WMT ’ 18 EnEt dataset', 'doc-WMT', 'doc-WMT+', 'WMT ’ 14 7', 'WMT ’ 13 WMT ’ 14', 'WMT ’ 14 system', 'WMT ’ 14 model', 'WMT ’ 14 corpora', 'WMT ’ 14 )', 'doc-WMT++', 'WMT ’ 17 5']"
136,296,Method,2.9836,2010,"{'2010': 0.4484, '2011': 0.8732, '2012': 0.5839, '2013': 1.2201, '2014': 1.1583, '2015': 1.5619, '2016': 2.1031, '2017': 2.4892, '2018': 2.693, '2019': 2.5724, '2020': 2.7852, '2021': 2.9836, '2022': 2.886}","['grid search', 'grid-search', 'gridsearch', 'Grid Search', 'Grid search', 'grid searches', 'grid searching', 'grid search method', 'grid search algorithm', 'grid search strategy', 'grid-searching', 'grid-search algorithm', 'grid search 18', 'grid search 5', 'GridSearchCV', 'Grid search ”', 'w/ grid-search', 'grid search )', 'grid-search method', 'GridSearch', 'grid-search technique', 'grid search technique', 'grid-based search', 'grid-search model', 'grid search algorithms', 'grid search analysis', 'grid search approach', 'grid search - 0.9', 'grid search 8', 'τ grid-search', 'gird search']"
137,286,Method,2.9488,2014,"{'2014': -0.1459, '2015': 0.2502, '2016': 2.5857, '2017': 2.2278, '2018': 2.9488, '2019': 1.878, '2020': 0.6906, '2021': 0.1935, '2022': 0.0563}","['bidirectional RNN', 'bidirectional RNN', 'BiRNN', 'BRNN', 'bidirectional RNNs', 'bidirectional RNNs', 'bidirectional recurrent neural network', 'biRNN', 'bidirectional recurrent neural network ( RNN )', 'bidirectional recurrent neural networks', 'BiRNN model', 'BiRNN-BiBS', 'BIRNN', 'BiRNN-GSN', 'PG-BRNN', 'bidirectional RNN model', 'bidirectional RNN models', '+BRNN', 'bidirectional recurrent neural networks ( RNN )', 'birnn', 'Bidirectional RNN', 'BiRNN models', 'uni-directional RNNs', 'uni-directional RNN', 'Bidirectional RNNs', 'Bidirectional Recurrent Neural Network', 'bidirectional RNN ( BRNN )', 'bidirectional recurrent neural networks ( RNNs )', 'deep bidirectional RNN', 'deep bidirectional RNNs', 'R-biRNN', 'bidirectional stacked RNNs', 'BiRNN 100d model', 'SHEF-bRNN', 'bidirectional MD-RNNs', 'BRNN model', 'bidirectional-RNN', 'Bidirectional recurrent neural network', 'Bidirectional-RNNs', 'biDirectional RNNs', 'biRNN models', 'bidirectional Recurrent Neural Network ( BiRNN )', 'bidirectional RNN ( biRNN )', 'Bidirectional RNN ( BiRNN )', 'bidirectional RNN ( BiRNN )', 'bidirectional recurrent neural network ( biRNN )', 'bidirectional recurrent neural network ( BiRNN )', 'bidirectional recurrent neural network ( BRNN )', 'bidirectional RNN networks', 'bidirectional recurrent neural network ( RNN ) model', 'bidirectional Recurrent Neural Network ( RNN )', 'bidirectional RNN-based models', 'BiRNN )', 'BiRNN-based architecture', 'bidirectional recurrent neural model', 'bidirectional recurrent neural models', 'bidirectional RNN ( BRNN ) model', 'BiRNN architectures', 'bidirectional recurrent neural networks ( BRNN/BiRNN )', 'bidirectional RNN method', 'bidirectional RNN architecture', 'bidirectional RNN classifier', 'bidirectional RNN network', 'deep ( bidirectional ) RNNs', 'BiRNN seq', 'BRNN JTR models', 'BiRNN-based ( distilled ) models', 'PG-BRNN model', 'EN BiRNN', 'BRNN-auto', 'bidirectional recurrent neural networks ( B-RNN )', 'stacked BiRNN', 'bidrectional RNN', 'bidirectional RNN based tagging model', 'BiRNN 100d', 'BiRNN-100d', 'bidirectional tree-structured RNNs']"
138,301,Method,2.9483,2014,"{'2014': 0.1701, '2015': 0.0507, '2016': 0.1784, '2017': 1.3752, '2018': 1.971, '2019': 2.1959, '2020': 2.1718, '2021': 2.6042, '2022': 2.9483}","['end-to-end', 'end-to-end model', 'end-to-end models', 'end-to-end approach', 'end-to-end system', 'end-to-end methods', 'end-to-end method', 'end-to-end approaches', 'end-to-end framework', 'end-to-end architecture', 'end-to-end architectures', 'End-to-end', 'End-to-End', 'End-to-end model', 'End-to-End model', 'End2end', 'End2End', 'end2end', 'End2end model', 'end-to-end systems', 'End-to-end models', 'End-to-end approaches', 'end-to-end modeling', 'end-to-end network', 'End-to-End Model', 'End-to-end systems', 'end-to-end algorithm', 'end2end model', 'End-to-end Methods', 'End-to-End models', 'end-to-end modeling techniques', 'End-to-End System', 'End-to-End Modeling', 'end-to-end modelling', 'end-to-end setting', 'end2end system', 'end-to-end S2I model', 'End-to-end approach', 'end-to end approach', 'End-to-End Architecture', 'end-to-end corpora', 'End-to-End framework', 'end-to-end-framework', 'End-to-End Systems', 'End-to-end architectures', 'End-to-end Architectures', 'end-to-end strategy', 'end-to-end based', 'end-to-end classification model', 'end-to-end mechanism', 'end-to-end structure', 'End-to-end Classifiers', 'end-to-end frameworks', 'corpus-based end-to-end systems', 'end-to-end module network', 'End-to-End Setting', 'end-to-end classifier', 'end-to-end–', 'end2end models', 'End2End system', 'End2End systems', 'End2End )', 'End-to-End 2', 'end2end methods', 'end2end approach']"
139,209,Metric,2.9345,2005,"{'2005': -0.2707, '2006': 0.1273, '2007': -0.0414, '2008': 0.4737, '2009': 0.1939, '2010': 0.6612, '2011': 0.4562, '2012': 0.0753, '2013': 0.9422, '2014': 0.5464, '2015': 1.3523, '2016': 0.7028, '2017': 0.985, '2018': 2.4178, '2019': 1.9146, '2020': 2.0287, '2021': 2.6487, '2022': 2.9345}","['ROUGE-2', 'Rouge-2', 'ROUGE2', 'ROUGE 2', 'ROUGE-1,2', 'Rouge2', 'ROUGE-1/2', 'ROUGE-W-1.2', 'Rouge1/2', 'ROUGE-1 & 2', 'ROUGE-1.2W', 'ROUGE-2 )', 'ROUGE-2 System', 'ROUGE-2-based', 'ROUGE-1 , 2', 'ROUGE W-1.2', 'ROUGE- 2', 'Rouge 2', 'rouge-2', 'ROUGE-2 measure', 'ROUGE-S2', 'ROUGE-2 metric', 'ROUGE package 2', 'ROUGE -1 , 2', 'ROUGE1/2', 'ROUGE 1-2']"
140,816,Method,2.924,2012,"{'2012': -0.0527, '2013': 0.2486, '2014': 0.42, '2015': 2.2772, '2016': 2.924, '2017': 2.5751, '2018': 1.6849, '2019': 1.2186, '2020': 0.4172, '2021': 0.0321, '2022': 0.1866}","['stochastic gradient descent ( SGD )', 'Stochastic Gradient Descent ( SGD )', 'stochastic gradient descent ( SGD ) algorithm', 'Stochastic gradient descent ( SGD )', 'stochastic gradient descent ( SGD ) method', 'Stochastic Gradient Descent ( SGD ) optimizer', 'Stochastic Gradient Descent ( SGD ) algorithm', 'stochastic gradient descent ( SGD ) optimizer', 'stochastic gradient descent ( SGD ) methods', 'mini-batch stochastic gradient descent ( SGD )', 'Stochastic gradient decent ( SGD )', 'distributed stochastic gradient descent ( SGD )', 'stochastic gradient descent ( SGD ) 5', 'minibatch stochastic gradient descent ( SGD ) algorithm', 'stochastic gradient Descent ( SGD ) algorithm', 'stochastic gradient descent method ( SGD )', 'Stochastic gradient descent ( SGD ) method', 'stochastic gradient descent ( SGD ) classifier', 'stochastic gradient descent algorithm ( SGD )']"
141,197,Method,2.8717,2017,"{'2017': 0.0876, '2018': 0.6175, '2019': 1.9447, '2020': 2.4133, '2021': 2.8717, '2022': 2.2266}","['back-translation', 'BT', 'backtranslation', 'back translation', 'Back-translation', 'BT models', 'BT data', 'BT model', 'back-translation method', 'back-translation model', 'back-translation ( BT )', 'topk-BT', 'Backtranslation', 'back-translation technique', 'Back-Translation', 'backtranslation model', 'back-translation approaches', 'Back Translation ( BT )', 'BT EC', 'BT D', 'back translation method', 'back-translation approach', 'BT-XX', 'Back-translation ( BT )', 'back-translation models', 'BT-ES', 'BackTranslation', 'Back translation', 'BT system', 'Back translation ( BT )', 'back translation ( BT )', 'back-translation techniques', 'backtranslation approach', 'T-BT model', 'bt5', 'back-translation strategy', 'Back Translation', 'BT systems', 'Back-Translation ( BT )', 'backtranslation system', 'back-translation mechanism', 'backtranslation data', 'backtranslation technique', 'backtranslation method', 'back-translation model ( BT )', 'back-translation )', 'back translation technique', 'back translation model', 'Back-Translation model', 'backtranslation models', 'backtranslation mechanism', 'backtranslation systems', 'backtranslation dataset', 'backtranslation techniques', 'backtranslation approaches', 'topk-BT models', 'bt ′ c', 'BT D.', 'BT & OP', 'BT-b', 'back-translations', 'Back translations', 'bt', 'back-translation system', 'Back-translation Back translation (', 'BACK TRANSLATION ( BT )', 'back-translation data', 'BT ) data', 'back translation )', 'back-translation—', 'BT )', 'BT methods', 'back-translation methods', 'Back-Translation methods', 'back-translations ( BT )', 'Back-Translation method', 'BT method', 'Back-translation method', 'BT mechanism', 'back-translation based methods', 'backtranslation strategy', 'back-translation method ( BT )', 'back-translation strategies', 'BackTranslation methods', 'back-translation ( BT ) techniques', 'Back-translation Method Back translation', 'back-translation dataset', 'back-translation corpora', 'back-translation corpus', 'back-translation-based', 'BT-l', '+BT', '• + BT', 'BT ( ZH ) model', 'BT-6', 'N+BT models', 'back translation 7', 'topk-BT method', 'topk-BT model', 'backtranslations', 'BT EC data', '• Back-Translation ( BT )', '• Back-translation ( BT )', 'bt/2c +', 'Classifier-BT ”', 'back translation ” approach', 'GPT2-BT']"
142,227,Metric,2.8711,2002,"{'2002': 0.2067, '2003': 0.4777, '2004': 0.7772, '2005': 0.2667, '2006': 0.9139, '2007': 0.7155, '2008': 1.3831, '2009': 1.1018, '2010': 1.5216, '2011': 1.2528, '2012': 2.8711, '2013': 2.0631, '2014': 1.7645, '2015': 2.6217, '2016': 1.6756, '2017': 0.9011, '2018': 1.1453, '2019': 0.8991, '2020': 1.9266, '2021': 1.9187, '2022': 1.5663}","['runtime', 'run-time', 'running time', 'run time', 'runtimes', 'Runtime', 'run times', 'Runtimes', 'run-times', 'running times', 'Run time', 'Run-time', 'Running time', 'Run Time', 'Running Time', 'runtime SA system', 'Run-Time', 'Run-times', 'Run Times', 'Running-time', 'runtime system', 'run-time analysis', 'run-time module', 'run-time models', 'Runing time', 'runtime dataset', 'run-time model', 'Runtime model', 'runtime system 8']"
143,383,Metric,2.8542,2004,"{'2004': 0.5292, '2005': 0.4553, '2006': 0.3256, '2007': 0.2549, '2008': 0.185, '2009': 1.2747, '2010': 1.1875, '2011': 0.1233, '2012': 0.1265, '2013': 0.5732, '2014': 0.8969, '2015': 0.9997, '2016': 1.1493, '2017': 1.1645, '2018': 1.21, '2019': 1.7414, '2020': 2.2529, '2021': 2.1718, '2022': 2.8542}","['Performance', 'performance', 'F1 performance', 'F 1 performance', 'performances', 'Performances', 'F1 Performance', 'F1-performance', 'dev performance', 'QA performance', 'F 1 -performance', 'f 1 performance', 'F-1 performance', 'F1 performances', 'F 1 performances', 'QE performance', 'F performance', 'LF Performances', 'RA performance', 'F1 performance 5', 'peformance', 'Performance ( mF1 )', 'performance @ 10.', 'F1 classification performance', 'F1 Performances', 'Performance-1', 'dev set performance', 'System Performance', 'Performance Model']"
144,190,Method,2.8446,2002,"{'2002': -0.0353, '2003': 0.1793, '2004': 0.2713, '2005': 1.0446, '2006': 1.1057, '2007': 1.9544, '2008': 1.2193, '2009': 1.462, '2010': 1.7789, '2011': 2.2764, '2012': 2.2857, '2013': 1.8716, '2014': 2.8446, '2015': 2.7346, '2016': 2.4964, '2017': 1.1236, '2018': 0.2904, '2019': 0.3357, '2020': 0.4154, '2021': 0.1462, '2022': -0.0111}","['perceptron', 'perceptron algorithm', 'structured perceptron', 'Perceptron', 'perceptron model', 'structured perceptron algorithm', 'perceptrons', 'structured perceptron model', 'Structured Perceptron', 'Perceptron algorithm', 'perceptron models', 'structured perceptrons', 'perceptron method', 'Perceptron Algorithm', 'perceptron classifier', 'structure perceptron', 'perceptron methods', 'perceptron-based approach', 'perceptron approach', 'Perceptrons', 'perceptron approaches', 'perceptron-based algorithm', 'Structured Perceptron model', 'o3-perceptron', 'Structured perceptron', 'structured perceptron approach', 'perceptron framework', 'perceptron-based algorithms', 'perceptron )', 'structured Perceptron algorithm', 'perceptron classifiers', 'perceptron-based', 'structured perceptron strategy', 'structured perceptron models', 'perceptron–based classifiers', 'perceptron-based model', 'structured perceptron 2', 'structured percetron algorithm', 'Structured Perceptrons', 'Perceptrons method', 'Perceptrons model', 'layer perceptrons', 'perceptrons models', 'PERCEPTRON', 'structured-perceptron', 'perceptron–based approach', 'perceptron ) model', 'Perceptron model', 'Perceptron models', 'Perceptron framework', 'Structured perceptron algorithm', 'Structured Perceptron algorithm', 'Structure Perceptron', '-perceptron', 'perceptron strategy', 'perceptron network', 'perceptron-based methods', 'perceptron-based method', 'structured perceptron based model', 'perceptron-based classifiers', 'perceptron system', 'structured perceptron (', 'gold- Perceptron', 'percptron', 'o2-perceptron', 'structured perceptron ( SP )']"
145,166,Dataset,2.8368,2016,"{'2016': 0.1803, '2017': 0.4992, '2018': 0.3433, '2019': 1.4544, '2020': 1.335, '2021': 1.9754, '2022': 2.8368}","['Wikidata', 'WikiData', 'Wikidata5M', 'Wikidata dataset', 'Wikidata 3', 'WIKIDATA', 'Wikidata5M dataset', 'Wikidata5m', 'Wikidata 5', 'wikidata', 'Wikidata 4', 'Wikidata )', 'Wikidata corpus', 'WikiData5M', 'Wikidata model', 'Wikidata 7', 'Wikidata Wikidata 4', 'WIKIDATA data set 4', 'WikiData 5', 'WikiData 3', 'Wikidata-5m', 'Wikidata.For', 'Wikidata data', 'gold Wikidata', ') Wikidata']"
146,579,Method,2.8362,2003,"{'2003': -0.2716, '2004': -0.1653, '2005': -0.0256, '2006': 0.284, '2007': 0.1442, '2008': 0.9693, '2009': 1.0009, '2010': 1.158, '2011': 2.2671, '2012': 1.2973, '2013': 2.8362, '2014': 1.8047, '2015': 1.3664, '2016': 1.0125, '2017': 0.3978, '2018': -0.0226, '2019': -0.1643, '2020': -0.0458, '2021': -0.1536, '2022': -0.2087}","['Kneser-Ney smoothing', 'KN smoothing', 'KneserNey smoothing', 'Kneser-Ney Smoothing', 'Kneser-Ney ( KN ) smoothing', 'Knesser-Ney smoothing', 'Kneser-Ney smoothed models', 'Kneyser-Ney smoothing', 'Kneser–Ney smoothing', 'Kneser-Ney smoothed SLM', 'KN-smoothing', 'KN-smoothed models', 'Kneser-Ney smoothing method', 'Kneser-Ney smoothing ( KN )', 'Kneser-Ney Smoothed', 'Kneser-Ney smoothed model', 'Kneser-Neysmoothing', 'Kneser-Ney smoothing 3', 'KneserNey smoothed SLM', 'Kneser-Kney smoothing', 'Keneser-Ney smoothing', 'Kneser-Neysmoothed models', 'Kneser-ney smoothing', 'KN Smoothing', 'Kneser-Ney-smoothing', 'Kneser Ney smoothing', 'KneserNey Smoothing', 'Kneser-Ney smoothing algorithm', 'KneserNey smoothing approach', 'KneserNey based smoothing methods', 'Kneser-Ney smoothing technique', 'Kneser-Ney Smoothing ( KN )', 'Kneser-Ney smoothing algorithms', 'KN-smoothed', 'Kneser-Ney smoothed', 'Kneser-Ney Smoothing Methods', 'KneserNey smoothing methods', 'Kneser-Ney Smooth', 'KneserNey smoothed models', 'Kneser-Ney Smoothing ( KN PP )', 'Kneser-Ney smoothing ( KN5 )', 'KN smoothing 2', 'KN3 smoothing']"
147,99,Dataset,2.8265,2015,"{'2015': -0.2536, '2016': 0.5394, '2017': 0.3373, '2018': 2.1566, '2019': 2.4748, '2020': 2.2319, '2021': 2.8265, '2022': 2.7927}","['SNLI', 'SNLI dataset', 'SNLI corpus', 'SNLI-hard', 'Stanford Natural Language Inference ( SNLI ) dataset', 'SNLI-VE', 'Stanford Natural Language Inference ( SNLI ) corpus', 'SNLI data', 'Stanford Natural Language Inference ( SNLI )', 'u-SNLI', 'SNLI task', 'δ -SNLI', 'SNLI-TR', 'SNLI models', 'SNLI→SICK', 'SNLI-test', 'Stanford Natural Language Inference', 'Stanford NLI dataset', 'Stanford Natural Language Inference dataset', 'δ-SNLI', 'SNLI train set', 'SNLI-dev', 'SNLI-VE dataset', 'nega-SNLI', 'SNLI CAD', 'SNLI-Hard', 'SNLI test set', 'Stanford Natural Language Inference corpus', 'Stanford NLI ( SNLI )', 'Stanford Natural Language Inference dataset ( SNLI )', 'SNLI )', 'SNLI 3', 'SNLI seed', 'SNLI training set', 'SNLI train', 'SNLI model', 'SNLI Dev', 'SNLI dev set', 'Stanford Natural Language Inference corpus ( SNLI )', 'SNLI data set', 'SNLI tasks', 'SNLI ’ s dev set', 'Seq-Z SNLI', 'SNLI-N', 'SNLI-C', 'SNLI w', 'SNLI Train', 'SNLI-trained NLI model', 'SNLI+SICK', 'SNLI-5', 'mapped SNLI', 'SNLI test', 'SNLI 7', 'u-SNLI dev set', 'Stanford Natural Language Inference ( SNLI ) Corpus', 'Snli', 'Stanford NLI', 'Stanford Natural Language Inference Corpus', 'Stanford Natural Language Inference Dataset', 'SNLI ( Stanford Natural Language Inference )', 'SNLI dev', 'Stanford Natural Language Inference Corpus ( SNLI )', 'Stanford Natural Language Inference ( SNLI ) task', 'SNLI GPT3', 'SNLI GenNLI', 'SNLI Pruthi', 'NLI SNLI 570k pairs', 'SNLI 550K 10K', 'SNLI Val', 'SNLI ex', 'SNLI 1.0', 'Stanford Natual Language Inference', 'SNLI 351k', 'SNLI seed (', 'SNLI train data', 'SNLI-trained models', 'SNLI training data', 'SNLI-trained model', 'SNLI training dataset', 'SNLI CAD dataset', 'SNLI5', 'Stanford Natural Language Inference ( SNLI ) corpus 1', 'SNLI 549K', 'SNLI Hard', 'SNLI hard set', 'SNLI NLI 560k', 'SNLI 6', 'SNLI Con', 'SNLI Premise', 'SNLI SNLI + u-SNLI', 'SNLI u-SNLI', 'u-SNLI dev', 'SNLI task.4', 'SNLI dataset—a', 'u-SNLI ’ s dev', 'SNLI 570K', 'SNLI ( Stanford Natural Language Inference ) corpus', 'Stanford natural language inference ( SNLI ) corpus', 'SNLI NLI Classification', 'Stanford Natural language Inference ( SNLI ) dataset', 'Stanford NLI ( SNLI ) dataset', 'Stanford NLI corpus', 'SNLI ) corpus', 'SNLI Dataset', 'SNLI ) dataset', 'SNLI Model', 'Stanford ( SNLI )', 'SNLI set', 'SNLI ( dev', 'SNLI Dev set', 'The Stanford Natural Language Inference', 'SNLI natural language inference data set', 'Language Inference ( SNLI )', 'SNLI dataset ( Stanford Natural Language Inference )', 'SNLI and', 'SNLI The Stanford Natural Language Inference', 'snli/', 'Stanford Natural Language Inference )', 'Natural Language Inference ( SNLI ) corpus', 'SNLI NLI', 'Stanford natural language inference dataset ( SNLI', 'SNLI natural language inference dataset', 'SNLI inference', 'SNLI data sets', 'Stanford Natural Language Inference ( SNLI ) data set', 'SNLI corpuses', 'Natural Language Inference ( SNLI )', 'SNLI dataset models', 'Stanford Natural Language Inference ( SNLI', 'Stanford Natural Language Inference ( SNLI ) model', 'SNLI dataset )', 'SNLI 550K', 'SNLI SICK', 'Stanford Natural Language Inference task', 'Stanford natural language inference ( SNLI ) task', 'SNLI classification task', 'SNLI classification dataset and task', 'SNLI Stanford language inference task', 'SNLI ” ( dataset ) 6', 'SNLI texts', 'NLVR2 SNLI-VE', 'SNLI splits', 'SNLI-R task', 'SNLI-VE tasks', 'SNLI Dataset 9', 'SNLI dataset 9', 'SNLI NLI 3', 'SNLI dataset 3', 'δ -SNLI ( natural language inference )', 'ford Natural Language Inference ( SNLI )', 'ford Natural Language Inference ( SNLI ) Corpus']"
148,160,Metric,2.8136,2007,"{'2007': -0.1271, '2008': 0.3617, '2009': 0.4417, '2010': 0.3804, '2011': 0.0891, '2012': -0.0495, '2013': 0.6806, '2014': 0.8193, '2015': 1.0259, '2016': 0.7788, '2017': 1.0709, '2018': 1.4205, '2019': 2.1154, '2020': 2.5099, '2021': 2.8136, '2022': 2.7586}","['informativeness', 'Informativeness', 'informativeness theory', 'informativeness ( Info . )', 'Informativeness Measure', 'informativeness measures', 'formativeness', 'Informativeness ( INF )', 'informativeness ( Inf . )', 'informativeness ( Info )', 'informativeness measure', 'Informativeness⇑', 'and Informativeness ( Inf . )', 'Informativeness ( Inf . )', 'informativeness ( info )', 'Informativeness ( INFO )', 'and informativeness ( Info . )', 'Informativeness - we', 'INFORMATIVENESS', 'informativeness classifier', 'informativeness-based measures', 'informativeness-based measure', 'informativeness ( IN )', 'informativeness metrics', 'Informativeness-']"
149,343,Method,2.7992,2020,"{'2020': 1.5228, '2021': 1.6176, '2022': 2.7992}","['DistilBERT', 'DistilBert', 'DistilBERT model', 'DistillBERT', 'distilBERT', 'DistilBERT base', 'DistilBERT 6', 'Distilbert', 'Z-DistilBERT', 'DISTILBERT', 'DistilBERT models', 'DistilBERT 4', 'DistilmBERT', 'mDistillBERT', 'distilbert', 'DistilBERT ∗', 'DistilBERT-based', 'DistilBERT-base', 'distilbert-base-cased', 'DistillBERT 6', 'DISTILLBERT', 'DistillBert', 'DistilBert model', 'distilbert-model', 'DistilBERT architecture', 'DistilBERT classifier', 'DistilBERT )', 'DistilBERT –', 'DistilBERT-based approach', 'DistilBERT-based QA system', 'WS DistilBERT', 'TQ DistilBERT', 'DistilBert6', 'distilmBERT', 'DistillBERT 4', 'distilbert-base-cased 10 model', 'P DistilBERT ∗', 'DistilBERT 2', 'DistillBERT model', '▷ DistilBERT', 'DistilBERT 20', 'DistilBERT 3', 'DistilBERT BASE -6L', 'SDistilBERT BASE', 'A.DistilBERT', 'DistilBert—a', 'DistilBERT †', 'DistilBERT BASE', 'Distilbert-base', 'Distilbert base', 'DistilBERT Base', 'DistilBERT – (', 'DistilBert models', 'DistilBERT base models', 'distilbert-base model', 'DistilBERT-based Model', 'DistilBERT-based model', 'DistilBERT based model']"
150,223,Method,2.7912,2008,"{'2008': 0.1936, '2009': -0.0425, '2010': 0.6967, '2011': 0.2602, '2012': 0.3056, '2013': 0.3934, '2014': 1.075, '2015': 0.6592, '2016': 0.148, '2017': 0.5968, '2018': 1.0913, '2019': 1.5178, '2020': 2.3012, '2021': 2.4716, '2022': 2.7912}","['span', 'span-based model', 'Span', 'span-based models', 'spans', 'span-based', 'span-based methods', 'SPAN', 'Spans', 'span-based method', 'span model', 'span-based approach', 'Span-based methods', 'gold span', 'span-based approaches', 'span-span', 'span classification', 'classifier-based span model', 'span-based classification', 'span-based framework', 'span classifier', 'Span-based', 'span models', 'SPANS', 'Span-based models', 'span-span strategy', 'span strategies', 'span-based system', 'Span-based model', 'Span Model', 'gold spans', 'span-based systems', 'span-based architecture', 'span structure', 'span-based algorithm', 'span mechanism', 'span based model', 'Span-based Model', 'Span-Based Model', 'span-based Model', 'Span-based Models', 'span based models', 'span set', 'SPAN model', 'Gold Spans', 'span-based modeling', 'Span-Based', 'span classification approach', 'SPAN models', 'classifier-based span models', 'span based systems', 'Span-based Method', 'Span-based method', 'span method', 'Span-Based methods', 'Span-based classification', 'span-based strategy', 'span.', 'span *', 'span (', 'big span', 'span-based algorithms', 'span modeling', 'spanning analysis', 'span ( S )', 'Span Analysis', '# spans', 'span-based corpora', '# Span', 'span-based setting', 'span-based dataset', 'Span-based data', 'span classifier approaches', 'span data', 'spans technique']"
151,455,Method,2.7717,2019,"{'2019': 0.2942, '2020': 1.1555, '2021': 1.515, '2022': 2.7717}","['QNLI', 'QNLI dataset', 'QNLI task', 'Question Natural Language Inference ( QNLI )', 'QNLI tasks', 'NQ-NLI', 'QNLI )', 'RoBERTa QNLI', 'QNLI 3', 'F1 QNLI', 'Question-NLI', 'Question Natural Language Inference', 'Question NLI ( QNLI )', 'QNLI Dataset', 'QNLI ) dataset', 'QNLI model', 'QNLI NLI', 'Language Inference ( QNLI )', 'QNLI cell', 'Question NLI ( QNLI', 'Question Natural Language Inference ( QNLI ) dataset', 'QNLI ( 105K )', 'QNLI QA/NLI Classification', 'QNLI QA', 'QNLI 100000']"
152,198,Method,2.7506,2005,"{'2005': 0.46, '2006': 1.1281, '2007': 2.7506, '2008': 1.5511, '2009': 2.2547, '2010': 1.3945, '2011': 2.2298, '2012': 1.4956, '2013': 1.8156, '2014': 2.2356, '2015': 1.0823, '2016': 1.7117, '2017': 1.3967, '2018': 0.5495, '2019': 0.6113, '2020': 0.1045, '2021': 0.0329, '2022': -0.0284}","['PBSMT', 'phrase-based SMT', 'phrase-based SMT system', 'phrase-based SMT systems', 'PB-SMT', 'PBSMT system', 'phrase-based statistical machine translation', 'PBSMT model', 'phrase-based SMT model', 'phrasebased SMT', 'phrase-based statistical machine translation system', 'PBSMT models', 'Phrase-based SMT', 'phrasebased SMT system', 'PBSMT systems', 'phrase-based statistical machine translation ( SMT )', 'phrase-based statistical MT system', 'phrase-based statistical machine translation ( PBSMT )', 'phrase-based statistical machine translation ( SMT ) systems', 'phrase-based SMT models', 'PBSMT-R', 'phrasal SMT', 'phrase-based SMT ( PBSMT )', 'Phrase-based SMT systems', 'PBSMT+RS', 'phrasebased SMT systems', 'PBSMT +', 'phrasal SMT system', 'PB-SMT model', 'phrase based SMT system', 'Phrase-Based SMT', 'phrase based SMT', 'Phrase-based statistical machine translation', 'phrase-based statistical machine translation models', 'phrase-based statistical machine translation ( SMT ) system', 'phrasebased SMT model', 'PB-SMT models', 'phrase-based Statistical Machine Translation ( SMT )', 'phrase-based statistical machine translation systems', 'PB-SMT system', 'phrase-based SMT framework', 'phrasebased statistical machine translation ( SMT ) systems', 'Phrase-based Statistical Machine Translation ( PBSMT )', 'phrase-based statistical machine translation model', 'phrasebased statistical machine translation system', 'phrasebased SMT framework', 'phrasebased SMT approach', 'SMT-based paraphrasing', 'SMT-based paraphrasing model', 'Phrasal SMT systems', 'Phrasal SMT', 'SMT-SemParse', 'parsing-based SMT system', 'phrase-based statistical machine translation ( PB-SMT )', 'Phrase-Based Statistical MT ( PB-SMT )', 'Phrase-Based Statistical Machine Translation ( PBSMT )', 'Phrase-based SMT system', 'Phrase based SMT', 'phrase-based statistical MT', 'phrase-based Statistical Machine Translation', 'phrase-based statistical machine translation ( SMT ) model', 'statistical phrase-based machine translation', 'Phrase-based statistical machine translation systems', 'Phrase-based SMT models', 'Phrase-based Statistical Machine Translation ( SMT ) systems', 'phrase-based statistical machine translation method', 'phrase-base SMT system', 'statistical phrase-based machine translation system', 'statistical phrase-based MT system', 'Phrase-Based Statistical Machine Translation ( PB-SMT ) systems', 'phrase-based statistical machine translation ( SMT ) approaches', 'phrase-based SMT approach', 'phrase-based statistical machine translation ( PBSMT ) system', 'phrase-based SMT ( P )', 'MOSES phrasebased SMT system', 'PBSMT+RS models', 'RWTH phrasebased SMT system', 'PhraseBased SMT', 'phrasebased statistical machine translation', 'Phrasebased Statistical Machine Translation', 'phrasebased statistical machine translation framework', 'phrasebased SMT approaches', 'phrasebased SMT models', 'statistical phrasebased MT model', 'PhraseBased SMT systems', 'phrasebased statistical machine translation method', 'phrasebased statistical machine translation ( SMT )', 'phrasebased Statistical Machine Translation ( SMT )', 'phrasebased statistical machine translation model', 'statistical phrasebased machine translation system', 'SMT-based paraphrasing system', 'SMT-based paraphrases', 'Paraphrasing SMT systems', 'RWTH phrase-based statistical machine translation system', 'phrase-based SMT ( PO-PBMT )', 'PhraseBased Statistical Machine Translation ( PBSMT )', 'phrasebased SMT ( PBSMT )', 'phrasebased statistical machine translation ( PBSMT )', 'phrasebased SMT ( PBSMT ) systems', 'phrasebased statistical machine translation ( PBMT )', 'PhraseBased Statistical Machine Translation ( PBSMT ) system', '\ue018 phrase-based SMT system', 'phrase-level SMT SMS', 'phrasal SMT systems', 'phrasal Statistical Machine Translation ( SMT ) techniques', 'phrasal statistical machine translation ( SMT ) technique', 'phrasal Statistical Machine Translation ( SMT ) technique', 'phrasal SMT techniques', 'phrasal SMT method', 'phrasal statistical machine translation ( SMT ) systems', 'phrase-based statistial machine translation', 'phrasebased SMT sytem', 'second-pass SMT system', 'phrase-based statistic machine translation', 'phrasebased SMT ( PSMT )', 'PBSMT models 7', 'Parsing-Based SMT', 'phrase-based SMT setup', 'Phrase-based statistical machine translation ( PSMT )', 'PBSMT )', 'PB SMT', 'PBSMT Model', 'PB SMT model', 'Phrase Based SMT ( PBSMT )', 'Phrase-Based SMT ( PBSMT )', 'Phrase-based SMT ( PBSMT )', 'Phrase-based SMT ( PB-SMT )', 'phrase-based statistical MT ( PB-SMT )', 'phrase based SMT ( PBSMT )', 'phrase-based statistical Machine translation ( PBSMT )', 'Phrase-Based SMT System', 'Phrase Based SMT model', 'phrase based SMT model', 'phrase-based statistical MT model', 'phrase-based statistical MT ( SMT ) model', 'phrase-based statistical machine translation ( SMT ) method', 'statistical phrase-based MT', 'phrase-based statistical MT systems', 'Phrase–based SMT systems', 'phrase based statistical MT models', 'phrase based statistical machine translation ( SMT ) systems', 'phrase-based Statistical Machine Translation ( SMT ) systems', 'phrase-based statistical MT method', 'PB-SMT systems', 'phrase-based SMT ( PBSMT ) models', 'phrase-based statistical machine translation ( PBSMT ) models', 'statistical phrase based MT system', 'Phrase-based statistical machine translation methods', 'phrase-based statistical MT ( PBSMT ) systems', 'Phrase-Based Statistical Machine Translation ( PBSMT ) systems', 'phrased-based SMT system', 'phrase-based approach to statistical machine translation ( PBSMT', 'phrase-based SMT ( PBMT )', 'Phrase-based SMT ( PBMT )', 'phrase-based statistical machine translation ( PBMT )', 'phrase-based SMT techniques', 'Phrase-based SMT techniques', 'Phrase-based statistical machine translation techniques', 'statistical phrase-based machine translation approach', 'phrase SMT system', 'phrase-based machine translation system ( SMT )', 'phrase-based statistical machine translation ( PBMT ) systems', 'PBSMT-based', 'Phrase-based Statistical Machine Translation ( SMT ) approaches', 'SMT ( phrase-based )', 'Phrase-Based SMT Architectures', 'phrase-based SMT architectures', 'phrase-based statistical machine translation ( MT ) model', 'phrase-based ( PB ) SMT system', 'phrase-based SMT approaches', 'phrase-based statistical machine translation approach', 'phrase-based statistical machine translation ( SMT', 'phrase-based machine translation approach ( SMT )', 'PBSMT approaches', 'phrased-based statistical machine translation ( SMT ) system', 'PBSMT dataset', 'statistical phrase-based MT approaches', 'PB SMT framework', 'phrase based system ( PB-SMT )', 'phrased-based SMT', 'phrase-based statistical machine translation ( SMT ) approach', 'phrase-based statistical machine translation system ( PBSMT )', 'nrc phrase-based SMT']"
153,538,Method,2.7456,2018,"{'2018': -0.1392, '2019': 0.2007, '2020': 0.5557, '2021': 1.1343, '2022': 2.7456}","['few-shot learning', 'FSL', 'Few-shot learning', 'few-shot learning methods', 'Few-Shot Learning', 'Few-shot Learning', 'few-shot learning models', 'few-shot learning method', 'few-shot learning approaches', 'few-shot learning ( FSL )', 'Few-Shot Learning ( FSL )', 'few-shot learning model', 'few-shot learning framework', 'FSL model', 'Few-shot learning ( FSL )', 'few-shot learning techniques', 'FSL methods', 'few-shot learning strategies', 'few-shot learner', 'few-shot learners', 'FSL 2', 'FSL ED models', 'FSL ED', 'Few-Shot learning', 'few shot-learning', 'few-shot Learning', 'few shot learning', 'few-shot-learning', 'Few-shot learning technique', 'few-shot learning technique', 'FSL framework', 'Few-shot learning systems', 'SotA few-shot learning methods', 'Few-shot learning methods', 'Few shot learning ( FSL )', 'Few-shot Learning ( FSL )', 'Few-Shot Learning Few-shot learning ( FSL )', 'FSL Model', 'Few-shot Learning Models', 'FSL models', 'Few-shot learning approaches', 'Few-Shot Learning FSL', 'few-shot learning approach', 'FSL approach', 'FSL )', 'few-shot learning )', 'few-shot learning architecture', 'few-shot learning ( FSL ) strategy', 'metric-based FSL', 'metric-based few-shot-learning methods', 'metric-based FSL methods', 'FSL algorithm', 'Shot Learning FSL', 'few shot learning ( T1 )', 'Few-shot Learner']"
154,224,Metric,2.7434,2005,"{'2005': -0.1764, '2006': -0.1317, '2007': -0.2092, '2008': -0.1943, '2009': -0.0454, '2010': -0.0306, '2011': -0.247, '2012': -0.0367, '2013': -0.1508, '2014': 0.4085, '2015': 0.5524, '2016': 0.5261, '2017': 1.2035, '2018': 1.5716, '2019': 1.752, '2020': 1.2412, '2021': 2.4106, '2022': 2.7434}","['MSE', 'mean squared error', 'mean squared error ( MSE )', 'mse', 'mean square error', 'Mean Squared Error ( MSE )', 'mean square error ( MSE )', 'Mean Square Error ( MSE )', 'Mean Squared Error', 'mean-squared error', 'Mean Square Error', 'Mean squared error', 'mean-square error', 'mean-squared error ( MSE )', 'mean-squared-error', 'Mean square error ( MSE )', 'MSE )', 'mean squared errors', 'meansquared error', 'Mean squared error by model', 'Mean-squared error ( MSE )', 'Mean Squared Error ( MSE', 'Mean 174 Square Error ( MSE )', 'MSE 1', 'mean-square error ( 2 )', 'mean-square-error', 'Mean square error', 'Mean-Squared Error', 'Mean-Squared-Error', 'mean-squared-error ( MSE )', 'Mean squared error ( MSE )', 'Mean-Squared Error ( MSE )', 'mean-square-error ( MSE )', 'mean-square error ( MSE )', 'MSE-based', 'squared error ( MSE )', 'mean squared error mse', 'mean squared error ( MSE', 'mean square error )', 'mse )', '( MSE )', 'Mean Square Error ( MSE ) method', 'M SE', 'mean-square errors', 'mer se', 'MSE models', 'MSE W', 'meansquared-error', 'MSE SCC']"
155,148,Metric,2.7405,2003,"{'2003': -0.318, '2004': -0.1107, '2005': -0.299, '2006': -0.1892, '2007': -0.0664, '2008': 0.3962, '2009': 0.0843, '2010': 0.8268, '2011': 0.2197, '2012': 0.8494, '2013': 0.8861, '2014': 0.7446, '2015': 1.0944, '2016': 0.9118, '2017': 1.2231, '2018': 1.373, '2019': 1.9016, '2020': 2.2337, '2021': 2.4479, '2022': 2.7405}","['coherence', 'Coherence', 'coherence model', 'coherence models', 'COHERENCE', 'coherence measures', 'Coherence models', 'coherence measure', 'coherence-based approach', 'Coherence Model', 'coherence-based algorithm', 'Coherence⇑', 'COHerence', 'coherence modeling', 'coherence theory', 'Coherence measures', 'coherence-based', 'coherence theories', 'coherence algorithm', 'network coherence', 'τ coherence', 'coherence∗', 'incoherence', 'INCOHERENCE', 'Coherence model', 'Coherence Models', 'coherence method', 'Coherence Modeling', 'Coherence modeling', 'Coherence-based approaches', 'coherence-based approaches', 'coherence-based method', 'coherence-based algorithms', 'Coherence Measure', 'Coherence Algorithm', 'Coherence structure', 'coherence methods', 'coherence system', 'coherence approaches', 'coherence value', 'coherences', 'coherence mechanisms', 'coherence metrics', 'coherence set', ') coherence', 'coherence measure ( CM )', 'Coherence measure ( CM )']"
156,108,Metric,2.7398,2002,"{'2002': -0.3695, '2003': -0.2252, '2004': -0.1702, '2005': 0.4553, '2006': 0.3, '2007': 0.3156, '2008': 0.3358, '2009': 0.4331, '2010': 1.1609, '2011': 1.2963, '2012': 0.7087, '2013': 1.0379, '2014': 0.7245, '2015': 0.5826, '2016': 1.0657, '2017': 0.5363, '2018': 1.5684, '2019': 1.6398, '2020': 2.2137, '2021': 2.354, '2022': 2.7398}","['Hits @ 1', 'Hits @ 10', 'HIT', 'HITS', 'hits', 'hits @ 1', 'hit', 'Hit @ 1', 'HITs', 'HITS algorithm', 'Hits @ k', 'Hits @ 3', 'Hits @ N', 'hits @ 10', 'HITS @ 10', 'Hit @ 10', 'hits @ k', 'Hits @ K', 'Hit @ k', 'HIT corpus', 'HIT @ 1', 'Hits', 'Hit', 'Hits @ n', 'Hit @ 5', 'Human Intelligence Task ( HIT )', 'HITS @ 1', 'Hit @ 3', 'Hits @ 5', 'Hit @ K', 'HITS @ N', 'Hits @ 50', 'Human Intelligence Tasks ( HITs )', 'HIT ( Human Intelligence Task )', 'HIT model', 'hit list', 'hit-list', 'HITS-aT', 'hit @ 10', 'HITS @ 50', 'train-HIT', 'human intelligence tasks ( HITs )', 'Hit @ N', 'HITS @ n', 'Hits @ N.', 'hit @ 1', 'HIT @ 50', 'Hits @ 1 ( H @ 1 )', 'hit @ 5', 'HITS model', 'HITS-based', 'HITS-sF', 'HITS-aF', 'hit @ k', 'HITS @ K', 'HIT @ K', 'HIT @ k', 'HIT 4', 'Hits @ 10s', 'hits @ 50', 'hit @ 2', 'TV_HITS', 'WTM_HITS', 'HITS )', 'human intelligence task ( HIT )', 'HITS measure', 'HITS-based method', 'Hits @', 'hit lists', 'HITS-sT', 'Hits @ 1/ K', 'Hits @ 1/K', 'Ref-hit', 'HITS gGmbH', 'near-hits', 'Method F1 Hits @ 1', 'F1 Hits @ 1', 'HITS @ 10 ”', 'HITS @ k.', 'HIT-SCIR', 'HIT-IR', 'Hit @ 1 metrics', 'Hits @ 1.', 'Hits @ 30', 'Hits @ 10 ( H @ 10 )', 'Hits @ 1/3', 'HITS_HUB', 'graph-based HITS algorithm', 'HIT-2', 'HIT @ 3', 'hits @ 5', 'HITS @ 5', 'HIT 5', 'HIT LTP package', 'HIT S algorithm', 'HITS-based approach', 'Hit-List', 'All−HIT R', 'top-K ( HITS @ K )', 'Web-hits', 'web hits', 'hit val', 'HITS @ k', 'Hits @ K.', 'Dataset Method Hits @ k', 'HITS-like algorithm', 'HIT 6', 'Hits @ kpk', 'Hits @ 100', 'Hits @ 3 / 10', 'HIT @ 5/10', 'Hits at N ( HIT @ N )', 'PMC hits', 'Hit @ n', 'hits @ n', 'Hit @ N.', 'Hits @ n metrics', 'human intelligence tests', 'F1 hits', 'hit @ 20', 'Hits @ 20', 'HIT price', 'hit-18.1', 'HIT 1', 'Hits @ 1 )', 'HIT corpus 1', 'Hit Hit Hit @ 1', 'HITS @ 1s', 'HIT @ 10', 'Hits @ 10 )', 'HITS @ 10.', 'Hit @ 10 ( Hit10 )', 'hit types', 'Train-HIT', 'train & train-HIT', 'hits @ 10 ( H @ 10 )', 'HIT S A', 'hit-PAST', 'hit @ l', 'NUR Hits @ 1', 'HITS graph algorithm', 'graph-based algorithm HITS', 'Hits @ 2', 'Hit @ 2', 'HITS @ 3', 'HITS @ 3s', 'F1 Hit @ 5', 'hits @ 1/20', 'HIT 7', 'EA Hits @ 1', 'EA hits @ 1', 'AMI HITs', 'HIT corpus 5', 'Hits-at-K ( Hits @ K )', 'seed-hit', 'Human Intelligence Task 89', 'Hit @ ALL', 'ICRC-HIT', 'Hits @ 1 Hits @ 3 Hits @ 5', 'hits @ P', 'HIT—', 'HITS-', 'Hit #', 'HIT S', 'HIT-corpus', 'HIT Human Intelligence Task', 'hits hits', 'HITS algorithms', 'HITS-based Approach', 'human intelligence tasks', 'HITS system', 'HITs ( Human Intelligence Tasks )', 'HITS strategy', 'Hit points', 'HIT @', 'HIT ( task )', 'Intelligence Tasks ( HITs )', 'Hits @ metric', 'HITS approach', 'Hits @ metrics', 's HIT points', 'hits neg', 'hit @ 1 hit @ 3 hit @ 5 hit @ 10', 'HIT R']"
157,486,Method,2.7106,2014,"{'2014': 0.0063, '2015': 0.089, '2016': 0.9289, '2017': 0.8836, '2018': 1.5068, '2019': 2.7106, '2020': 2.0697, '2021': 2.6444, '2022': 2.5234}","['validation', 'validation set', 'validation sets', 'validation dataset', 'Validation', 'validation data', 'validation-set', 'validation framework', 'C4 validation set', 'validations', 'K-fold validation', 'ID validation data', 'ID validation set', 'Validation set', 'validation algorithm', 'validation data set', 'validation strategies', 'validation method', 'k-fold validation', 'n-fold validation', 'validation 2', 'validation τ', 'MCD validation sets', 'valdation set', 'validation ( dev )', 'validation metrics', 'dev/validation set', 'dev.-set validation', 'validation set method', 'Validation Methods', 'validation methods', 'K-Fold Validation', 'vaidation sets', 'validation set 1']"
158,321,Method,2.7104,2020,"{'2020': 1.8819, '2021': 2.7104, '2022': 2.671}","['ALBERT', 'ALBERT model', 'Albert', 'ALBERT-base', 'AlBERT', 'ALBERT base', 'ALBERT models', 'Alberti', 'ALBERT-large', 'ALBERT large', 'ALBERT )', 'ALBERT-base model', 'albert', 'ALBERT – Base', 'ALBERT Base', 'ALBERT-Large', 'ALBERT-based model', 'ALBERT ” model', 'Albert models', 'Albert model', 'AlBERT model', 'ALBERT classifier model', 'ALBERT ]', 'ALBERT (', 'Albert-base', 'ALBERT-Base', 'AlBERT base', 'AlBERT-base', 'ALBERT BASE', 'AlBERT large', 'ALBERT Large', 'base ALBERT', 'Albert-base model', 'ALBERT Base model', 'AlBERT architecture', 'ALBERT architecture', 'ALBERT-large model', 'ALBERT-based classifier', 'AlBERTo', 'Alberto', 'ALBERT †', 'ALBERT ’ s approach', 'ALBERT 3']"
159,638,Method,2.7008,2018,"{'2018': 1.6007, '2019': 2.7008, '2020': 2.0241, '2021': 1.7796, '2022': 1.6033}","['selfattention', 'selfattention mechanism', 'selfattention networks', 'SelfAttention', 'selfattention network', '+self-attention', 'selfattentions', 'Selfattention', 'selfattention architecture', 'selfattention module', 'selfattention mechanisms', 'selfattentional models', 'selfattention model', '+self-attn', 'task-based selfattention mechanism', 'slot selfattention', 'selfattentional approach', 'selfattentional', 'SelfAttention Mechanism', 'Selfattention networks', 'selfattention based models', 'selfattention models', 'selfattention-based approaches', 'selfattention strategy', 'selfattention modules']"
160,113,Method,2.6846,2001,"{'2001': -0.0025, '2002': 0.6216, '2003': 0.292, '2004': 0.5391, '2005': 1.2945, '2006': 0.7029, '2007': 1.1939, '2008': 0.6375, '2009': 0.459, '2010': 0.8239, '2011': 0.9229, '2012': 1.0638, '2013': 1.2224, '2014': 2.2672, '2015': 2.5673, '2016': 2.6846, '2017': 1.4669, '2018': 2.1739, '2019': 1.9626, '2020': 1.3359, '2021': 1.771, '2022': 2.3279}","['NN', 'kNN', 'KNN', 'k-NN', 'k NN', 'KNN classifier', 'nearest neighbors', 'nearest neighbor', 'k-nearest neighbors', 'nearest neighbor approach', 'nearest neighbor algorithm', 'KNN-EC', 'K-NN', 'kNN method', 'Nearest Neighbor', 'k-nearest neighbor', 'k-NN classifier', 'knn', 'k -NN', 'KNNs', 'nearest neighbor classifier', 'k-nearest neighbor algorithm', 'NN method', 'Nearest Neighbor ( NN )', 'KNN-based', 'NN )', 'nearest neighbor models', 'k -nearest neighbors', 'K-Nearest Neighbor', 'kNN algorithm', 'KNN method', 'kNN approach', 'nearest neighbour', 'NN-based model', 'K-nearest neighbors', 'k-nearest neighbors algorithm', 'nearest neighbor classification', 'Nearest Neighbors', 'Nearest neighbors', 'nearest-neighbor', 'nearest neighbor methods', 'R-KNN', 'kNNs', 'k-Nearest Neighbor', 'KNN model', 'k nearest neighbor mechanism', 'KNN approach', 'KNN system', 'nearest-neighbour', 'nearest neighbors ( NN )', 'Nearest neighbor', 'nearest-neighbor classifier', 'k-NN algorithm', 'k-nearest neighbor classifier', 'NN classifier', 'nearest neighbor system', 'nearest neighbor ( NN )', 'K-nearest neighbors ( KNNs )', 'k-nearest neighbors ( kNN )', 'k nearest neighbors', 'K-Nearest Neighbors', 'k-NN method', 'k NN method', 'k NN classifier', 'k NN mechanism', 'kNN classifiers', 'k-nearest neighbors approach', 'k-NN-based', 'nearest neighbour classifier', 'nearest neighbour methods', 'nearest-neighbour analysis', 'nearest neighbor method', 'NN algorithm', 'nearest neighbor approaches', 'nearest-neighbor approach', 'nearest neighbor technique', 'nearest neighbor model', 'nearest neighbor based approach', 'K-Nearest Neighbors ( KNN )', 'k-Nearest Neighbors', 'k-Nearest Neighbor ( kNN )', 'K Nearest Neighbor ( KNN )', 'K-nearest neighbor', 'k -nearest neighbor', 'k -NN method', 'k NN model', 'kNN model', 'k-NN model', 'k-nn classifier', 'K NN classifier', 'kNN mechanism', 'k-nearest neighbor approach', 'K-Nearest Neighbor approach', 'k-nearest neighbors classifier', 'k-NN approach', 'KNN models', 'k-NN classifiers', 'KNN set', 'kNN-cos', 'nearest-neighbour approach', 'nearest neighbour algorithm', 'nearest-neighbour algorithm', 'Nearest Neighbours', 'nearest neighbours', 'nearest neighbour classification', 'nearest-neighbour models', 'Nearest neighbor classification', 'nearest-neighbors', 'nearest-neighbor method', 'Nearest Neighbor Classifier', 'nearest-neighbor approaches', 'Nearest Neighbor approach', 'nearest neighbor techniques', 'nearest neighbor analysis', 'nearest neighbor-based approach', 'structured nearest neighbor approach', 'K-Nearest Neighbours', 'k -nearest neighbours', 'k-nearest neighbour classifier', 'k-nearest neighbour approach', 'k-nearest neighbour algorithm', '\ue01c k -NN', 'KNN-3', 'K-nearest neighbors ( KNN )', 'k-Nearest Neighbors ( kNN )', 'K-Nearest-Neighbors ( KNN )', 'k -nearest neighbors ( k NN )', 'k-Nearest Neighbors ( KNN )', 'k-nearest neighbors ( k-NN )', 'K-nearest neighbor ( KNN )', 'K-Nearest Neighbor ( KNN )', 'k-nearest-neighbor ( kNN )', 'k nearest neighbor', 'KNN algorithm', 'kNN classifier', 'k nearest neighbor ( k NN ) mechanism', 'k-nearest neighbor ( kNN ) algorithm', 'Nearest Neighbor ( k-NN )', 'K-nearest neighbor algorithm', 'K-nearest-neighbor algorithm', 'k -NN-based', 'K-NN )', 'KNN classification', 'KNN classification algorithm', 'k -nearest-neighbor ( k NN ) classifier', 'KNN strategy', 'RN- k NN', 'NN ’ s NN', 'nearest-neighbour classifier', 'nearest-neighbour methods', 'nearest neighbour approach', 'nearest-neighbour method', 'nearest neighbours Model', 'nearest neighbours strategy', 'nearest neighbour approaches', 'nearest neighbour techniques', 'nearest-neighbour classifiers', 'nearest-neighbours approach', 'nearest-neighbors classifier', 'nearest-neighbor classification', 'Nearest Neighbor based methods', 'nearest neighbor-based methods', 'Nearest-Neighbor (', 'Nearest-neighbor', 'Nearest Neighbor classifier', 'Nearest neighbor classifier', 'nearest-neighbor models', 'Nearest Neighbor Methods', 'nearest-neighbor methods', 'nearest neighbor ( NN ) algorithm', 'nearest-neighbor techniques', 'nearest neighbor strategy', 'nearest neighbors strategy', 'nearest neighbor based approaches', 'nearest-neighbor based approaches', 'nearest-neighbor based models', 'Nearest Neighbor based models', 'nearest neighbor classification model', 'nearest-neighbor classification model', 'Nearest-neighbor analysis', 'nearest-neighbor analysis', 'nearest neighbor based classification', 'nearest-neighbor model', 'Nearest Neighbors method', 'nearest neighbors method', 'nearest neighbors algorithm', 'nearest neighbors algorithms', 'nearest neighbor classification algorithms', 'nearest-neighbor classification algorithms', 'nearest neighbor-based', 'nearest neighbor classifiers', 'nearest-neighbor classifiers', 'nearest neighbor-based method', 'Nearest Neighbor-based method', 'nearest neighbors classifiers', 'nearest neighbors approach', 'nearest neighbor algorithms', 'k-NN †', 'k-NN—we', '\ue001 k-Nearest Neighbors ( kNN )', 'KNN10', 'k-NN ”', 'k nearest neighbours', 'k-nearest neighbours', 'K-Nearest neighbours', 'K-nearest neighbour classifier', 'K-Nearest Neighbour approach', 'k-nearest neighbour classifiers', 'k-nearest neighbour classification', 'k nearest-neighbour algorithm', 'k-nearest-neighbour algorithm', 'K Nearest Neighbour', 'k-nearest neighbour', 'k-Nearest Neighbours classifier', 'f KNN', 'CR Nearest neighbor', 'k-NN pe', 'nearest neighbor classification ( 1-kNN )', 'k -NN algorithm 3', 'kNN QA model', 'nearest-neighbor QA', 'k nn', 'k-nearest-neighbors ( KNN )', 'K-Nearest Neighbors ( K-NN )', 'k-nearest-neighbors ( k-NN )', 'k-Nearest Neighbors ( k-NN )', 'k -nearest neighbors ( KNN )', 'k-nearest neighbors ( Knn )', 'k nearest neighbors ( k-NN )', 'k-nearest neighbors ( KNN )', 'K Nearest Neighbors', 'K-Nearest-Neighbors', 'k Nearest Neighbors', 'k -nearest-neighbors', 'K-nearest-neighbors', 'k-nearest neighbor ( kNN )', 'k-nearest neighbor ( k-NN )', 'k nearest neighbor ( k-NN )', 'k-nearest neighbor ( KNN )', 'k-Nearest Neighbor ( KNN )', 'k-nearest-neighbor ( KNN )', 'k-nearest-neighbor ( knn )', 'k-Nearest Neighbor ( k-NN )', 'k-Nearest-Neighbor', 'k-nearest-neighbor', 'k -nearest neighbor model ( kNN )', 'k -NN algorithm', 'k-nn algorithm', 'K-NN algorithm', 'k -NN model', 'K-NN model', 'k -NN classifier', 'KNN Classifier', 'k-NN methods', 'k -NN methods', 'K-Nearest Neighbors ( KNN ) algorithm', 'K -nearest neighbors ( KNN ) algorithm', 'k-Nearest Neighbors ( kNN ) model', 'K nearest neighbors ( K NN ) model', 'K NN classifier models', 'k -Nearest-Neighbors ( k -NN ) method', 'k nearest neighbors ( k-NN method', 'k-nearest neighbors ( kNN ) method', 'K-Nearest-Neighbor ( KNN', 'k nearest neighbor ( k NN ) approaches', 'k-nearest neighbors models', 'k-nearest neighbor ( KNN ) algorithm', 'k-Nearest Neighbor ( kNN ) algorithm', 'k-Nearest Neighbor ( k-NN ) algorithm', 'k-nearest-neighbor ( kNN ) algorithm', 'k-nearest neighbor ( k-NN ) algorithm', 'nearest Neighbor ( KNN )', 'Nearest Neighbor ( kNN )', 'k-Nearest-Neighbor classifier', 'k-Nearest Neighbor classifier', 'K-nearest neighbor classifier', 'K-Nearest Neighbor ( KNN ) method', 'k-nearest-neighbor algorithm', 'k-Nearest Neighbor algorithm', 'K Nearest Neighbor Algorithm', 'KNN ( k Nearest Neighbors )', 'KNN ( k-Nearest-Neighbors )', 'k nearest neighbor approach', 'K nearest neighbor approach', 'k -nearest neighbors classifier', 'K-nearest neighbors classifier', 'k nearest neighbors classifier', 'k-nearest neighbors ( KNN ) classifier', 'K-Nearest Neighbors ( KNN ) classifier', 'KNN based classifier', 'KNN-based classifier', 'k nearest neighbors algorithm', 'k-nearest neighbor method', 'k-nearest neighbors method', 'k-Nearest Neighbors Method', 'KNN-based classification algorithms', 'k-Nearest Neighbor system', 'k-nearest neighbor ( k-NN ) classifiers', 'K-NN approach', 'k -NN approach', 'k-NN system', 'K-NN models', 'KNN classifiers', 'k nearest neighbor algorithm ( kNN )', 'K Nearest Neighbor algorithm ( KNN )', 'k nearest neighbor algorithm ( k-NN )', 'Nearest Neighbors ( k-NN )', 'nearest neighbors ( KNN )', 'KNN ( K-Nearest Neighbor )', 'Nearest Neighbors ( k-NN ) method', 'Nearest Neighbors ( kNN ) method', 'k-nearest-neighbors approach', 'K-NN K-NN', 'k-nearest neighbor algorithms ( KNN )', 'kNN/', 'kNN )', 'k-nearest neighbor classification method ( kNN )', 'K-Nearest Neighbor ( KNN ) approach', 'K-nearest-neighbor ( k-NN ) approach', 'kNN-based algorithms', 'k-nearest neighbor classification model', 'k-nearest neighbor classifiers', 'K-nearest neighbor classifiers', 'kNN classification', 'k-NN algorithms', 'KNN algorithms', 'kNN algorithms', 'KNN-based approach', 'k NN-based approach', 'KNN-based methods', 'k-Nearest Neighbors techniques', 'K-Nearest Neighbors ( KNN ) -based algorithms', 'KNN classification methods', 'KNN algorithm System', 'k-NN based model', 'k-nearest-neighbor algorithms', 'k NN-based strategy', 'K-Nearest Neighbor ( KNN ) strategy', 'k -nearest neighbors mechanism', 'SR - kNN', 'nearest neighbour ”']"
161,326,Dataset,2.6754,2018,"{'2018': 0.0012, '2019': 0.8007, '2020': 1.3277, '2021': 2.3131, '2022': 2.6754}","['STS-B', 'STSB', 'STSb', 'STS Benchmark', 'STS benchmark', 'STS benchmarks', 'STS-B dataset', 'STS-B dev set', 'STS-Benchmark', 'STS benchmark dataset', 'STS-B datasets', 'STS-b', 'Semantic Textual Similarity Benchmark ( STS-B )', 'STS benchmark ( STSb )', 'STS-B task', 'STS Benchmark dataset', 'STS-B-filter', 'STS benchmark ( STS-B )', 'STSB task', 'STS Benchmark test set', 'STS-benchmark', 'STS-Benchmark dataset', 'STSb dataset', 'STS-B tasks', 'STSb-train', 'STS 2017 benchmark', 'STS-B data', 'STS Benchmark ( STS-B )', 'Semantic Textual Similarity Benchmark ( STSB )', 'STS-B benchmark', 'STSB dev set', 'STS-B dataset ( dev )', 'STSb datasets', 'STS benchmark datasets', 'STS benchmark test set', 'STS-B RS', 'Dataset STS-B SICK-R STS-12', 'STS-B tsak', 'STS benchmark training set', 'STS-B trained', 'STS benchmark training data', 'fact-based STSB', 'STSb ( English )', 'STS-B 1k', 'SemEval ( STS-B )', 'STS-Benchmark testset', 'STS Benchmark 4', 'STS-Benchmark ( a )', 'STS-B + STR', 'stsb', 'Semantic Textual Similarity benchmark ( STS-B )', 'STS benchmark ( STSB )', 'STS-B-dev', 'STS-B benchmark ( STS-B )', 'semantic textual similarity ( STS ) benchmark', 'Semantic Textual Similarity ( STS ) benchmark', 'Semantic Textual Similarity ( STS ) Benchmark Model', 'STS-Benchmark ( STS', 'Semantic Textual Similarity Benchmark ( STS', 'semantic textual similarity ( STSB ) data', 'STS-B model', 'STS-b Model', 'STSb )', 'STS-B )', 'STS Benchmark ( STS-b', 'Semantic Textual Similarity Benchmark ( STS-B', 'STS benchmark ( STSb ) dataset', 'The STS benchmark ( STSb )', 'Semantic Textual Similarity ( STS ) benchmarks', 'STSb tasks', 'Semantic Textual Similarity ( STS ) benchmark datasets', 'STS-B 5', 'STS-B-test sets', 'STSb-test', 'STS benchmark training set 13', 'SICK-R STS-B', 'WiC STS-B', 'Semantic Text Similarity Benchmark ( STSB )']"
162,933,Method,2.6659,2019,"{'2019': 0.0633, '2020': 0.7526, '2021': 2.6659, '2022': 2.367}","['encoder-decoder Transformer', 'Transformer encoder-decoder model', 'Transformer encoder-decoder', 'encoder-decoder Transformer model', 'encoder-decoder transformer', 'Transformer encoder-decoder architecture', 'transformer-based encoder-decoder architecture', 'transformer-based encoder-decoder model', 'Transformer-based encoder-decoder model', 'encoderdecoder Transformer', 'encoder-decoder transformer model', 'encoderdecoder Transformer model', 'transformer encoder-decoder model', 'encoder-decoder transformers', 'Transformer encoderdecoder architecture', 'transformer encoder-decoder', 'Transformer-based encoder-decoder architecture', 'Transformer encoderdecoder model', 'transformer encoderdecoder', 'Transformer encoderdecoder', 'transformer encoder-decoder architecture', 'Encoder-Decoder Transformer', 'Transformer encoder-decoder structure', 'Transformer-based encoder-decoder models', 'Transformer encoder-decoder models', 'transformerbased encoder-decoder architecture', 'Transformer-based encoderdecoder model', 'Transformer-based encoderdecoder models', 'encoderdecoder transformer model', 'Transformer-based encoderdecoder framework', 'transformer-based encoderdecoder', 'encoder-decoder Transformers', 'Transformer Encoder-Decoder structure', 'transformer-based encoder-decoder models', 'transformer encoder-decoder models', 'encoder-decoder Transformer-based model', 'encoder-decoder Transformer models', 'transformerbased encoder-decoder', 'DeFormer Transformer Encoder', 'encoder-decoder transfomer', 'Transfomer-based encoder-decoder architecture', 'encoder-decoder transformer model 3', 'transformer encoderdecoder model', 'transformer encoderdecoder architecture', 'Transformer EncoderDecoder', 'Transformer encoderdecoder framework', 'transformer encoderdecoder framework', 'encoderdecoder transformer', 'transformer-based encoderdecoder model', 'encoderdecoder transformer framework', 'transformer-based encoderdecoder network', 'encoderdecoder Transformer architecture', 'encoderdecoder transformer models', 'Transformer-based encoderdecoder architecture', 'encoderdecoder transformer-based model', 'full transformers ( encoder-decoder )', 'encoder-to-decoder Transformer model', 'Transformers encoder-decoder', 'transformer encoder-decoder system', 'encoder–decoder transformer', 'transformer-based encoder-decoder', 'Transformer-based encoder-decoder', 'Transformer Encoder-Decoder', 'Transformer-based encoder–decoder architecture', 'encoder-decoder ( transformer ) model', 'Transformer encoder-decoder framework', 'transformer based encoder-decoder model', 'Transformer based encoder-decoder model', 'Transformer encoder-decoder )', 'Transformer-based encoder-decoder network', 'transformer-based encoder-decoder network', 'encoder-decoder Transformer architecture', 'encoder-decoder transformer architecture', 'encoder decoder transformer architecture', 'Transformer based encoder-decoder framework', 'transformer-based encoder-decoder framework', 'encoder-decoder Transformer networks', 'encoder-decoder Transformer framework', 'Transformer encoder-decoder structures', 'encoder-decoder-based Transformer', 'transformer encoder-decoders', 'transformer encoder-decoder architectures', 'encoder-decoder TRANSFORMER-based model', 'Transformer encoder-decoder network', 'encoder-decoder transformer models', 'Transformer-based encoder-decoder approaches', 'encoder-decoder based Transformer models', 'Transformer-based encoder-decoder architectures', 'Transformerbased encoder-decoder architecture', 'Transformerbased encoder-decoder framework', 'Transformerbased encoder-decoder model', 'Transformerbased encoder-decoder methods', 'transformer based encoder-decoder model BART', 'BART-based encoder-decoder Transformer model', 'encoder–decoder NAR Transformer', 'tree-structured transformer encoder-decoder architecture', 'Transformer Decoders ( Dec )']"
163,351,Metric,2.6508,2006,"{'2006': -0.0357, '2007': -0.0307, '2008': 0.0772, '2009': 0.0036, '2010': 0.6465, '2011': 0.0859, '2012': 0.4816, '2013': 0.7717, '2014': 0.262, '2015': 0.8869, '2016': 0.2962, '2017': 0.7021, '2018': 1.7087, '2019': 1.717, '2020': 2.5263, '2021': 2.2758, '2022': 2.6508}","['mean', 'Mean', 'MEAN', 'MEANT', 'mean value', 'means', 'mean values', 'Means', 'mean method', 'meaning-based approach', 'meaning', 'Meaning', 'MEAN model', 'Mean #', 'mean-', 'meaning-based framework', 'Mean model', 'MEANS', 'S- means', 'meanT', 'corpus mean', 'mean technique', 'MEANT-', 'MEANT algorithm', 'mean mean mean mean', 'mean based', 'MEAN -strategy', 'mean mean', 'mean metric value', 'mean ( Mean )', 'meaning model', 'MEANT metric', 'means analysis', '.mean ( )']"
164,349,Method,2.6415,2016,"{'2016': 0.3038, '2017': 1.5489, '2018': 1.8933, '2019': 2.6415, '2020': 1.7707, '2021': 1.0468, '2022': 0.1931}","['BiGRU', 'bidirectional GRU', 'BIGRU-LWAN', 'biGRU', 'BIGRU', 'BiGRU model', 'bidirectional GRUs', 'bidirectional gated recurrent unit ( GRU )', 'BiGRUs', 'bidirectional Gated Recurrent Unit ( GRU )', 'BiGRU+2ATT', 'GC-BIGRU-LWAN', 'EMR-biGRU', 'DC-BIGRU-LWAN', 'bidirectional-GRU', 'bidirectional GRU network', 'Bidirectional GRU', 'bidirectional gated recurrent unit ( BiGRU )', 'biGRU model', 'C-BIGRU-LWAN', 'bidirectional Gated Recurrent Unit ( BiGRU )', 'bidirectional gated recurrent units ( GRU )', 'bidirectional GRU ( BiGRU )', 'GNC-BIGRU-LWAN', 'biGRU GAUSS', 'biGRU REG', 'BiGRU F', 'BIGRUs', 'ZERO-BIGRU-LWAN', 'biGRU + AC', 'biGRU POISSON', 'DN-BIGRU-LWAN', 'DNC-BIGRU-LWAN', 'BiGRU-last', 'BiGRU generic', 'ZERO-BIGRU-LAWN', 'bidirectional gated recurrent unit', 'bidirectional Gated Recurrent Unit', 'BiGRU network', 'bidirectional gated recurrent units', 'bidirectional Gated Recurrent Unit ( GRU ) network', 'directional GRU', 'PAIGE-BiGRU', 'biGRU IF model', 'Bidirectional Gated Recurrent Unit ( BGRU )', 'BIGRU-LWAN ( L2V )', 'bidirectional gated recurrent unit network', 'BiGRU-based models', 'BIGRU-LAWN', 'Ind BiGRU models', 'bidirectional GRU recur', 'multitask BiGRU models', 'biGRU based S2S model', 'BiGRU2', 'biGRU +AC-gold', 'BiGRU T', 'BiGRU F 2', 'BiGRU ( 16 )', 'BiGRU+2ATT models', 'BiGRU_Stacked *', 'stacked BiGRU', 'bidirectional Tree-GRU', 'BiGRU ( 32 )', 'bidirectional Gated Recurrent Units ( MBi-GRU )', 'multi-layer bidirectional GRU', 'bidirectional gated recurrent unit ( BGRU )', 'bidirectional GRU ( BGRU )', 'BIGRU ( BIGRU-LWAN )', 'biGRU ORD', 'biGRU ORD +', 'bidrectional GRU', 'biGRU+AC', 'biGRU+AC-gold', 'Bidirectional gated recurrent unit ( BiGRU )', 'BiGRU classifiers', 'BiGRU classifier', 'bidirectional Recurrent Gated Unit ( BiGRU )', 'bidirectional Gated Recurrent Units', 'bidirectional gated recurrent units ( GRUs )', 'Bidirectional Gated Recurrent Units ( GRU )', 'bidirectional Gated Recurrent Units ( GRU )', 'bidirectional Gated Recurrent Units ( GRUs )', 'bidirectional GRU ( biGRU )', 'bidirectional-GRU ( BiGRU )', 'bidirectional GRU model', 'bidirectional GRU :', 'BiGRU cells', 'Gated Recurrent Units ( BiGRU )', 'gated recurrent units ( biGRU )', 'Bidirectional Gated Recurrent Units ( BiGRU )', 'bidirectional Gated Recurrent Units ( BiGRU )', 'BiGRU models', 'biGRU models', 'Gated Recurrent Unit ( BiGRU )', 'BiGRU/GRU', 'bidirectional GRU cell', 'Bidirectional GRU cell', 'bidirectional gated recurrent unit ( BiGRU ) models', 'BiGRU networks', 'bidirectional GRU network ( biGRU )', 'bidirectional GRU s', 'Bidirectional Gated Recurrent Unit ( BiGRU ) network']"
165,185,Method,2.6351,2010,"{'2010': -0.0484, '2011': 0.0859, '2012': 0.6415, '2013': 0.9071, '2014': 1.4255, '2015': 1.3725, '2016': 2.067, '2017': 2.3605, '2018': 1.9483, '2019': 2.2049, '2020': 2.0916, '2021': 2.6351, '2022': 2.4061}","['crowdsourcing', 'Crowdsourcing', 'crowdsourcing approach', 'crowdsourcing methods', 'crowdsourcing techniques', 'crowdsourcing method', 'crowdsourcing system', 'crowdsourcing framework', 'crowdsourcing corpus', 'crowdsourcing approaches', 'crowdsourcing models', 'crowdsourcing 2', 'crowdsourcing data', 'crowdsourcing-based method', 'crowdsourcing strategy', 'crowdsourcing technique', 'crowdsourcing model', 'crowdsoucing', 'paid crowdsourcing', 'crowsourcing', 'crowdousrcing', 'crowdsourcing job', 'crowdsourcing jobs', 'Crowdsourcing 5', 'Crowdsourcing 1', 'crowdsourcing 1', 'crowdsourcing 4', 'Crowdsourcing Data', 'Crowdsourcing system', 'Crowdsourcing approaches', 'crowdsourcing based system', 'crowdsourcing mechanisms', 'crowdsourcing frameworks', 'crowdsourcing ( CS )', 'Crowdsourcing ”', 'crowdsroucing', 'crowdscouring', 'web crowdsourcing', 'crowdsourcing 3', 'GMB crowdsourcing']"
166,255,Dataset,2.6344,2004,"{'2004': -0.1057, '2005': 0.7759, '2006': 0.511, '2007': 2.4328, '2008': 0.8616, '2009': 1.8396, '2010': 1.9504, '2011': 1.7133, '2012': 2.6344, '2013': 1.6895, '2014': 1.8104, '2015': 2.116, '2016': 2.0575, '2017': 1.6562, '2018': 1.4367, '2019': 1.0731, '2020': 0.6168, '2021': 0.4896, '2022': 0.2228}","['CoNLL shared tasks', 'CoNLL-X shared task', 'CoNLL shared task', 'CoNLL-2009 shared task', 'CoNLL-2014 shared task', 'CoNLL-2012 shared task', 'CoNLL 2000 shared task', 'CoNLL-2000 shared task', 'CoNLL-2008 shared task', 'CoNLL 2003 shared task', 'CoNLL-2005 shared task', 'CoNLL 2008 shared task', 'CoNLL 2009 shared task', 'CoNLL 2005 shared task', 'CoNLL-2003 shared task', 'CoNLL 2006 shared task', 'CoNLL 2018 shared task', 'CoNLL 2007 shared task', 'CoNLL 2012 Shared Task', 'CoNLL 2017 Shared Task', 'CoNLL-X Shared Task', 'CoNLL ’ 12 shared task', 'CoNLL 2012 shared task', 'CoNLL Shared Task', 'CoNLL Shared Tasks', 'CoNLL-14 shared task', 'CoNLL-2013 shared task', 'CoNLL-2012 Shared Task', 'CoNLL shared task data', 'CoNLL-2009 Shared Task', 'CoNLL-2008 Shared Task', 'CoNLL 2011 shared task', 'CoNLL-2011 shared task', 'CoNLL-2005 Shared Task', 'CoNLL 2007 Shared Task', 'CoNLL 2009 Shared Task', 'conll-2003 shared task', 'CoNLL 2012 Shared Task data', 'CONLL shared task', 'CoNLL-2006 shared task', 'CoNLL-2014 Shared Task', 'CoNLL 2014 shared task', 'CoNLL 2017 shared task', 'CoNLL 2008 shared task dataset', 'CoNLL ’ 2004 Shared Task', 'CoNLL-2011 Shared Task', 'CoNLL ’ 03 shared task data', 'CoNLL2003 shared task', 'CoNLL-09 shared task', 'CoNLL 2000 shared task dataset', 'CoNLL2012 shared task', 'CoNLL–2012 Shared Task', 'CoNLL-2012 shared task dataset', 'CoNLL-2012 shared task data set', 'CoNLL ’ 17 shared task', 'CoNLL-2004 shared task', 'CONLL-2007 shared task', 'CoNLL-2007 shared tasks', 'CoNLL ’ 00 shared task data', 'CoNLL 2018 Shared Task', 'CoNLL-2014 shared task data', 'CoNLL ’ 09 shared task', 'CoNLL2009 shared task', 'CoNLL 2009 Shared Task data sets', 'CoNLL 2003 shared tasks', 'CONLL 2003 shared task', 'CoNLL 2003 shared task dataset', 'CoNLL-2003 shared task dataset', 'CoNLL 2003 shared task data', 'CoNLL-X shared task data', 'CoNLL 2009 shared task organizers', 'CoNLL-2009 shared-task organizers', 'CoNLL-2012 Shared Task data', 'CoNLL 2012 Shared Task dataset', 'CoNLL2012 shared task dataset', 'CoNLL shared-tasks', 'CoNLL shared task dataset', 'CoNLL Shared Task dataset', 'CoNLL2007 shared task', 'CoNLL 2007 shared tasks', 'CoNLL 2007 shared task data', 'CoNLL-2011/2012 shared tasks', 'CoNLL18 shared task', 'CoNLL2014 shared task', 'CoNLL-2014 shared task dataset', 'CoNLL-2014 shared tasks', 'CoNLL 2006/2007 shared-task', 'CoNLL 2006/2007 shared tasks', 'CoNLL–2009 shared task', 'CoNLL 2009 shared task dataset', 'CoNLL 2006–7 shared tasks', 'CoNLL2013 shared task', 'CoNLL 2013 shared task', 'CoNLL shared task 2006', 'CoNLL 2008 Shared Task', 'CoNLL-2008 shared task data set', 'CoNLL ’ 03 shared tasks', 'CoNLL ’ 03 shared task', 'CoNLL 2003 Shared Task', 'CoNLL-2003 shared task data', 'CoNLL shared tasks 4', 'CoNLL 2002 and 2003 shared tasks', 'CoNLL2002/2003 shared tasks', 'CoNLL 2004 and 2005 shared tasks', 'CoNLL-2002 shared task', 'CoNLL 2002 shared task', 'CoNLL X shared task', 'CoNLL-03 shared task', 'CoNLL2005 Shared Task', 'CoNLL2000 shared task', 'CoNLL-2000 Shared Task', 'conll-2000 shared task', 'CoNLL-2001 shared task', 'CoNLL-2005 shared task winner system', 'CoNLL shared task 2012', 'CoNLL-2009 sharedtask', 'CoNLL 2009 sharedtask', 'CoNLL2012 Shared Task', 'Conll-2012 Shared Task', 'CoNLL-2012 shared task data', 'CoNLL 2012 shared task data', 'CoNLL 2012 shared task dataset', 'CoNLL -2012 shared task dataset', 'CoNLL 2012 shared task (', 'CoNLL 2012 Shared Tasks', 'CoNLL-2012 Shared Tasks', 'CoNLL 2012 shared tasks', 'CoNLL-2014 shared task test set', 'CoNLL2014 shared task teams', 'CoNLL 2001 shared task data 1', 'CoNLL 2017 sharedtask data', 'CoNLL 2001 shared task test set', 'CoNLL ’ X Shared Tasks', 'CoNLL 2016 shared task data 5', 'CoNLL2015 Shared Task', 'CoNLL-2000 shared task 3', 'CoNLL-2003 shared task 4', 'CoNLL ’ 03 shared-task 3 corpora', 'CoNLL2015 Shared Task training data', 'CoNLL ’ 00 shared-task 5 corpus', 'CoNLL ’ X Shared Task organizers', 'CoNLL 2017 shared task test sets', 'CoNLL2009 shared task 1', 'CoNLL-2010 Shared Task', 'CoNLL 2010 shared task', 'CoNLL 2010 Shared Task', 'CoNLL 2000 NPC shared task', 'CoNLL 2017 shared task setup', 'CONLL Shared Task', 'CoNLL-shared task', 'CoNLL shared-task data', 'CoNLL shared task data sets', 'CoNLL shared task data set', 'CoNLL-2004 shared task—the', 'CoNLL-X shared task training sets', 'canonical CONLL 2003 shared task dataset', 'CoNLL 2007 shared-task', 'ConLL 2007 shared task', 'CoNLL-2007 Shared Task', 'CoNLL 2007 shared task system', 'CoNLL Shared Task 2007', 'CoNLL Shared Task-2007', 'CoNLL shared task 2007', 'CoNLL shared task 2018', 'CoNLL-2011/2012 shared task systems', 'CoNLL-2011/2012 Shared Tasks', 'CoNLL Shared Task 2009', 'CoNLL shared task scorer 7', 'CoNLL-2016 Shared Task', 'CoNLL 2016 shared task', 'CoNLL07 shared task', 'CoNLL 2006 Shared Task', 'CoNLL-2006 shared tasks', 'CoNLL 2006 shared tasks', 'CoNLL 2006 Shared Task data sets', 'CoNLL18 Shared Task', 'CoNLL-2008 open-track models', 'CoNLL Shared Task 2010', 'CoNLL-14 Shared Task', 'CoNLL-14 shared task dataset', 'CoNLL 2012-Shared Task 4', 'CoNLL-2018 shared task setting', 'ConLL 2018 shared task', 'CoNLL-14 shared task test dataset', 'CoNLL 2006 and 2007 shared-task test data', 'CoNLL2014 Shared Task', 'CoNLL 2014 shared task dataset', 'CoNLL-2014 shared task ,', 'Conll 2017 shared task', 'CoNLL 2017 shared task data', 'CoNLL 2000 shared task setup', 'CoNLL ’ 09 shared task data', 'CoNLL ’ 09 Shared Task', 'CoNLL-2008 and 2009 shared tasks', 'CoNLL 2008 and 2009 shared tasks', 'CoNLL 2008-2009 shared tasks', 'CoNLL 2008/2009 shared tasks', 'CoNLL 2006 and 2007 shared tasks', 'CoNLL shared tasks 5', 'CoNLL 2006 shared task 1', 'CoNLL 2002-3 shared tasks', 'conll-2009 shared task', 'CoNLL2009 Shared task', 'CoNLL-2009 shared Task', 'CoNLL-2009 Shared Task dataset', 'CoNLL-2009 shared task dataset', 'CoNLL 2009 Shared Task data set', 'CoNLL2019 shared task', 'CoNLL2005 shared task 3', 'CoNLL-2005 shared task 3', 'CoNLL 2009 SRLonly task data', 'CoNLL-2013 shared task system', 'CoNLL-2013 shared tasks', 'CoNLL Shared Task 2006', 'CoNLL 2007 shared trees', 'CoNLL 2003 shared task date', 'CoNLL 2011 shared task 2', 'CoNLL2008 Shared Task', 'CoNLL 2008 shared-task dataset', 'CoNLL 2008 Shared Task data', 'CoNLL-2008 shared task data', 'CoNLL-2008 Shared Task data', 'CoNLL 2008 Shared Task data set', 'CoNLL 2008 shared tasks', 'CoNLL-2008 shared tasks', 'CoNLL 2008 shared task corpus', 'CoNLL 2012 shared task test set', 'CONLL 2000 text chunking shared dataset 6', 'ConLL-2003 Shared Task', 'CoNLL 2003 shared-task', 'CoNLL2003 Shared Task', 'CoNLL-2003 Shared Task', 'CONLL 2003 shared task data', 'CoNLL-2003 Shared Task data', 'CoNLL shared task series', 'CoNLL-09 Shared task', 'CoNLL09 shared task', 'CoNLL shared task 4', 'CoNLL 2002 and 2003 shared task', 'CoNLL 2002 and 2003 shared task data sets', 'shared CoNLL Shared Task 2009', 'CoNLL-2002 Shared Task', 'CoNLL 2002 shared task dataset', 'CoNLL ’ 2010 shared task', 'CoNLL 2006 shared task 8', 'CoNLL-X shared-task', 'Conll-x shared task', 'CONLL-X Shared Task', 'CoNLL-X shared task data sets', 'CoNLL-X shared tasks', 'CoNLL 2006-2008 Shared Tasks', 'CoNLL ’ 09 shared task 2', 'CoNLL 2018 Shared Task script 7', 'CoNLL-05 Shared Task', 'CoNLL-05 Shared-task', 'CoNLL-05 shared task', 'CoNLL shared task 2011', 'CoNLL-2005 shared task software 6', 'CoNLL03 shared task', 'CoNLL03 shared task dataset', 'CoNLL03 shared task data set', 'CoNLL 2007 shared-task organizers', 'CoNLL shared task reports', 'CoNLL task reports', 'CoNLL-2005 shared task software', 'CoNLL00 shared task', 'CoNLL shard task 2012', 'CoNLL-2008 Shared Task organizers', 'CoNLL-2005 shared tasks', 'conll2005 shared task', 'CoNLL 2005 Shared Task', 'CoNLL-2005 shared task dataset', 'CoNLL 2005 shared task dataset', 'CoNLL-2005 shared task data', 'CoNLL 2005 shared task data', 'CoNLL-2005 shared task data set', 'CoNLL2005 shared task framework', 'CONLL-2000 shared task', 'CoNLL2000 shared task dataset', 'CoNLL-2000 shared task dataset', 'CONLL2000 sharetask', 'CoNLL-2000 shared tasks', 'CoNLL 2008 shared task conversion', 'CoNLL 2001 shared task', 'CoNLL ’ 11 shared task', 'CoNLL shared task work', 'CoNLL2008 Shared Task 1', 'newswire CoNLL-2005 shared task dataset', 'CoNLL Shared Task 2017', 'CoNLL12 shared task', 'CONLL 2009 shared task processing tools']"
167,300,Method,2.6122,2017,"{'2017': 0.2612, '2018': 1.4745, '2019': 2.6122, '2020': 1.7707, '2021': 2.4286, '2022': 2.0058}","['adversarial', 'adversarial data', 'adversarial model', 'adversarial networks', 'adversarial methods', 'adversarial approach', 'adversarial classifier', 'adversarial network', 'AdversarialQA', 'adversarial framework', 'Adversarial', 'adversarial classifiers', 'adversarial method', 'adversarial dataset', 'adversarial strategy', 'adversarial models', 'adversarial approaches', 'adversarial module', 'ADVERSARIAL', 'adversarial set', 'Adversarial networks', 'adversarial techniques', 'adversarial mechanism', 'Adversarial Networks', 'Adversarial methods', 'adversarial sets', 'adversarial architecture', 'adversarial algorithm', 'adversarial setting', 'adversarial algorithms', 'Adversarial Data', 'Adversarial Dataset', 'adversarial technique', 'adversarial dev set', 'adversarial-based methods', 'Adversarial QA', 'AdversarialQA dataset', 'AdversarialQA data', 'adversarial QA model', 'adversarial ( Adv )', 'Adversarial Methods', 'Adversarial data', 'Adversarial models', 'adversarial )', 'adversarial strategies', 'adversarial cases', 'adversarial classification', 'Adversarial Q & A', 'adversarial model ( A )', 'adversarial ( A i )', 'adversarial QA models', 'AdversarialQA corpus', 'Adversarial ( S2A )', 'Adversarial ”', 'adversarial model 5', 'adversarial ( ADV )', 'Adversarial ( ADV )', 'adversarial adv', 'adversarial model ( Adv )', 'Adversarial approaches', 'Adversarial classifiers', 'Adversarial Model', 'adversarial # Model', 'Adversarial model', 'adversarial modules', 'Adversarial Network', 'Adversarial sets', 'Adversarial Sets', 'Adversarial dataset', 'Adversarial techniques', 'adversarial mechanisms', 'Adversarial method', 'Adversarial-based approaches', 'Ad versarial', 'adversarial-based', 'Adversarial based', 'adversarial—', 'Adversarial )', 'adversarial measures', 'adversarial case', 'adversarial system', 'adversarial analysis', 'adversarial dev sets', 'adversarial-based techniques', 'Adv ersarial data', 'adversarial base', 'adversarial based algorithms', 'adversarial systems', 'adversarial network approach']"
168,432,Dataset,2.5834,2019,"{'2019': 0.3576, '2020': 1.0816, '2021': 1.505, '2022': 2.5834}","['CoLA', 'COLA', 'CoLA dataset', 'Corpus of Linguistic Acceptability ( CoLA )', 'cola', 'CoLA task', 'CoLa', 'Corpus of Linguistic Acceptability ( CoLA', 'AJ-CoLA', 'Cola', 'COLA dataset', 'CoLA ( RoBERTa )', 'roberta-base-CoLA', 'CoLA corpus', 'CoLA tasks', 'CoLA 2 dataset', 'Linguistic acceptability ( LA )', 'COLA training set', 'CoLA training set', 'cola & acid', 'μ COLA', 'linguistic acceptability ( CoLA )', 'CoLA Corpus', 'CoLA data', 'linguistic acceptability task', 'linguistic acceptability task ( CoLA )', 'CoLA OLM OLM-S', 'soda 2 cola']"
169,273,Method,2.5588,2016,"{'2016': -0.0743, '2017': -0.1875, '2018': -0.0345, '2019': 0.3576, '2020': 1.2876, '2021': 1.5602, '2022': 2.5588}","['teacher model', 'teacher models', 'teacher', 'Teacher model', 'Teacher', 'teacher network', 'Teacher Model', 'teacher classifier', 'teacher dataset', 'Teacher models', 'Teacher Models', 'teachers', 'teacher ” model', 'Teacher ”', 'teacher model 2', 'teacher architecture', 'Teacher-Classifier', '` teacher models', 'base ( teacher ) model', 'large teacher', 'model-based teacher', 'teacher networks', 'large teacher model']"
170,467,Dataset,2.5582,2002,"{'2002': 0.2297, '2003': 0.0135, '2004': 1.1492, '2005': 1.1955, '2006': 0.8116, '2007': 1.0118, '2008': 1.2107, '2009': 0.8856, '2010': 0.5045, '2011': 0.758, '2012': 0.955, '2013': 2.5582, '2014': 1.1066, '2015': 0.4174, '2016': 0.4596, '2017': 0.8465, '2018': 0.4966, '2019': 0.4674, '2020': 0.6368, '2021': 0.2731, '2022': -0.0726}","['LDC', 'LDC corpora', 'LDC corpus', 'LDC data', 'LDC05', 'LDC algorithm', 'LDC 2', 'LDC dataset', 'LDC 3', 'LDC1.3', 'LDC corpora 2', 'LDC 1', 'LDC 4', 'LDC corpora 5', 'LDC corpora 3', 'LDC news corpora', 'LDC 7', 'P Ldc', 'LDC05 corpus', 'LDC 1 data', 'LDC corpus 1', 'LDC 6', 'LDC news corpus', 'LDC 64', 'LDC corpora 4', 'LDC MTC', 'LDC15 )', 'LDC17 )', 'LDC corpus 5', 'LDC 3 corpus', 'LDC FBIS corpus', 'LDC 125', 'LDC 9', 'LDC99T42 )', 'LDC98T31', 'MTO LDC', 'OTO LDC', 'LDC1.3 set', 'LDC1.3 data', 'LDC data 1', 'LDC corpora 6', 'w/ LDC ”', 'LDC93T3A )', 'LDC †', 'LDC95T7 )', 'news corpora LDC', 'LDC news', 'Ldc', 'Latent-Descriptor Clustering', 'Latent-Descriptor Clustering ( LDC )', 'LDC approach', 'LDC model', 'LDC )', 'LDC packages', 'LDC97S44', 'LDC98S71', 'LDC 14', 'LDC 21', 'LDC data 4', 'LDC98T30', 'LDC98T26']"
171,279,Dataset,2.5569,2016,"{'2016': 0.3038, '2017': 0.8114, '2018': 1.0071, '2019': 1.8504, '2020': 2.0223, '2021': 2.5569, '2022': 2.3294}","['crowdworkers', 'crowd workers', 'crowd-workers', 'Crowdworkers', 'crowd worker', 'crowdworker', 'crowd-worker', 'Crowd-workers', 'crowdworker task', 'Crowd workers', 'Crowd Workers', 'Crowdworker', 'Crowdworker data', 'crowdworker data', 'crowdworkets', 'crowdworkers Dataset']"
172,2314,Tool,2.5536,2015,"{'2015': 0.3831, '2016': 1.9568, '2017': 2.5536, '2018': 0.3099, '2019': -0.0391, '2020': -0.1999}","['Theano', 'Theano 2', 'theano', 'THEANO', 'Theano 3', 'Theano 5', 'Theano 7', 'theano/', 'Theano framework', 'theano package 3', 'Theano 8', 'Theano 1', 'Theano system', 'Theano package', 'Theano 4']"
173,594,Method,2.5325,2016,"{'2016': 0.1594, '2017': 1.1626, '2018': 2.2322, '2019': 2.5325, '2020': 2.1955, '2021': 2.1066, '2022': 1.6358}","['sequence-to-sequence ( seq2seq )', 'sequence-to-sequence ( Seq2Seq )', 'sequence-to-sequence ( seq2seq ) models', 'sequence-to-sequence ( seq2seq ) model', 'sequence-to-sequence ( Seq2Seq ) model', 'sequence-to-sequence ( Seq2Seq ) models', 'sequence-tosequence ( seq2seq )', 'sequence-tosequence ( Seq2Seq )', 'Sequence-to-sequence ( seq2seq )', 'sequence to sequence ( seq2seq )', 'Sequence-to-Sequence ( Seq2Seq )', 'Sequence-to-sequence ( seq2seq ) models', 'sequence-to-sequence model ( Seq2Seq )', 'sequenceto-sequence ( seq2seq )', 'Sequence-to-Sequence ( Seq2Seq ) model', 'sequence-to-sequence ( SEQ2SEQ )', 'sequence to sequence ( seq2seq ) models', 'sequenceto-sequence ( Seq2Seq )', 'sequence-tosequence ( seq2seq ) models', 'sequence to sequence ( seq2seq ) model', 'sequence to sequence ( Seq2Seq )', 'sequence-to-sequence ( Seq2Seq ) framework', 'sequence-to-sequence ( seq2seq ) task', 'sequence-tosequence ( Seq2Seq ) model', 'sequence-to-sequence ( SEQ2SEQ ) model', 'sequence to sequence ( Seq2Seq ) models', 'sequence-to-sequence ( seq2seq ) framework', 'sequenceto-sequence ( seq2seq ) models', 'Sequence to Sequence ( Seq2Seq )', 'sequenceto-sequence ( Seq2Seq ) model', 'Sequence-to-Sequence ( seq2seq ) models', 'Sequence-to-Sequence ( Seq2Seq ) models', 'Sequence-to-sequence ( Seq2Seq ) models', 'sequence to sequence model ( seq2seq )', 'sequence-to-sequence ( seq2seq ) approaches', 'sequence-to-sequence ( seq2seq ) approach', 'sequence-to-sequence ( seq2seq ) architectures', 'Sequenceto-Sequence ( Seq2Seq )', 'sequence-to-sequence ( seq2seq ) tasks', 'sequence-tosequence ( Seq2Seq ) models', 'sequence-tosequence ( seq2seq ) framework', 'Sequence-to-sequence ( Seq2Seq ) Model', 'sequence to sequence ( Seq2Seq ) model', 'Sequence-to-sequence ( SEQ2SEQ ) model', 'Sequence-to-sequence ( seq2seq ) network', 'Sequence-to-sequence ( Seq2Seq )', 'Sequence to sequence ( Seq2seq )', 'Sequence to Sequence ( seq2seq )', 'sequence-to-sequence framework ( Seq2Seq )', 'sequence to sequence ( Seq2seq ) models', 'Sequence-to-sequence ( SEQ2SEQ ) models', 'Sequence to sequence ( Seq2Seq ) models', 'sequence-to-sequence ( SEQ2SEQ ) models', 'Sequence-to-sequence ( Seq2seq ) models', 'Sequence to sequence ( seq2seq ) models', 'network-based sequence-to-sequence ( seq2seq ) models', 'network-based sequence-to-sequence ( Seq2Seq ) models', 'Sequence-to-Sequence model ( Seq2Seq )', 'Sequence to sequence model ( SEQ2SEQ )', 'sequence-to-sequence model ( seq2seq )', 'seq2seq ( sequence-to-sequence ) model', 'sequence-to-sequence architectures ( seq2seq )', 'Sequence-to-sequence ( seq2seq ) approaches', 'sequence-to-sequence ( seq2seq', 'Sequence-to-Sequence Seq2Seq', 'sequence-to-sequence ( Seq2Seq ) methods', 'sequence-to-sequence ( seq2seq ) methods', 'Sequence-to-sequence ( Seq2Seq ) modeling', 'Sequence-to-Sequence ( seq2seq ) modeling', 'sequence-to-sequence ( seq2seq ) architecture', 'sequence-to-sequence models ( SEQ2SEQ )', 'Sequence-to-Sequence ( Seq2Seq ) type models', 'sequence-tosequence ( Seq2Seq ) task', 'sequence-tosequence ( seq2seq ) task', 'sequenceto-sequence ( seq2seq ) approach', 'sequenceto-sequence ( Seq2Seq ) models', 'Sequenceto-Sequence ( Seq2Seq ) models', 'sequenceto-sequence ( seq2seq ) model', 'sequenceto-sequence model ( seq2seq )', 'sequenceto-sequence ( seq2seq ) framework', 'sequence-tosequence ( seq2seq ) model', 'Sequence-tosequence ( Seq2Seq ) model', 'Sequence-toSequence ( seq2seq ) model', 'sequence-tosequence ( Seq2seq )', 'SOTA sequence-tosequence ( Seq2Seq )', 'sequence-tosequence ( SEQ2SEQ ) models', 'sequence-tosequence architecture ( Seq2Seq ) model', 'sequence-tosequence ( seq2seq ) approach', 'sequence-tosequence architecture ( seq2seq )', 'sequence-tosequence ( Seq2Seq ) architecture', 'sequence-tosequence model ( Seq2Seq )', 'SOTA sequence-tosequence ( Seq2Seq ) model']"
174,631,Method,2.5311,2019,"{'2019': 0.556, '2020': 1.3533, '2021': 2.5311, '2022': 2.5241}","['fairseq', 'Fairseq', 'FAIRSEQ', 'FairSeq', 'fairseq 4', 'Fairseq framework', 'fairseq 3', 'fairseq 5', 'fairseq 7', 'Fairseq 4', 'fairseq framework', 'Fairseq 7', 'fairseq 11', 'FAIRSEQ model', 'FAIRSEQ system', 'FairSeq framework', 'Fairseq 3', 'fairseq 6', 'Fairseq 5', 'FAIRSEQ 1', 'FairSeq 1', 'fairseq 1', 'fairseq 14', 'fairseq 9', 'Fairseq ‡', 'FairSeq ‡', 'FairSeq 4', 'FAIRSeq', 'fairSeq', 'FAIRSEQ systems', 'fairseq/', 'Fairseq Base', 'Fairseq Models', 'Fairseq models', 'Fairseq ( base )', 'fairseq data', 'fairseq framework 3', 'fairseq 15', 'fairseq 8', 'Fairseq 8', 'fairseq 10']"
175,245,Method,2.5258,2001,"{'2001': 0.2882, '2002': 0.2989, '2003': -0.0528, '2004': 0.4399, '2005': 0.8607, '2006': 1.3839, '2007': 1.8044, '2008': 1.4261, '2009': 1.096, '2010': 1.8351, '2011': 1.9155, '2012': 1.8603, '2013': 2.2149, '2014': 2.5258, '2015': 2.2349, '2016': 2.0746, '2017': 1.8942, '2018': 1.6331, '2019': 1.5105, '2020': 1.6331, '2021': 1.9331, '2022': 2.1259}","['learning algorithm', 'learning', 'Learning', 'metric learning', 'learning-based approach', 'structured learning', 'learning-based approaches', 'learning-based methods', 'learning-based', 'structure learning', 'structured learning approach', 'learning-based system', 'learning-based systems', 'learning-based models', 'learning-based method', '∆-learning', 'Metric Learning', 'learning approach', 'learning mechanism', 'learning based methods', 'metric learning methods', 'learnt', 'learning model', 'learning methods', 'learning method', 'learn', 'Learning-based approaches', 'learning approaches', 'structured learning methods', 'learning-based framework', 'learning models', 'Learning Algorithm', 'structure learning framework', 'structured learning algorithm', 'metric learning framework', 'metric-learning', 'metric learning method', 'learning based approaches', 'learning techniques', 'learning technique', 'learning system', 'base learning algorithm', 'learned', 'model-based learning', 'learnt model', 'learning mechanisms', 'S2S learning', '∆-learning approach', '∆-learning framework', 'Ψ -learning', 'Learning based methods', 'Learning-based methods', 'Metric learning', 'Learning Method', 'LEARN', 'learning strategy', 'Learning based approaches', 'learning theory', 'network learning', 'learning based approach', 'learned-based approach', 'learning systems', 'learning-based metrics', 'Learned', 'learning-based classifier', 'structure learning method', 'corpus-based learning method', 'metric learning algorithm', 'learnt models', 'learning-based model', 'structured learning model', 'learning architecture', 'learning to', 'structure learning module', 'structure learning algorithm', 'metric learning approach', 'learning-based architecture', 'metric-based learning', '∆-Learning', '∆-learning algorithm', 'learning based', 'Learning Methods', 'learning based models', 'Learning-based models', 'learning-based architectures', 'learned network', 'metric-learning method', 'Learn', 'Learning-based Method', 'structured learning method', 'set learning', 'Learning algorithm', 'Learning Theory', 'Learning theory', 'learning strategies', 'Learning Strategies', 'metric-learning methods', 'Learning-based systems', 'learning based systems', 'Network Learning', 'metric learning based approaches', 'learning -based approach', 'Learnt', 'based learning mechanism', 'Learning techniques', 'Learning Approach', 'Learning Approaches', 'Base learning algorithm', 'classifier learning approaches', 'classifier learning', 'Structured Learning Methods', 'Learning systems', 'Learning Systems', 'classification-based learning', 'learnt strategy', 'learnt Strategy', 'learning framework', 'structured learning techniques', 'Structured Learning Techniques', 'Learning model', 'learning-based mechanisms', 'corpus-based learning approach', 'Structured Learning', 'Structured learning', 'learning-based techniques', 'Our learning algorithm', 'model learning', 'structured learning models', 'Structured learning models', 'Structure Learning', 'structure-learning', 'structure learn', 'structured learning technique', 'learning based model', 'metric learning-based', 'metric learning based', 'learned base network', 'learns', 'learning theory.', 'metric-based learning framework', 'learning-based analysis', 'metric-learn', 'corpus-based learning techniques', 'learning corpus', 'learnt system', 'learning architectures', 'learned model', 'metric learning based method', 'Our learning framework', 'metric-learning model', 'metric-learning approach', 'learning frameworks', 'learned system', 'metric learning models', 'value learning', 'Learning Classifier Systems', 'learned base models', 'classification learning method', 'structure learning methods', 'learning networks system', 'corpus-based learning', 'based learning', 'metric learning strategies', 'metric learning technique', 'metric-based learning method', 'Metric-based learning methods', 'Metric learning-based methods', 'Learned metrics', 'Learning Algorithm )']"
176,124,Metric,2.5218,2002,"{'2002': -0.5194, '2003': -0.139, '2004': -0.1305, '2005': -0.1057, '2006': 0.0442, '2007': 0.3584, '2008': 0.0385, '2009': 0.1881, '2010': 0.5844, '2011': 0.1731, '2012': 0.0433, '2013': 0.7927, '2014': 0.3654, '2015': 0.9775, '2016': 1.5426, '2017': 0.31, '2018': 1.5036, '2019': 1.8642, '2020': 2.1253, '2021': 2.5218, '2022': 2.5016}","['MRR', 'MRR @ 5', 'MRR model', 'MRR-5', 'MRR @ k', 'MRR H @ 10', 'mrr 5', 'MRR @ 3', 'F MRR', 'M MRR', 'mrr', 'MRR/', 'MRR )', 'MRR measure', 'Mean MRR', 'MRR MRR MRR', 'MRR5', 'MRR ( K )', 'MRR @ K', 'MRR P', '( MRR )', 'mean ( MRR )', 'mean replacement rate ( MRR )', 'Mean Replacement Rate ( MRR )', 'MRR S', 'MRR HR @ 10', 'F1 MRR', 'MRR↑ HR @ 10↑', 'MRR @ 1K', 'mrr n @ 10', 'MRR ’', 'MRR 5', 'MRR Model 5', 'MRR—A', 'MRR 7']"
177,517,Method,2.5211,2013,"{'2013': -0.1531, '2014': 0.0638, '2015': 1.1428, '2016': 2.5211, '2017': 1.9488, '2018': 2.1318, '2019': 2.052, '2020': 1.1309, '2021': 0.8625, '2022': 0.3777}","['recurrent networks', 'recurrent network', 'recurrent models', 'recurrent', 'recurrent model', 'recurrent architecture', 'recurrent architectures', 'recurrent mechanism', 'recurrent layers', 'Recurrent', 'recurrent structures', 'recurrent mechanisms', 'recurrent structure', 'recurrent cell architecture', 'recurrent cell', 'recurrent layer', 'Recurrent Network', 'Recurrent Networks', 'recurrent cells', 'Recurrent Models', 'recurrent network based models', 'recurrent network architecture', 'recurrent approaches', 'recurrent module', 'recurrent ( R )', 'Recurrent networks', 'recurrent network-based models', 'recurrent-based approaches', 'recurrent network architectures', 'recurrent networks models', 'recurrent-based', 'recurrent based', 'recurrent-based model', 'recurrent algorithm', 'recurrent approach', 'recurrent network-based', 'recurrent network model', 'recurrent-based modules', 'recurrent method', 'recurrent system', 'recurrent network-based model', 'recurrent SOTA model', 'recurrent LT model', 'reccurrent architectures', 'recurrent S2S models']"
178,587,Method,2.5103,2018,"{'2018': 0.1156, '2019': 0.4154, '2020': 1.4107, '2021': 2.0679, '2022': 2.5103}","['autoregressive models', 'autoregressive model', 'autoregressively', 'auto-regressive', 'autoregressive', 'auto-regressive model', 'Autoregressive models', 'auto-regressive models', 'autoregressive ( AR )', 'autoregressive approaches', 'autoregressive modeling', 'Autoregressive', 'large autoregressive models', 'autoregressive architecture', 'autoregressive methods', 'Autoregressive Models', 'auto regressive models', 'auto-regressive approach', 'auto-regressive modeling', 'auto-regressive architecture', 'auto-regressive methods', 'autoregressive approach', 'autoregressive method', 'autoregressive ( AR ) model', 'autoregressive way', 'Autoregressive model', 'Auto-regressive models', 'Auto Regressive', 'autoregressive framework', 'autoregressive frameworks', 'auto-regressive pointing mechanism', 'SOTA autoregressive models', 'autoregressive techniques', 'large auto-regressive models', 'Auto-regressive models ( ARM )', 'autogressive architectures', 'autogressive', 'autoregressive ( AT )', 'autoregressive ( AT ) models', 'autoregressive ( AR', 'autoregressive models ( AR )', 'auto-regressive ( AR ) models', 'autoregressive ( AR ) models', 'Autoregressive ( AR ) models', 'auto-regressive ( AR ) method', 'auto-regressive ( AR )', 'autoregresssive ( AR )']"

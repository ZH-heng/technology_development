{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eae6ffe0a21809f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1. Obtain the year of first occurrence for each entity, its category, and the corresponding entity ids for each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68d31f62481b8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T09:27:43.751305700Z",
     "start_time": "2023-08-15T09:27:34.168803400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Construct a normalized entity-to-id dictionary.\n",
    "df = pd.read_parquet(\"./data/normalized-ents.parquet\")\n",
    "# Filter entities with a frequency greater than 5.\n",
    "df = df[df.num>=5]\n",
    "print(len(df))\n",
    "ent2id = {}\n",
    "for row in df.itertuples():\n",
    "    for e in row.ents:\n",
    "        ent2id[e] = row[1]\n",
    "print(len(ent2id))\n",
    "\n",
    "df_paper_ents = pd.read_parquet(\"./data/paper-ents.parquet\")\n",
    "ent_year_count = defaultdict(dict)\n",
    "for row in df_paper_ents.itertuples():\n",
    "    # Obtain the year through the paper id\n",
    "    y = re.findall('[0-9]+', row[1].split('-')[0])[0]\n",
    "    year = y if len(y)==4 else f'20{y}'\n",
    "    year = int(year)\n",
    "    for e in row[2]:\n",
    "        # Entities not found in the ent2id are filtered out.\n",
    "        if e[1] not in ent2id: continue\n",
    "        ent_id = ent2id[e[1]]\n",
    "        # Count the frequency of entity occurrence each year:\n",
    "        ent_year_count[ent_id].setdefault(year, 0)\n",
    "        ent_year_count[ent_id][year] += 1\n",
    "    print(f'\\r{row[0]+1}/{len(df_paper_ents)}', end='')\n",
    "print(f'\\r{row[0]+1}/{len(df_paper_ents)}')\n",
    "ent2year = {}\n",
    "for ent_id, d in ent_year_count.items():\n",
    "    # If a new technology entity appears for the first time in NLP, it is likely to be mentioned multiple times in papers. \n",
    "    # If its frequency is too low in a certain year, it could be due to an error in entity recognition.\n",
    "    if sum(d.values())>100:\n",
    "        # For entities with a total frequency greater than 100, their first appearance year can be determined as the first year \n",
    "        # with consecutive frequency greater than 5 in two or more years.\n",
    "        years = [k for k, v in d.items() if v>=5 and d.get(k+1, 0)>=5]\n",
    "        if years:\n",
    "            ent2year[ent_id] = min(years)\n",
    "        else:\n",
    "            years = [k for k, v in d.items() if v>=5]\n",
    "            ent2year[ent_id] = min(years)\n",
    "    else:\n",
    "        # For entities with a total frequency less than or equal to 100, their first appearance year can be determined as the first year \n",
    "        # with frequency greater than 5.\n",
    "        years = [k for k, v in d.items() if v>=5]\n",
    "        if years:\n",
    "            ent2year[ent_id] = min(years)\n",
    "            \n",
    "print(len(ent2year))\n",
    "with open(r\"./data/ent-first-year.txt\", 'w', encoding='utf8') as f:\n",
    "    json.dump(ent2year, f, indent=4)\n",
    "    \n",
    "ent_type, pid_ents = defaultdict(dict), {}\n",
    "for row in df_paper_ents.itertuples():\n",
    "    # Obtain the year through the paper id\n",
    "    y = re.findall('[0-9]+', row[1].split('-')[0])[0]\n",
    "    year = y if len(y)==4 else f'20{y}'\n",
    "    year = int(year)\n",
    "    ents = set()\n",
    "    for e in row[2]:\n",
    "        # Entities not found in the ent2id are filtered out.\n",
    "        if ent2id.get(e[1], 'None') not in ent2year: continue\n",
    "        ent_id = ent2id[e[1]]\n",
    "        # If the publication date of a paper is earlier than the determined first appearance year of an entity, which is based on its frequency\n",
    "        # of occurrence, then the entity should be filtered out.\n",
    "        if year<ent2year[ent_id]: continue\n",
    "        ents.add(ent_id)\n",
    "        # The predicted count of each entity type for an entity.\n",
    "        ent_type[ent_id].setdefault(e[0], 0)\n",
    "        ent_type[ent_id][e[0]] += 1\n",
    "    pid_ents[row[1]] = ents\n",
    "    print(f'\\r{row[0]+1}/{len(df_paper_ents)}', end='')\n",
    "# The entity type is determined by the most frequently predicted type.\n",
    "ent2type = {k:sorted(v.items(), key=lambda x:x[1], reverse=True)[0][0] for k, v in ent_type.items()}\n",
    "with open(r\"F:\\tmp\\aclanthology4\\ent-type.txt\", 'w', encoding='utf8') as f:\n",
    "    json.dump(ent2type, f, indent=4)\n",
    "\n",
    "# Dictionary of paper_id to conference\n",
    "with open(\"./data/pid2conf.txt\", \"rb\") as f:\n",
    "    pid2conf = json.load(f)\n",
    "\n",
    "data = []\n",
    "for row in df_paper_ents.itertuples():\n",
    "    y = re.findall('[0-9]+', row[1].split('-')[0])[0]\n",
    "    year = y if len(y)==4 else f'20{y}'\n",
    "    year = int(year)\n",
    "    data.append([row[1], pid2conf[row[1]], year, pid_ents[row[1]]])\n",
    "\n",
    "df_ = pd.DataFrame(data, columns=['id', 'conference', 'year', 'ents'])\n",
    "df_.to_parquet(\"./data/paper-ent-ids.parquet\")\n",
    "# df_.to_csv(\"./data/paper-ent-ids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e1e54d1e186a76",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. z-score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b15e55def5be09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T09:28:25.495655600Z",
     "start_time": "2023-08-15T09:27:59.102329700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "df = pd.read_parquet(r\"./data/paper-ent-ids.parquet\")\n",
    "d = defaultdict(int)\n",
    "for row in df.itertuples():\n",
    "    for e in row.ents:\n",
    "        d[e] += 1\n",
    "\n",
    "# Entities that appear in fewer than three papers.\n",
    "filter_ids = {k for k, v in d.items() if v<3}\n",
    "\n",
    "year_ids, year_net = defaultdict(set), defaultdict(dict)\n",
    "for row in df.itertuples():\n",
    "    # Filter out entities that appear in fewer than three papers.\n",
    "    ents = set(row.ents)-filter_ids\n",
    "    combs = combinations(sorted(ents), 2)\n",
    "    for comb in combs:\n",
    "        # Record the co-occurrence frequency of entities in papers each year, that is, construct a co-occurrence network of entities for each year.\n",
    "        year_net[row.year].setdefault(f'{comb[0]}-{comb[1]}', 0)\n",
    "        year_net[row.year][f'{comb[0]}-{comb[1]}'] += 1\n",
    "    print(f'\\rbuild co_occurrence network: {row[0]+1}/{len(df)}', end='')\n",
    "print(f'\\rbuild co_occurrence network: {row[0]+1}/{len(df)}')\n",
    "\n",
    "z_dict = defaultdict(dict)\n",
    "for year in range(2000, 2023):\n",
    "    # The co-occurrence network of entities for that year.\n",
    "    net = year_net[year]\n",
    "    ent_w = defaultdict(int)\n",
    "    for k in net:\n",
    "        e1, e2 = k.split('-')\n",
    "        # Accumulate the weight between each entity and other entities.\n",
    "        ent_w[e1] += net[k]\n",
    "        ent_w[e2] += net[k]\n",
    "    # Accumulated weight list for all entities\n",
    "    w_list = list(ent_w.values())\n",
    "    # The square of the cumulative weights of all entities\n",
    "    w_2 = (w**2 for w in w_list)\n",
    "    # average value\n",
    "    avg_w = sum(w_list)/len(w_list)\n",
    "    # standard deviation\n",
    "    std = (sum(w_2)/len(w_list)-avg_w**2)**0.5\n",
    "    for e in ent_w:\n",
    "        # z-score of each entity in that year\n",
    "        z = (ent_w[e]-avg_w)/std\n",
    "        z_dict[e][year] = round(z, 4)\n",
    "    print(f'\\rcompute z_score: {year}', end='')\n",
    "with open(\"./data/z_dict.txt\", 'w', encoding='utf8') as f:\n",
    "    json.dump(z_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d6124",
   "metadata": {},
   "source": [
    "### 3. High-impact new entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e76d01aa256fd07",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"./data/ent-type.txt\", 'rb') as f:\n",
    "    id2type = json.load(f)\n",
    "with open(\"./data/ent-first-year.txt\", 'rb') as f:\n",
    "    id2year = json.load(f)\n",
    "df = pd.read_parquet(\"./data/normalized-ents.parquet\")\n",
    "df = df[df.num>=5]\n",
    "id2ents = {row.ent_id:row.ents for row in df.itertuples()}\n",
    "with open(\"./data/z_dict.txt\", 'rb') as f:\n",
    "    z_dict = json.load(f)\n",
    "# Starting from 2001, any new entity with the largest z-score exceeding 2.5 will be considered a high-impact entity.\n",
    "data = []\n",
    "for k, v in z_dict.items():\n",
    "    year = id2year[k]\n",
    "    if year==2000: continue\n",
    "    max_z = max(v.values())\n",
    "    if max_z>2.5:\n",
    "        data.append([k, id2type[k], max_z, year, v, list(id2ents[int(k)])])\n",
    "data = sorted(data, key=lambda x:x[2], reverse=True)\n",
    "df_ = pd.DataFrame(data, columns=['ent_id', 'ent_type', 'max_z-score', 'first_year', 'z-score_dict', 'ent_cluster'])\n",
    "df_.to_csv(\"./data/top-ents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7e4fbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ent_id</th>\n",
       "      <th>ent_type</th>\n",
       "      <th>max_z-score</th>\n",
       "      <th>first_year</th>\n",
       "      <th>z-score_dict</th>\n",
       "      <th>ent_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Method</td>\n",
       "      <td>43.3138</td>\n",
       "      <td>2019</td>\n",
       "      <td>{'2019': 14.3712, '2020': 34.6266, '2021': 43....</td>\n",
       "      <td>['BERT', 'BERT', 'BERT model', 'BERT-base', 'B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Method</td>\n",
       "      <td>34.6696</td>\n",
       "      <td>2018</td>\n",
       "      <td>{'2018': 3.0913, '2019': 13.3371, '2020': 26.3...</td>\n",
       "      <td>['Transformer', 'transformer', 'Transformers',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Method</td>\n",
       "      <td>28.8231</td>\n",
       "      <td>2014</td>\n",
       "      <td>{'2014': -0.0454, '2015': 3.329, '2016': 15.10...</td>\n",
       "      <td>['LSTM', 'LSTMs', 'LSTM model', 'LSTM models',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>Method</td>\n",
       "      <td>26.2604</td>\n",
       "      <td>2006</td>\n",
       "      <td>{'2006': -0.1572, '2007': -0.1271, '2008': -0....</td>\n",
       "      <td>['attention', 'attention mechanism', 'attentio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>Method</td>\n",
       "      <td>20.3561</td>\n",
       "      <td>2016</td>\n",
       "      <td>{'2016': 2.886, '2017': 9.4933, '2018': 14.846...</td>\n",
       "      <td>['Adam', 'Adam optimizer', 'ADAM', 'Adam algor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ent_id ent_type  max_z-score  first_year   \n",
       "0           0       1   Method      43.3138        2019  \\\n",
       "1           1       7   Method      34.6696        2018   \n",
       "2           2       6   Method      28.8231        2014   \n",
       "3           3      11   Method      26.2604        2006   \n",
       "4           4      67   Method      20.3561        2016   \n",
       "\n",
       "                                        z-score_dict   \n",
       "0  {'2019': 14.3712, '2020': 34.6266, '2021': 43....  \\\n",
       "1  {'2018': 3.0913, '2019': 13.3371, '2020': 26.3...   \n",
       "2  {'2014': -0.0454, '2015': 3.329, '2016': 15.10...   \n",
       "3  {'2006': -0.1572, '2007': -0.1271, '2008': -0....   \n",
       "4  {'2016': 2.886, '2017': 9.4933, '2018': 14.846...   \n",
       "\n",
       "                                         ent_cluster  \n",
       "0  ['BERT', 'BERT', 'BERT model', 'BERT-base', 'B...  \n",
       "1  ['Transformer', 'transformer', 'Transformers',...  \n",
       "2  ['LSTM', 'LSTMs', 'LSTM model', 'LSTM models',...  \n",
       "3  ['attention', 'attention mechanism', 'attentio...  \n",
       "4  ['Adam', 'Adam optimizer', 'ADAM', 'Adam algor...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"./data/top-ents.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "302d8d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'Method': 130, 'Dataset': 24, 'Metric': 19, 'Tool': 6})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The total number of high-impact entities, and the quantity of entities of each type.\n",
    "print(len(df))\n",
    "from collections import Counter\n",
    "Counter(df.iloc[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a971dd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method\n",
      "[('BERT', 43.3138), ('Transformer', 34.6696), ('LSTM', 28.8231), ('attention', 26.2604), ('Adam', 20.3561)]\n",
      "Dataset\n",
      "[('Wikipedia', 17.4187), ('MNLI', 6.7163), ('SQuAD', 5.783), ('Twitter', 5.3056), ('SST-2', 5.2605)]\n",
      "Metric\n",
      "[('BLEU', 15.9303), ('cross-entropy', 13.1292), ('ROUGE', 7.8905), ('fluency', 6.9009), ('standard deviation', 6.1762)]\n",
      "Tool\n",
      "[('PyTorch', 6.1565), ('Moses', 5.3327), ('GIZA++', 5.2089), ('TensorFlow', 3.563), ('Stanford parser', 3.2967)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(dict)\n",
    "for row in df.itertuples():\n",
    "    ents = eval(row[7])\n",
    "    d[row[3]][ents[0]] = row[4]\n",
    "# The top 5 high-impact entities for each type.\n",
    "for k, v in d.items():\n",
    "    v = sorted(v.items(), key=lambda x:x[1], reverse=True)\n",
    "    print(k)\n",
    "    print(v[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

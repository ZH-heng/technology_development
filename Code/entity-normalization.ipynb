{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da3340a2f7f23f7a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Replace each abbreviation entity with its fullname in every paper, assuming that each abbreviation corresponds to only one fullname in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb636bf0bfe1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T09:30:32.208826100Z",
     "start_time": "2023-08-12T09:29:28.974882700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flashtext import KeywordProcessor\n",
    "import pandas as pd\n",
    "from fasttext import load_model\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "abb2full = {}\n",
    "with open(\"./data/mapping-list.txt\", encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        a = line.strip().lower().split('\\t')\n",
    "        fulls = [_.strip() for _ in a[1:]]\n",
    "        abb2full[a[0]] = fulls\n",
    "        abb2full[a[0].replace(' ', '')] = fulls\n",
    "abbs = sorted(abb2full.keys(), key=lambda x:len(x), reverse=True)\n",
    "kp = KeywordProcessor()\n",
    "kp.add_keywords_from_list(abbs)\n",
    "\n",
    "'''\n",
    "Load pre-trained FastText model.\n",
    "We trained a FastText model based on the collected research paper data to generate entity representation vectors.\n",
    "Due to the large file size, it was not uploaded. If necessary, you can train the model using your own data or \n",
    "obtain word vectors through other means.\n",
    "'''\n",
    "ftt = load_model(\"ftt.model\")\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "    sim = x.dot(y.T)/(np.linalg.norm(x) * np.linalg.norm(y))\n",
    "    return sim\n",
    "\n",
    "def replace_abb2full(ents):\n",
    "    ents_str = '  '.join(ents)\n",
    "    ents_vec = ftt.get_sentence_vector(ents_str)\n",
    "    ents_str = re.sub('[!\"#$%&\\'()*,-./:;<=>?[\\\\]^_`{|}~（）—_–]', ' ', ents_str)\n",
    "    ents_str = re.sub(r'(\\d+)', r' \\1', ents_str)\n",
    "    ents_str = re.sub(' +', ' ', ents_str)\n",
    "    # Match abbreviation entities.\n",
    "    matched_abbs = kp.extract_keywords(ents_str)\n",
    "    kws = []\n",
    "    for abb in matched_abbs:\n",
    "        fullnames = abb2full[abb]\n",
    "        if len(fullnames)==1:\n",
    "            kws.append([abb, fullnames[0]])\n",
    "        else:\n",
    "            sims = [(fullname, cosine_similarity(ftt.get_word_vector(fullname), ents_vec)) for fullname in fullnames]\n",
    "            # Get the fullname with the highest similarity score.\n",
    "            sims = sorted(sims, key=lambda _:_[1])\n",
    "            kws.append([abb, sims[-1][0]])\n",
    "    kp2 = KeywordProcessor()\n",
    "    kws = sorted(kws, key=lambda _:len(_[0]), reverse=True)\n",
    "    for kw in kws:\n",
    "        kp2.add_keyword(kw[0], kw[1])\n",
    "    new_ents = {}\n",
    "    for ent in ents:\n",
    "        e = re.sub('[!\"#$%&\\'()*,-./:;<=>?[\\\\]^_`{|}~（）—_–]', ' ', ent)\n",
    "        # Add a space between digits and characters to separate them.\n",
    "        e = re.sub(r'(\\d+)', r' \\1', e)\n",
    "        e = re.sub(' +', ' ', e)\n",
    "        # Replace the abbreviated entity with its fullname.\n",
    "        new_ents[ent] = kp2.replace_keywords(e)\n",
    "    return new_ents\n",
    "\n",
    "df_ents = pd.read_parquet(\"./data/paper-ents.parquet\")\n",
    "d = defaultdict(set)\n",
    "for row in df_ents.itertuples():\n",
    "    ents = {_[1].lower() for _ in row[2]}\n",
    "    new_ents = replace_abb2full(ents)\n",
    "    new_ents_2 = replace_abb2full(set(new_ents.values()))\n",
    "    new_ents = {k:new_ents_2[v] for k, v in new_ents.items()}\n",
    "    for e in row[2]:\n",
    "        d[new_ents[e[1].lower()]].add(e[1])\n",
    "    print(f'\\r{row[0]+1}/{len(df_ents)}', end='')\n",
    "data = [(k, v) for k, v in d.items()]\n",
    "df = pd.DataFrame(data)\n",
    "# df.to_csv('./data/replaced-ents.csv')\n",
    "df.to_parquet('./data/replaced-ents.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491101efe90dbaf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Lemmatize the replaced entity words and remove some words that do not contribute much to the semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe80573041059b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T10:12:48.694646700Z",
     "start_time": "2023-08-12T10:12:35.671311400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from flashtext import KeywordProcessor\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# List of words that do not contribute much to the semantics.\n",
    "with open(\"./data/remove-words.txt\", encoding='utf8') as f:\n",
    "    kws = f.read().strip().split('\\n')\n",
    "kp = KeywordProcessor()\n",
    "kp.add_keywords_from_dict({' ':kws})\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "def word_lemma(w):\n",
    "    # Lemmatize nouns, verbs, and adjectives\n",
    "    w_ = wnl.lemmatize(w, 'n')\n",
    "    if w_ != w: return w_\n",
    "    w_ = wnl.lemmatize(w, 'v')\n",
    "    if w_ != w: return w_\n",
    "    w_ = wnl.lemmatize(w, 'a')\n",
    "    if w_ != w: return w_\n",
    "    return w\n",
    "\n",
    "df = pd.read_parquet('./data/replaced-ents.parquet')\n",
    "words = set()\n",
    "for row in df.itertuples():\n",
    "    ws = row[1].split(' ')\n",
    "    # Add all entity words\n",
    "    for w in ws:\n",
    "        words.add(w)\n",
    "\n",
    "abb2lemma_full = {}\n",
    "with open(\"./data/mapping-list.txt\", encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        a = line.strip().lower().split('\\t')\n",
    "        fulls = [_.strip() for _ in a[1:]]\n",
    "        # For an abbreviation with only one fullname, lemmatize it to its original word form, remove insignificant words that do not impact its meaning,\n",
    "        # delete spaces, and also remove spaces in the abbreviation. Use it to replace an entity that remains an abbreviation after processing.\n",
    "        if len(fulls)==1:\n",
    "            ws = fulls[0].split(' ')\n",
    "            ws_lemma = [word_lemma(w) for w in ws]\n",
    "            ent = ' '.join(ws_lemma)\n",
    "            ent = kp.replace_keywords(ent)\n",
    "            ws = ent.split(' ')\n",
    "            abb2lemma_full[a[0].replace(' ', '')] = ''.join(sorted(ws))\n",
    "        for full in fulls:\n",
    "            ws = full.split(' ')\n",
    "            # Add the entity word from the mapping dictionary.\n",
    "            for w in ws:\n",
    "                words.add(w)\n",
    "word2lemma = {w:word_lemma(w) for w in words}\n",
    "\n",
    "d = defaultdict(list)\n",
    "for row in df.itertuples():\n",
    "    words = row[1].split(' ')\n",
    "    ws_lemma = [word2lemma[w] for w in words]\n",
    "    words = [ws_lemma[0]]\n",
    "    for w in ws_lemma[1:]:\n",
    "        # Exclude consecutive identical words, as this may be caused by two consecutive replacements.\n",
    "        if w!=words[-1]:\n",
    "            words.append(w)\n",
    "    cou = Counter(words)\n",
    "    # Remove duplicate words when there are at least two repeated words, as this may occur when an abbreviation and its fullname appear in the same entity.\n",
    "    if len(cou)>2 and cou.most_common(2)[-1][-1]>1:\n",
    "        ws, se = [], set()\n",
    "        for w in words:\n",
    "            if w not in se:\n",
    "                ws.append(w)\n",
    "                se.add(w)\n",
    "        ent = ' '.join(ws)\n",
    "    else:\n",
    "        ent = ' '.join(words)\n",
    "    ent = kp.replace_keywords(ent)\n",
    "    if len(ent)>1 and ent[0].isdigit(): continue\n",
    "    ws = re.split(' +|(\\d+)', ent)\n",
    "    ws = [w for w in ws if w]\n",
    "    if not ws: continue\n",
    "    ws_ = [ws[0]]\n",
    "    for w in ws[1:]:\n",
    "        # Merge the number with the preceding word.\n",
    "        if w.isdigit():\n",
    "            ws_[-1] = ws_[-1]+w\n",
    "        else:\n",
    "            ws_.append(w)\n",
    "    ent = ''.join(ws_)\n",
    "    if ent in abb2lemma_full:\n",
    "        # If, after the previous processing step, the entity is still an abbreviation without spaces, then replace it with the corresponding fullname.\n",
    "        ws_ = abb2lemma_full[ent].split(' ')\n",
    "    # Sorting the processed entity words can eliminate the impact of word order on calculating the edit distance.\n",
    "    ws = sorted(ws_)\n",
    "    ent = ''.join(ws)\n",
    "    # If the processed entity contains only one word, such as \"score,\" it may lack meaning. If there are similar meaningless words discovered later on,\n",
    "    # they can also be filtered here.\n",
    "    if ent in {'score'}:\n",
    "        continue\n",
    "    # Retain only entities with at least two characters.\n",
    "    if len(ent)>1:\n",
    "        d[ent].extend(row[2])\n",
    "    if (row[0]+1)%10000==0:\n",
    "        print(f'\\r{row[0]+1}/{len(df)}', end='')\n",
    "print(f'\\r{row[0]+1}/{len(df)}', end='')\n",
    "data = [(k, v) for k, v in d.items()]\n",
    "df_ = pd.DataFrame(data)\n",
    "# df_.to_csv('./data/replaced-ents-lemma.csv')\n",
    "df_.to_parquet('./data/replaced-ents-lemma.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385c33ee4324f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. Similarity calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b142f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('./data/replaced-ents-lemma.parquet')\n",
    "ent2lemma = {}\n",
    "for row in df.itertuples():\n",
    "    for e in row[2]:\n",
    "        ent2lemma[e] = row[1]\n",
    "df_ent = pd.read_parquet(\"./data/paper-ents.parquet\")\n",
    "d = defaultdict(int)\n",
    "# Calculate the frequency of entities after the previous processing.\n",
    "for row in df_ent.itertuples():\n",
    "    for e in row[2]:\n",
    "        if e[1] in ent2lemma:\n",
    "            d[ent2lemma[e[1]]] += 1\n",
    "data = sorted(d.items(), key=lambda x:x[1], reverse=True)\n",
    "df_ = pd.DataFrame(data)\n",
    "# df_.to_csv('./data/replaced-ents-lemma-count.csv')\n",
    "df_.to_parquet('./data/replaced-ents-lemma-count.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from thefuzz import fuzz\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('./data/replaced-ents-lemma-count.parquet')\n",
    "# Construct a trigram index.\n",
    "ents, trigram_index = [], defaultdict(set)\n",
    "for row in df.itertuples():\n",
    "    ents.append((row[0], row[1]))\n",
    "    for i in range(len(row[1])-2):\n",
    "        trigram_index[row[1][i:i+3]].add(row[0])\n",
    "        \n",
    "def data_gen(ent_list):\n",
    "    for i, ent in ent_list:\n",
    "        # Do not calculate similarity for entities with a length of less than three characters.\n",
    "        if len(ent)<3: continue\n",
    "        # if of entity with at least one trigram\n",
    "        ids = set()\n",
    "        for j in range(len(ent)-2):\n",
    "            ids |= trigram_index[ent[j:j+3]]\n",
    "        l1 = len(ent)\n",
    "        for j in ids:\n",
    "            # Filter out the id less than i, and calculate the similarity of each entity only with the entities whose id are greater than i.\n",
    "            if j<=i: continue\n",
    "            l2 = len(ents[j][1])\n",
    "            # Filter out the entities whose length differs by more than 50%.\n",
    "            if l1/l2>1.5 or l2/l1>1.5: continue\n",
    "            yield i, j, ent, ents[j][1] \n",
    "\n",
    "def fuzz_sim(item):\n",
    "    # Utilize `thefuzz` library to calculate the Levenshtein distance between two entities.\n",
    "    s = fuzz.ratio(item[2], item[3])\n",
    "    return item[0], item[1], s\n",
    "\n",
    "n = len(ents)\n",
    "print(n, ents[:5])\n",
    "for i in range(0, n, 500):\n",
    "    print(f'\\r{i}/{n},  {(i)/n*100:.2f}%', end='')\n",
    "    items = data_gen(ents[i:i+500])\n",
    "    data = map(fuzz_sim, items)\n",
    "    # Retain only the entities with a similarity score of 80 or higher.\n",
    "    data = [j for j in data if j[2]>80]\n",
    "    df_ = pd.DataFrame(data)\n",
    "    df_.to_csv('./data/sims.csv', index=False, header=False, mode='a')\n",
    "print(f'\\r{n}/{n},  {(n)/n*100:.2f}%', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da9b1d9",
   "metadata": {},
   "source": [
    "### 4. Entity clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a363566024bfacc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T12:42:20.931807700Z",
     "start_time": "2023-08-12T12:42:19.158428900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "df = pd.read_parquet(\"./data/replaced-ents-lemma-count.parquet\")\n",
    "d_len = {}\n",
    "for row in df.itertuples():\n",
    "    d_len[row[0]] = len(row[1])\n",
    "\n",
    "df = pd.read_csv(\"./data/sims.csv\", header=None)\n",
    "d_sim, id2ids, clusters, clu_ids  = {}, defaultdict(set), [], set()\n",
    "for row in df.itertuples():\n",
    "    d_sim[(row[1], row[2])] = row[3]\n",
    "    # Initialize a cluster with the entities that have a similarity score greater than 95.\n",
    "    if row[3]>95:\n",
    "        if row[1] not in clu_ids and row[2] not in clu_ids:\n",
    "            clusters.append({row[1], row[2]})\n",
    "            clu_ids.add(row[1])\n",
    "            clu_ids.add(row[2])\n",
    "    # Record the id of every entity that has a similarity score greater than 85 with another entity. The clustering threshold will be set at 85\n",
    "    # such that only clusters with at least one entity having a similarity score greater than the threshold will have an average similarity score\n",
    "    # greater than the threshold.\n",
    "    if row[3]>85:\n",
    "        id2ids[row[1]].add(row[2])\n",
    "        id2ids[row[2]].add(row[1])\n",
    "\n",
    "# Entities with similarity scores less than or equal to 95 will each be initialized as a separate cluster.\n",
    "ids_ = set(id2ids.keys())-clu_ids\n",
    "for i in ids_:\n",
    "    clusters.append({i})\n",
    "print(f'all: {len(id2ids)}  clu_ids: {len(clu_ids)}  ids-clu_ids: {len(ids_)}')\n",
    "print(f'Initialized cluster: {len(clusters)}')\n",
    "print(clusters[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a97b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def avg_sim(c1, c2):\n",
    "    # Calculate the average similarity between entities within two clusters.\n",
    "    combs = product(c1, c2)\n",
    "    sim_list = []\n",
    "    for comb in combs:\n",
    "        s = d_sim.get(comb, 0)+d_sim.get((comb[1], comb[0]), 0)\n",
    "        if s==0: return 0\n",
    "        sim_list.append(s)\n",
    "    return sum(sim_list)/len(sim_list)\n",
    "\n",
    "threshold = 93\n",
    "for i in range(15):\n",
    "    print(len(clusters))\n",
    "    new_clusters = []\n",
    "    entid2cluid = defaultdict(set)\n",
    "    for m, cl in enumerate(clusters):\n",
    "        l = min([d_len[idx] for idx in cl])\n",
    "        # The threshold must be no less than 90 when the length of entities is no more than 5.\n",
    "        t = 90 if l<6 and threshold<90 else threshold\n",
    "        # Entity ids with similarity greater than 85 to the entities within the cluster.\n",
    "        filter_ids = set()\n",
    "        for j in cl:\n",
    "            filter_ids |= id2ids.get(j, set())\n",
    "        # Retrieve the clusters that contain these entities with similarity greater than 85, as only the clusters that include these entities\n",
    "        # can possibly have an average similarity greater than the threshold of the given cluster.\n",
    "        cluid_set = set()\n",
    "        for idx in filter_ids:\n",
    "            cluid_set |= entid2cluid.get(idx, set())\n",
    "        # If the set of cluid_set is empty, then add the cluster as a new cluster.\n",
    "        if not cluid_set:\n",
    "            for j in cl:\n",
    "                entid2cluid[j].add(len(new_clusters))\n",
    "            new_clusters.append(cl)\n",
    "            continue\n",
    "        for cluid in cluid_set:\n",
    "            # Calculate the average similarity between two clusters.\n",
    "            avg_s = avg_sim(cl, new_clusters[cluid])\n",
    "            # If the similarity exceeds a certain threshold, merge the clusters. Otherwise, create a new cluster.\n",
    "            if avg_s>t:\n",
    "                for j in cl:\n",
    "                    entid2cluid[j].add(cluid)\n",
    "                new_clusters[cluid] = new_clusters[cluid] | cl\n",
    "                break\n",
    "        else:\n",
    "            for j in cl:\n",
    "                entid2cluid[j].add(len(new_clusters))\n",
    "            new_clusters.append(cl)\n",
    "        if (m+1)%10000==0:\n",
    "            print(f'\\repoch:{i+1}, {m+1}/{len(clusters)}', end='')\n",
    "    print(f'\\repoch:{i+1}, {m+1}/{len(clusters)}, threshold:{threshold}')\n",
    "    # Decrease the threshold by 2 after each epoch until it reaches 85 and no longer decreases.\n",
    "    if threshold>85:\n",
    "        threshold -= 2\n",
    "    # Terminate the loop when the number of clusters no longer changes.\n",
    "    if len(new_clusters)==len(clusters):\n",
    "        break\n",
    "    else:\n",
    "        clusters = new_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a305f57",
   "metadata": {},
   "source": [
    "### 5. Find the original entities corresponding to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f3540e4ec576a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T13:07:30.103663200Z",
     "start_time": "2023-08-12T13:07:28.186999600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ents_lemma = pd.read_parquet(\"./dada/replaced-ents-lemma.parquet\")\n",
    "# Lemma entities and their corresponding original entities.\n",
    "d = {row[1]:list(row[2]) for row in df_ents_lemma.itertuples()}\n",
    "df_count = pd.read_parquet(r\"F:\\tmp\\aclanthology4\\replaced-ents-lemma-count.parquet\")\n",
    "id2lemma_ent = {row[0]:row[1] for row in df_count.itertuples()}\n",
    "clu_ents, clustered_ids = [], set()\n",
    "for i, ncl in enumerate(new_clusters):\n",
    "    ents = []\n",
    "    for j in ncl:\n",
    "        # Find the original entity corresponding to the lemma_ent id.\n",
    "        ents.extend(d[id2lemma_ent[j]])\n",
    "        # 记录被聚类的lemma_ent id\n",
    "        clustered_ids.add(j)\n",
    "    clu_ents.append(ents)\n",
    "    if (i+1)%10000==0:\n",
    "        print(f'\\r{i+1}/{len(new_clusters)}', end='')\n",
    "print(f'\\r{i+1}/{len(new_clusters)}')\n",
    "\n",
    "# Take the lemma_ent that was not included in clustering and form it into a cluster by itself. \n",
    "# Then, find the corresponding original entity for this lemma_ent.\n",
    "ids_ = set(d_len.keys())-clustered_ids\n",
    "for i, j in enumerate(ids_):\n",
    "    clu_ents.append(d[id2lemma_ent[j]])\n",
    "    if (i+1)%10000==0:\n",
    "        print(f'\\r{i+1}/{len(ids_)}', end='')\n",
    "print(f'\\r{i+1}/{len(ids_)}')\n",
    "    \n",
    "from collections import defaultdict\n",
    "df = pd.read_parquet(r\"F:\\tmp\\aclanthology4\\paper-ents.parquet\")\n",
    "# Compute the frequency for each original entity.\n",
    "ent_count = defaultdict(int)\n",
    "for row in df.itertuples():\n",
    "    for e in row[2]:\n",
    "        ent_count[e[1]] += 1\n",
    "data = []\n",
    "for i in clu_ents:\n",
    "    # Sort the original entities in the cluster by their frequency.\n",
    "    ents = sorted(i, key=lambda x:ent_count[x], reverse=True)\n",
    "    n = sum(ent_count[x] for x in ents)\n",
    "    data.append([n, ents])\n",
    "data = sorted(data, key=lambda x:x[0], reverse=True)\n",
    "data = [[i, j[0], j[1]] for i, j in enumerate(data)]\n",
    "df_ = pd.DataFrame(data, columns=['ent_id', 'num', 'ents'])\n",
    "# df_.to_csv(\"./data/normalized-ents.csv\")\n",
    "df_.to_parquet('./data/normalized-ents.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
